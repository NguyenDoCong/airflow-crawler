[2025-05-07T02:19:44.681+0000] {processor.py:186} INFO - Started process (PID=57) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:19:44.682+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:19:44.686+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:19:44.685+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:19:44.923+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:19:50.290+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:19:50.289+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:19:50.299+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:19:50.299+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:19:50.318+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.604 seconds
[2025-05-07T02:20:45.295+0000] {processor.py:186} INFO - Started process (PID=56) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:20:45.297+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:20:45.298+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:20:45.298+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:20:45.380+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:20:45.407+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:20:45.407+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:20:45.417+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:20:45.417+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:20:45.433+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.143 seconds
[2025-05-07T02:21:15.748+0000] {processor.py:186} INFO - Started process (PID=71) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:21:15.797+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:21:15.801+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:21:15.801+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:21:15.874+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:21:15.892+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:21:15.892+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:21:15.902+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:21:15.901+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:21:16.019+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.275 seconds
[2025-05-07T02:21:46.851+0000] {processor.py:186} INFO - Started process (PID=92) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:21:46.852+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:21:46.854+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:21:46.854+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:21:46.944+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:21:46.966+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:21:46.965+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:21:47.124+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:21:47.123+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:21:47.148+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.303 seconds
[2025-05-07T02:22:20.495+0000] {processor.py:186} INFO - Started process (PID=107) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:22:20.496+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:22:20.497+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:22:20.497+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:22:20.561+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:22:20.580+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:22:20.579+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:22:20.589+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:22:20.589+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:22:20.601+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.110 seconds
[2025-05-07T02:22:50.827+0000] {processor.py:186} INFO - Started process (PID=126) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:22:50.829+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:22:50.830+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:22:50.830+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:22:50.912+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:22:50.930+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:22:50.930+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:22:50.939+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:22:50.939+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:22:50.951+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.128 seconds
[2025-05-07T02:23:21.563+0000] {processor.py:186} INFO - Started process (PID=139) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:23:21.564+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:23:21.565+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:23:21.565+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:23:21.640+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:23:21.657+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:23:21.657+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:23:21.667+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:23:21.667+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:23:21.681+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.122 seconds
[2025-05-07T02:23:51.727+0000] {processor.py:186} INFO - Started process (PID=154) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:23:51.728+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:23:51.730+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:23:51.730+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:23:51.805+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:23:51.822+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:23:51.822+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:23:51.948+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:23:51.948+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:23:51.961+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.238 seconds
[2025-05-07T02:24:22.613+0000] {processor.py:186} INFO - Started process (PID=169) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:24:22.625+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:24:22.630+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:24:22.630+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:24:22.735+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:24:22.754+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:24:22.753+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:24:22.763+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:24:22.762+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:24:22.775+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.168 seconds
[2025-05-07T02:24:53.546+0000] {processor.py:186} INFO - Started process (PID=184) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:24:53.549+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:24:53.551+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:24:53.550+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:24:53.641+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:24:53.659+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:24:53.659+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:24:53.669+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:24:53.669+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:24:53.682+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.140 seconds
[2025-05-07T02:25:23.710+0000] {processor.py:186} INFO - Started process (PID=205) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:25:23.711+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:25:23.712+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:25:23.712+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:25:23.784+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:25:23.801+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:25:23.801+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:25:23.809+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:25:23.809+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:25:23.822+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2025-05-07T02:25:55.874+0000] {processor.py:186} INFO - Started process (PID=220) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:25:55.892+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:25:55.893+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:25:55.893+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:25:55.959+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:25:55.977+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:25:55.976+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:25:56.099+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:25:56.099+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:25:50.571+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.253 seconds
[2025-05-07T02:26:20.712+0000] {processor.py:186} INFO - Started process (PID=233) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:26:20.720+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:26:20.722+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:26:20.721+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:26:20.798+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:26:20.826+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:26:20.826+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:26:20.839+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:26:20.839+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:26:25.894+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.148 seconds
[2025-05-07T02:26:56.733+0000] {processor.py:186} INFO - Started process (PID=251) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:26:56.734+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:26:56.735+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:26:56.735+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:26:56.843+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:26:56.867+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:26:56.867+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:26:56.883+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:26:56.883+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:26:56.896+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.168 seconds
[2025-05-07T02:27:27.443+0000] {processor.py:186} INFO - Started process (PID=266) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:27:27.443+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:27:27.445+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:27:27.445+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:27:27.518+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:27:27.536+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:27:27.535+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:27:27.547+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:27:27.547+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:27:27.559+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.122 seconds
[2025-05-07T02:27:58.190+0000] {processor.py:186} INFO - Started process (PID=281) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:27:58.191+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:27:58.193+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:27:58.192+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:27:58.360+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:27:58.381+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:27:58.380+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:27:58.390+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:27:58.390+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:27:58.928+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.743 seconds
[2025-05-07T02:28:29.205+0000] {processor.py:186} INFO - Started process (PID=296) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:28:29.206+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:28:29.208+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:28:29.207+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:28:29.279+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:28:29.298+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:28:29.298+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:28:29.417+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:28:29.417+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:28:29.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.228 seconds
[2025-05-07T02:29:01.465+0000] {processor.py:186} INFO - Started process (PID=311) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:29:01.466+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:29:01.468+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:29:01.468+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:29:01.536+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:29:01.556+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:29:01.555+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:29:01.564+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:29:01.564+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:29:01.575+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.114 seconds
[2025-05-07T02:29:32.025+0000] {processor.py:186} INFO - Started process (PID=332) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:29:32.026+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:29:32.028+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:29:32.028+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:29:32.153+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:29:32.181+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:29:32.180+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:29:32.203+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:29:32.203+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:29:32.226+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.209 seconds
[2025-05-07T02:30:03.111+0000] {processor.py:186} INFO - Started process (PID=347) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:30:03.112+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:30:03.113+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:30:03.113+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:30:03.193+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:30:03.213+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:30:03.213+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:30:03.224+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:30:03.223+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:30:03.452+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.345 seconds
[2025-05-07T02:30:34.349+0000] {processor.py:186} INFO - Started process (PID=364) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:30:34.352+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:30:34.354+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:30:34.354+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:30:34.431+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:30:34.448+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:30:34.448+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:30:34.578+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:30:34.578+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:30:34.590+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.246 seconds
[2025-05-07T02:31:06.577+0000] {processor.py:186} INFO - Started process (PID=377) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:31:06.577+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:31:06.579+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:31:06.579+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:31:01.116+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:31:01.135+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:31:01.135+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:31:01.145+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:31:01.144+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:31:01.158+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.127 seconds
[2025-05-07T02:31:31.610+0000] {processor.py:186} INFO - Started process (PID=390) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:31:31.611+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:31:31.612+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:31:31.612+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:31:26.152+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:31:26.171+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:31:26.171+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:31:26.181+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:31:26.181+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:31:26.193+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.132 seconds
[2025-05-07T02:32:01.443+0000] {processor.py:186} INFO - Started process (PID=405) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:01.444+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:32:01.446+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:01.446+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:01.515+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:01.533+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:01.533+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:32:01.542+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:01.541+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:32:01.553+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.116 seconds
[2025-05-07T02:32:31.652+0000] {processor.py:186} INFO - Started process (PID=422) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:31.653+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:32:31.656+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:31.655+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:31.728+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:31.749+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:31.749+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:32:26.341+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:26.341+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:32:26.353+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.250 seconds
[2025-05-07T02:32:56.714+0000] {processor.py:186} INFO - Started process (PID=432) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:56.715+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:32:56.716+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:56.716+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:56.793+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:32:51.262+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:51.262+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:32:51.400+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:32:51.400+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:32:51.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.254 seconds
[2025-05-07T02:33:26.643+0000] {processor.py:186} INFO - Started process (PID=447) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:33:26.657+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:33:26.658+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:33:26.658+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:33:26.728+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:33:26.747+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:33:26.747+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:33:26.756+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:33:26.756+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:33:26.768+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2025-05-07T02:33:57.584+0000] {processor.py:186} INFO - Started process (PID=462) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:33:57.585+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:33:57.588+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:33:57.587+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:33:57.671+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:33:57.690+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:33:57.690+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:33:57.700+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:33:57.700+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:33:57.714+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.134 seconds
[2025-05-07T02:34:28.488+0000] {processor.py:186} INFO - Started process (PID=477) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:34:28.500+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:34:28.502+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:34:28.501+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:34:28.583+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:34:28.602+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:34:28.601+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:34:28.612+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:34:28.612+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:34:28.803+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.320 seconds
[2025-05-07T02:34:58.870+0000] {processor.py:186} INFO - Started process (PID=498) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:34:58.871+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:34:58.873+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:34:58.873+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:34:58.940+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:34:58.959+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:34:58.959+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:34:59.081+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:34:59.080+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:34:59.091+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.226 seconds
[2025-05-07T02:35:29.868+0000] {processor.py:186} INFO - Started process (PID=513) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:35:29.869+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:35:29.870+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:35:29.870+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:35:29.939+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:35:29.958+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:35:29.958+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:35:29.967+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:35:29.967+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:35:29.978+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.114 seconds
[2025-05-07T02:36:00.929+0000] {processor.py:186} INFO - Started process (PID=528) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:36:00.933+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:36:00.935+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:36:00.934+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:36:01.003+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:36:01.022+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:36:01.022+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:36:01.030+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:36:01.030+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:36:01.042+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2025-05-07T02:36:31.794+0000] {processor.py:186} INFO - Started process (PID=543) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:36:31.795+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:36:31.797+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:36:31.796+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:36:36.937+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:36:36.956+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:36:36.955+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:36:36.966+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:36:36.965+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:36:37.091+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.248 seconds
[2025-05-07T02:37:07.735+0000] {processor.py:186} INFO - Started process (PID=559) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:37:07.737+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:37:07.739+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:37:07.739+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:37:07.817+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:37:07.835+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:37:07.835+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:37:07.963+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:37:07.963+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:37:07.972+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.243 seconds
[2025-05-07T02:37:42.020+0000] {processor.py:186} INFO - Started process (PID=574) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:37:42.021+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:37:42.023+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:37:42.022+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:37:42.112+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:37:42.135+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:37:42.134+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:37:42.145+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:37:42.145+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:37:42.160+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.146 seconds
[2025-05-07T02:38:12.271+0000] {processor.py:186} INFO - Started process (PID=592) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:38:12.273+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:38:12.274+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:38:12.274+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:38:06.801+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:38:06.819+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:38:06.818+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:38:06.827+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:38:06.827+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:38:06.839+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.125 seconds
[2025-05-07T02:38:42.090+0000] {processor.py:186} INFO - Started process (PID=610) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:38:42.091+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:38:42.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:38:42.093+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:38:42.171+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:38:42.189+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:38:42.189+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:38:42.202+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:38:42.202+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:38:36.838+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.295 seconds
[2025-05-07T02:39:07.376+0000] {processor.py:186} INFO - Started process (PID=618) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:39:07.376+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:39:07.378+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:39:07.378+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:39:01.905+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:39:01.925+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:39:01.925+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:39:02.064+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:39:02.063+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:39:02.076+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.257 seconds
[2025-05-07T02:39:26.897+0000] {processor.py:186} INFO - Started process (PID=633) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:39:26.899+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:39:26.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:39:26.901+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:39:26.976+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:39:26.995+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:39:26.995+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:39:27.004+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:39:27.004+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:39:27.017+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.125 seconds
[2025-05-07T02:39:57.093+0000] {processor.py:186} INFO - Started process (PID=648) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:39:57.094+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:39:57.104+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:39:57.104+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:40:02.231+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:40:02.248+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:40:02.248+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:40:02.257+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:40:02.256+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:40:02.268+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2025-05-07T02:40:33.098+0000] {processor.py:186} INFO - Started process (PID=669) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:40:33.101+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:40:33.103+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:40:33.103+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:40:33.179+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:40:33.197+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:40:33.196+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:40:33.205+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:40:33.205+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:40:33.340+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.247 seconds
[2025-05-07T02:41:07.457+0000] {processor.py:186} INFO - Started process (PID=684) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:41:07.458+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:41:07.460+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:41:07.460+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:41:07.534+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:41:07.553+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:41:07.552+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T02:41:02.134+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:41:02.134+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T02:41:02.145+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.241 seconds
[2025-05-07T02:41:24.656+0000] {processor.py:186} INFO - Started process (PID=699) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:41:24.657+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:41:24.659+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:41:24.659+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:41:50.595+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:41:50.592+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 23, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561dad49178a <unknown>
#1 0x561dacf340a0 <unknown>
#2 0x561dacf6e1ff <unknown>
#3 0x561dacf69c0f <unknown>
#4 0x561dacfb9d75 <unknown>
#5 0x561dacfb9296 <unknown>
#6 0x561dacfab173 <unknown>
#7 0x561dacf77d4b <unknown>
#8 0x561dacf789b1 <unknown>
#9 0x561dad45693b <unknown>
#10 0x561dad45a83a <unknown>
#11 0x561dad43e692 <unknown>
#12 0x561dad45b3c4 <unknown>
#13 0x561dad4234cf <unknown>
#14 0x561dad47f568 <unknown>
#15 0x561dad47f746 <unknown>
#16 0x561dad4905f6 <unknown>
#17 0x7ff583ec61f5 <unknown>
[2025-05-07T02:41:50.596+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:41:50.617+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 28.463 seconds
[2025-05-07T02:42:21.386+0000] {processor.py:186} INFO - Started process (PID=830) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:21.390+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:42:21.393+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:42:21.393+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:27.395+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:42:27.394+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 23, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55aa51d4378a <unknown>
#1 0x55aa517e60a0 <unknown>
#2 0x55aa518201ff <unknown>
#3 0x55aa5181bc0f <unknown>
#4 0x55aa5186bd75 <unknown>
#5 0x55aa5186b296 <unknown>
#6 0x55aa5185d173 <unknown>
#7 0x55aa51829d4b <unknown>
#8 0x55aa5182a9b1 <unknown>
#9 0x55aa51d0893b <unknown>
#10 0x55aa51d0c83a <unknown>
#11 0x55aa51cf0692 <unknown>
#12 0x55aa51d0d3c4 <unknown>
#13 0x55aa51cd54cf <unknown>
#14 0x55aa51d31568 <unknown>
#15 0x55aa51d31746 <unknown>
#16 0x55aa51d425f6 <unknown>
#17 0x7f3565d7e1f5 <unknown>
[2025-05-07T02:42:27.396+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:27.410+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.982 seconds
[2025-05-07T02:42:48.803+0000] {processor.py:186} INFO - Started process (PID=866) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:48.804+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:42:48.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:42:48.805+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:48.807+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:42:48.807+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1017, in get_code
  File "<frozen importlib._bootstrap_external>", line 947, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 24
    with DAG(
    ^^^^
IndentationError: expected an indented block after 'with' statement on line 9
[2025-05-07T02:42:48.808+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:48.823+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.026 seconds
[2025-05-07T02:42:53.334+0000] {processor.py:186} INFO - Started process (PID=867) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:53.336+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:42:53.337+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:42:53.337+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:54.304+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:42:54.303+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 30, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5645219e978a <unknown>
#1 0x56452148c0a0 <unknown>
#2 0x5645214c61ff <unknown>
#3 0x5645214c1c0f <unknown>
#4 0x564521511d75 <unknown>
#5 0x564521511296 <unknown>
#6 0x564521503173 <unknown>
#7 0x5645214cfd4b <unknown>
#8 0x5645214d09b1 <unknown>
#9 0x5645219ae93b <unknown>
#10 0x5645219b283a <unknown>
#11 0x564521996692 <unknown>
#12 0x5645219b33c4 <unknown>
#13 0x56452197b4cf <unknown>
#14 0x5645219d7568 <unknown>
#15 0x5645219d7746 <unknown>
#16 0x5645219e85f6 <unknown>
#17 0x7ffb7497e1f5 <unknown>
[2025-05-07T02:42:54.305+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:42:54.319+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.991 seconds
[2025-05-07T02:43:25.043+0000] {processor.py:186} INFO - Started process (PID=903) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:43:25.047+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:43:25.047+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:43:25.047+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:43:25.892+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:43:25.891+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 30, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5641dab2978a <unknown>
#1 0x5641da5cc0a0 <unknown>
#2 0x5641da6061ff <unknown>
#3 0x5641da601c0f <unknown>
#4 0x5641da651d75 <unknown>
#5 0x5641da651296 <unknown>
#6 0x5641da643173 <unknown>
#7 0x5641da60fd4b <unknown>
#8 0x5641da6109b1 <unknown>
#9 0x5641daaee93b <unknown>
#10 0x5641daaf283a <unknown>
#11 0x5641daad6692 <unknown>
#12 0x5641daaf33c4 <unknown>
#13 0x5641daabb4cf <unknown>
#14 0x5641dab17568 <unknown>
#15 0x5641dab17746 <unknown>
#16 0x5641dab285f6 <unknown>
#17 0x7f9668a421f5 <unknown>
[2025-05-07T02:43:25.893+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:43:25.907+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.868 seconds
[2025-05-07T02:43:56.052+0000] {processor.py:186} INFO - Started process (PID=939) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:43:56.052+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:43:56.053+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:43:56.053+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:43:57.021+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:43:57.020+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 30, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x555f289f778a <unknown>
#1 0x555f2849a0a0 <unknown>
#2 0x555f284d41ff <unknown>
#3 0x555f284cfc0f <unknown>
#4 0x555f2851fd75 <unknown>
#5 0x555f2851f296 <unknown>
#6 0x555f28511173 <unknown>
#7 0x555f284ddd4b <unknown>
#8 0x555f284de9b1 <unknown>
#9 0x555f289bc93b <unknown>
#10 0x555f289c083a <unknown>
#11 0x555f289a4692 <unknown>
#12 0x555f289c13c4 <unknown>
#13 0x555f289894cf <unknown>
#14 0x555f289e5568 <unknown>
#15 0x555f289e5746 <unknown>
#16 0x555f289f65f6 <unknown>
#17 0x7fd5380821f5 <unknown>
[2025-05-07T02:43:57.022+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:43:57.036+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.989 seconds
[2025-05-07T02:44:27.604+0000] {processor.py:186} INFO - Started process (PID=975) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:27.605+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:44:27.606+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:27.606+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:22.908+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:22.907+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 30, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55df31b5378a <unknown>
#1 0x55df315f60a0 <unknown>
#2 0x55df316301ff <unknown>
#3 0x55df3162bc0f <unknown>
#4 0x55df3167bd75 <unknown>
#5 0x55df3167b296 <unknown>
#6 0x55df3166d173 <unknown>
#7 0x55df31639d4b <unknown>
#8 0x55df3163a9b1 <unknown>
#9 0x55df31b1893b <unknown>
#10 0x55df31b1c83a <unknown>
#11 0x55df31b00692 <unknown>
#12 0x55df31b1d3c4 <unknown>
#13 0x55df31ae54cf <unknown>
#14 0x55df31b41568 <unknown>
#15 0x55df31b41746 <unknown>
#16 0x55df31b525f6 <unknown>
#17 0x7fe11819f1f5 <unknown>
[2025-05-07T02:44:22.908+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:22.921+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.876 seconds
[2025-05-07T02:44:23.942+0000] {processor.py:186} INFO - Started process (PID=997) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:23.942+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:44:23.943+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:23.943+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:24.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:24.417+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:44:24.418+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:24.431+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.493 seconds
[2025-05-07T02:44:25.110+0000] {processor.py:186} INFO - Started process (PID=1010) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:25.111+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:44:25.111+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:25.111+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:25.552+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:25.552+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:44:25.552+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:25.565+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.460 seconds
[2025-05-07T02:44:55.602+0000] {processor.py:186} INFO - Started process (PID=1037) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:55.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:44:55.604+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:55.603+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:56.037+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:44:56.036+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:44:56.037+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:44:56.050+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.453 seconds
[2025-05-07T02:45:26.521+0000] {processor.py:186} INFO - Started process (PID=1070) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:45:26.525+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:45:26.525+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:45:26.525+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:45:26.951+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:45:26.951+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:45:26.952+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:45:26.965+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.448 seconds
[2025-05-07T02:45:58.053+0000] {processor.py:186} INFO - Started process (PID=1097) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:45:52.507+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:45:52.507+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:45:52.507+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:45:52.928+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:45:52.928+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:45:52.928+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:45:52.944+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.442 seconds
[2025-05-07T02:46:23.018+0000] {processor.py:186} INFO - Started process (PID=1114) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:46:23.019+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:46:23.020+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:46:23.020+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:46:17.898+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:46:17.897+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:46:17.898+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:46:17.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.448 seconds
[2025-05-07T02:46:52.940+0000] {processor.py:186} INFO - Started process (PID=1150) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:46:52.941+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:46:52.942+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:46:52.942+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:46:47.841+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:46:47.841+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:46:47.841+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:46:47.856+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.469 seconds
[2025-05-07T02:47:22.968+0000] {processor.py:186} INFO - Started process (PID=1174) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:47:22.969+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:47:22.970+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:47:22.969+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:47:17.860+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:47:17.860+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:47:17.861+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:47:17.875+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.461 seconds
[2025-05-07T02:47:48.100+0000] {processor.py:186} INFO - Started process (PID=1201) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:47:48.101+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:47:48.102+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:47:48.101+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:47:42.995+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:47:42.995+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:47:42.996+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:47:43.011+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.469 seconds
[2025-05-07T02:48:13.807+0000] {processor.py:186} INFO - Started process (PID=1228) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:48:13.808+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:48:13.809+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:48:13.809+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:48:14.240+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:48:14.240+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:48:14.240+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:48:14.255+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.453 seconds
[2025-05-07T02:48:44.377+0000] {processor.py:186} INFO - Started process (PID=1255) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:48:44.378+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:48:44.379+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:48:44.378+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:48:44.833+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:48:44.832+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:48:44.833+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:48:44.847+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.475 seconds
[2025-05-07T02:49:14.899+0000] {processor.py:186} INFO - Started process (PID=1282) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:49:14.900+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:49:14.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:49:14.901+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:49:15.402+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:49:15.401+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:49:15.402+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:49:15.414+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.521 seconds
[2025-05-07T02:49:46.050+0000] {processor.py:186} INFO - Started process (PID=1309) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:49:46.051+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:49:46.052+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:49:46.052+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:49:46.478+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:49:46.478+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T02:49:46.479+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:49:46.492+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.447 seconds
[2025-05-07T02:50:14.156+0000] {processor.py:186} INFO - Started process (PID=1336) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:50:14.157+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:50:14.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:50:14.157+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:50:15.116+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:50:15.115+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5653d910a78a <unknown>
#1 0x5653d8bad0a0 <unknown>
#2 0x5653d8be71ff <unknown>
#3 0x5653d8be2c0f <unknown>
#4 0x5653d8c32d75 <unknown>
#5 0x5653d8c32296 <unknown>
#6 0x5653d8c24173 <unknown>
#7 0x5653d8bf0d4b <unknown>
#8 0x5653d8bf19b1 <unknown>
#9 0x5653d90cf93b <unknown>
#10 0x5653d90d383a <unknown>
#11 0x5653d90b7692 <unknown>
#12 0x5653d90d43c4 <unknown>
#13 0x5653d909c4cf <unknown>
#14 0x5653d90f8568 <unknown>
#15 0x5653d90f8746 <unknown>
#16 0x5653d91095f6 <unknown>
#17 0x7fe0333071f5 <unknown>
[2025-05-07T02:50:15.117+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:50:15.136+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.985 seconds
[2025-05-07T02:50:48.389+0000] {processor.py:186} INFO - Started process (PID=1373) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:50:48.390+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:50:48.391+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:50:48.391+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:50:43.695+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:50:43.693+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56347ab3578a <unknown>
#1 0x56347a5d80a0 <unknown>
#2 0x56347a6121ff <unknown>
#3 0x56347a60dc0f <unknown>
#4 0x56347a65dd75 <unknown>
#5 0x56347a65d296 <unknown>
#6 0x56347a64f173 <unknown>
#7 0x56347a61bd4b <unknown>
#8 0x56347a61c9b1 <unknown>
#9 0x56347aafa93b <unknown>
#10 0x56347aafe83a <unknown>
#11 0x56347aae2692 <unknown>
#12 0x56347aaff3c4 <unknown>
#13 0x56347aac74cf <unknown>
#14 0x56347ab23568 <unknown>
#15 0x56347ab23746 <unknown>
#16 0x56347ab345f6 <unknown>
#17 0x7f5b139311f5 <unknown>
[2025-05-07T02:50:43.696+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:50:43.729+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.903 seconds
[2025-05-07T02:51:14.361+0000] {processor.py:186} INFO - Started process (PID=1409) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:51:14.362+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:51:14.363+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:51:14.362+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:51:15.315+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:51:15.314+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b87bac278a <unknown>
#1 0x55b87b5650a0 <unknown>
#2 0x55b87b59f1ff <unknown>
#3 0x55b87b59ac0f <unknown>
#4 0x55b87b5ead75 <unknown>
#5 0x55b87b5ea296 <unknown>
#6 0x55b87b5dc173 <unknown>
#7 0x55b87b5a8d4b <unknown>
#8 0x55b87b5a99b1 <unknown>
#9 0x55b87ba8793b <unknown>
#10 0x55b87ba8b83a <unknown>
#11 0x55b87ba6f692 <unknown>
#12 0x55b87ba8c3c4 <unknown>
#13 0x55b87ba544cf <unknown>
#14 0x55b87bab0568 <unknown>
#15 0x55b87bab0746 <unknown>
#16 0x55b87bac15f6 <unknown>
#17 0x7f82edf1c1f5 <unknown>
[2025-05-07T02:51:15.316+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:51:15.331+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.975 seconds
[2025-05-07T02:51:48.602+0000] {processor.py:186} INFO - Started process (PID=1444) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:51:48.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:51:48.604+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:51:48.604+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:51:43.956+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:51:43.956+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ea0b8b678a <unknown>
#1 0x55ea0b3590a0 <unknown>
#2 0x55ea0b3931ff <unknown>
#3 0x55ea0b38ec0f <unknown>
#4 0x55ea0b3ded75 <unknown>
#5 0x55ea0b3de296 <unknown>
#6 0x55ea0b3d0173 <unknown>
#7 0x55ea0b39cd4b <unknown>
#8 0x55ea0b39d9b1 <unknown>
#9 0x55ea0b87b93b <unknown>
#10 0x55ea0b87f83a <unknown>
#11 0x55ea0b863692 <unknown>
#12 0x55ea0b8803c4 <unknown>
#13 0x55ea0b8484cf <unknown>
#14 0x55ea0b8a4568 <unknown>
#15 0x55ea0b8a4746 <unknown>
#16 0x55ea0b8b55f6 <unknown>
#17 0x7fcb931101f5 <unknown>
[2025-05-07T02:51:43.957+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:51:43.971+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.929 seconds
[2025-05-07T02:52:18.422+0000] {processor.py:186} INFO - Started process (PID=1480) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:52:18.423+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:52:18.424+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:52:18.424+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:52:13.728+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:52:13.727+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a02e95f78a <unknown>
#1 0x55a02e4020a0 <unknown>
#2 0x55a02e43c1ff <unknown>
#3 0x55a02e437c0f <unknown>
#4 0x55a02e487d75 <unknown>
#5 0x55a02e487296 <unknown>
#6 0x55a02e479173 <unknown>
#7 0x55a02e445d4b <unknown>
#8 0x55a02e4469b1 <unknown>
#9 0x55a02e92493b <unknown>
#10 0x55a02e92883a <unknown>
#11 0x55a02e90c692 <unknown>
#12 0x55a02e9293c4 <unknown>
#13 0x55a02e8f14cf <unknown>
#14 0x55a02e94d568 <unknown>
#15 0x55a02e94d746 <unknown>
#16 0x55a02e95e5f6 <unknown>
#17 0x7ff246ef11f5 <unknown>
[2025-05-07T02:52:13.729+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:52:13.744+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.876 seconds
[2025-05-07T02:52:43.814+0000] {processor.py:186} INFO - Started process (PID=1521) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:52:43.815+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:52:43.816+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:52:43.816+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:52:44.728+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:52:44.727+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b626ac978a <unknown>
#1 0x55b62656c0a0 <unknown>
#2 0x55b6265a61ff <unknown>
#3 0x55b6265a1c0f <unknown>
#4 0x55b6265f1d75 <unknown>
#5 0x55b6265f1296 <unknown>
#6 0x55b6265e3173 <unknown>
#7 0x55b6265afd4b <unknown>
#8 0x55b6265b09b1 <unknown>
#9 0x55b626a8e93b <unknown>
#10 0x55b626a9283a <unknown>
#11 0x55b626a76692 <unknown>
#12 0x55b626a933c4 <unknown>
#13 0x55b626a5b4cf <unknown>
#14 0x55b626ab7568 <unknown>
#15 0x55b626ab7746 <unknown>
#16 0x55b626ac85f6 <unknown>
#17 0x7fa477ec61f5 <unknown>
[2025-05-07T02:52:44.728+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:52:44.744+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.935 seconds
[2025-05-07T02:53:14.940+0000] {processor.py:186} INFO - Started process (PID=1558) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:53:14.941+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:53:14.942+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:53:14.942+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:53:15.913+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:53:15.912+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x555ec16ae78a <unknown>
#1 0x555ec11510a0 <unknown>
#2 0x555ec118b1ff <unknown>
#3 0x555ec1186c0f <unknown>
#4 0x555ec11d6d75 <unknown>
#5 0x555ec11d6296 <unknown>
#6 0x555ec11c8173 <unknown>
#7 0x555ec1194d4b <unknown>
#8 0x555ec11959b1 <unknown>
#9 0x555ec167393b <unknown>
#10 0x555ec167783a <unknown>
#11 0x555ec165b692 <unknown>
#12 0x555ec16783c4 <unknown>
#13 0x555ec16404cf <unknown>
#14 0x555ec169c568 <unknown>
#15 0x555ec169c746 <unknown>
#16 0x555ec16ad5f6 <unknown>
#17 0x7fcacd7341f5 <unknown>
[2025-05-07T02:53:15.913+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:53:15.927+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.994 seconds
[2025-05-07T02:53:46.025+0000] {processor.py:186} INFO - Started process (PID=1594) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:53:46.043+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:53:46.044+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:53:46.044+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:53:46.976+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:53:46.975+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x564c2d2fe78a <unknown>
#1 0x564c2cda10a0 <unknown>
#2 0x564c2cddb1ff <unknown>
#3 0x564c2cdd6c0f <unknown>
#4 0x564c2ce26d75 <unknown>
#5 0x564c2ce26296 <unknown>
#6 0x564c2ce18173 <unknown>
#7 0x564c2cde4d4b <unknown>
#8 0x564c2cde59b1 <unknown>
#9 0x564c2d2c393b <unknown>
#10 0x564c2d2c783a <unknown>
#11 0x564c2d2ab692 <unknown>
#12 0x564c2d2c83c4 <unknown>
#13 0x564c2d2904cf <unknown>
#14 0x564c2d2ec568 <unknown>
#15 0x564c2d2ec746 <unknown>
#16 0x564c2d2fd5f6 <unknown>
#17 0x7fc83983f1f5 <unknown>
[2025-05-07T02:53:46.976+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:53:47.007+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.986 seconds
[2025-05-07T02:54:17.992+0000] {processor.py:186} INFO - Started process (PID=1630) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:54:18.008+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:54:18.009+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:54:18.008+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:54:18.376+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:54:18.375+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559fb5c4e78a <unknown>
#1 0x559fb56f10a0 <unknown>
#2 0x559fb572b1ff <unknown>
#3 0x559fb5726c0f <unknown>
#4 0x559fb5776d75 <unknown>
#5 0x559fb5776296 <unknown>
#6 0x559fb5768173 <unknown>
#7 0x559fb5734d4b <unknown>
#8 0x559fb57359b1 <unknown>
#9 0x559fb5c1393b <unknown>
#10 0x559fb5c1783a <unknown>
#11 0x559fb5bfb692 <unknown>
#12 0x559fb5c183c4 <unknown>
#13 0x559fb5be04cf <unknown>
#14 0x559fb5c3c568 <unknown>
#15 0x559fb5c3c746 <unknown>
#16 0x559fb5c4d5f6 <unknown>
#17 0x7fc3eaf951f5 <unknown>
[2025-05-07T02:54:18.376+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:54:18.391+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.903 seconds
[2025-05-07T02:54:48.778+0000] {processor.py:186} INFO - Started process (PID=1664) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:54:48.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:54:48.779+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:54:48.779+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:54:44.195+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:54:44.194+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556fd82a478a <unknown>
#1 0x556fd7d470a0 <unknown>
#2 0x556fd7d811ff <unknown>
#3 0x556fd7d7cc0f <unknown>
#4 0x556fd7dccd75 <unknown>
#5 0x556fd7dcc296 <unknown>
#6 0x556fd7dbe173 <unknown>
#7 0x556fd7d8ad4b <unknown>
#8 0x556fd7d8b9b1 <unknown>
#9 0x556fd826993b <unknown>
#10 0x556fd826d83a <unknown>
#11 0x556fd8251692 <unknown>
#12 0x556fd826e3c4 <unknown>
#13 0x556fd82364cf <unknown>
#14 0x556fd8292568 <unknown>
#15 0x556fd8292746 <unknown>
#16 0x556fd82a35f6 <unknown>
#17 0x7f03c3b3b1f5 <unknown>
[2025-05-07T02:54:44.195+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:54:44.208+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.990 seconds
[2025-05-07T02:55:18.882+0000] {processor.py:186} INFO - Started process (PID=1700) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:55:18.884+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:55:18.885+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:55:18.885+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:55:14.250+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:55:14.249+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5610f2d0a78a <unknown>
#1 0x5610f27ad0a0 <unknown>
#2 0x5610f27e71ff <unknown>
#3 0x5610f27e2c0f <unknown>
#4 0x5610f2832d75 <unknown>
#5 0x5610f2832296 <unknown>
#6 0x5610f2824173 <unknown>
#7 0x5610f27f0d4b <unknown>
#8 0x5610f27f19b1 <unknown>
#9 0x5610f2ccf93b <unknown>
#10 0x5610f2cd383a <unknown>
#11 0x5610f2cb7692 <unknown>
#12 0x5610f2cd43c4 <unknown>
#13 0x5610f2c9c4cf <unknown>
#14 0x5610f2cf8568 <unknown>
#15 0x5610f2cf8746 <unknown>
#16 0x5610f2d095f6 <unknown>
#17 0x7f1341fb61f5 <unknown>
[2025-05-07T02:55:14.251+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:55:14.265+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.937 seconds
[2025-05-07T02:55:48.815+0000] {processor.py:186} INFO - Started process (PID=1737) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:55:48.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:55:48.818+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:55:48.817+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:55:44.221+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:55:44.220+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55fac64c778a <unknown>
#1 0x55fac5f6a0a0 <unknown>
#2 0x55fac5fa41ff <unknown>
#3 0x55fac5f9fc0f <unknown>
#4 0x55fac5fefd75 <unknown>
#5 0x55fac5fef296 <unknown>
#6 0x55fac5fe1173 <unknown>
#7 0x55fac5fadd4b <unknown>
#8 0x55fac5fae9b1 <unknown>
#9 0x55fac648c93b <unknown>
#10 0x55fac649083a <unknown>
#11 0x55fac6474692 <unknown>
#12 0x55fac64913c4 <unknown>
#13 0x55fac64594cf <unknown>
#14 0x55fac64b5568 <unknown>
#15 0x55fac64b5746 <unknown>
#16 0x55fac64c65f6 <unknown>
#17 0x7f4bd08ef1f5 <unknown>
[2025-05-07T02:55:44.221+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:55:44.238+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.981 seconds
[2025-05-07T02:56:18.884+0000] {processor.py:186} INFO - Started process (PID=1772) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:56:18.885+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:56:18.887+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:56:18.887+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:56:14.409+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:56:14.408+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563fb398878a <unknown>
#1 0x563fb342b0a0 <unknown>
#2 0x563fb34651ff <unknown>
#3 0x563fb3460c0f <unknown>
#4 0x563fb34b0d75 <unknown>
#5 0x563fb34b0296 <unknown>
#6 0x563fb34a2173 <unknown>
#7 0x563fb346ed4b <unknown>
#8 0x563fb346f9b1 <unknown>
#9 0x563fb394d93b <unknown>
#10 0x563fb395183a <unknown>
#11 0x563fb3935692 <unknown>
#12 0x563fb39523c4 <unknown>
#13 0x563fb391a4cf <unknown>
#14 0x563fb3976568 <unknown>
#15 0x563fb3976746 <unknown>
#16 0x563fb39875f6 <unknown>
#17 0x7f60edfa61f5 <unknown>
[2025-05-07T02:56:14.409+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:56:14.421+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.100 seconds
[2025-05-07T02:56:48.948+0000] {processor.py:186} INFO - Started process (PID=1808) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:56:48.948+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:56:48.949+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:56:48.949+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:56:44.414+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:56:44.412+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c4361f578a <unknown>
#1 0x55c435c980a0 <unknown>
#2 0x55c435cd21ff <unknown>
#3 0x55c435ccdc0f <unknown>
#4 0x55c435d1dd75 <unknown>
#5 0x55c435d1d296 <unknown>
#6 0x55c435d0f173 <unknown>
#7 0x55c435cdbd4b <unknown>
#8 0x55c435cdc9b1 <unknown>
#9 0x55c4361ba93b <unknown>
#10 0x55c4361be83a <unknown>
#11 0x55c4361a2692 <unknown>
#12 0x55c4361bf3c4 <unknown>
#13 0x55c4361874cf <unknown>
#14 0x55c4361e3568 <unknown>
#15 0x55c4361e3746 <unknown>
#16 0x55c4361f45f6 <unknown>
#17 0x7f52a49d21f5 <unknown>
[2025-05-07T02:56:44.415+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:56:44.433+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.045 seconds
[2025-05-07T02:57:19.021+0000] {processor.py:186} INFO - Started process (PID=1844) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:57:19.021+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:57:19.022+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:57:19.022+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:57:14.443+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:57:14.442+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b162fa078a <unknown>
#1 0x55b162a430a0 <unknown>
#2 0x55b162a7d1ff <unknown>
#3 0x55b162a78c0f <unknown>
#4 0x55b162ac8d75 <unknown>
#5 0x55b162ac8296 <unknown>
#6 0x55b162aba173 <unknown>
#7 0x55b162a86d4b <unknown>
#8 0x55b162a879b1 <unknown>
#9 0x55b162f6593b <unknown>
#10 0x55b162f6983a <unknown>
#11 0x55b162f4d692 <unknown>
#12 0x55b162f6a3c4 <unknown>
#13 0x55b162f324cf <unknown>
#14 0x55b162f8e568 <unknown>
#15 0x55b162f8e746 <unknown>
#16 0x55b162f9f5f6 <unknown>
#17 0x7f6f597401f5 <unknown>
[2025-05-07T02:57:14.444+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:57:14.459+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.996 seconds
[2025-05-07T02:57:45.140+0000] {processor.py:186} INFO - Started process (PID=1886) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:57:45.144+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:57:45.145+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:57:45.145+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:57:46.062+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:57:46.061+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556cf03ef78a <unknown>
#1 0x556cefe920a0 <unknown>
#2 0x556cefecc1ff <unknown>
#3 0x556cefec7c0f <unknown>
#4 0x556ceff17d75 <unknown>
#5 0x556ceff17296 <unknown>
#6 0x556ceff09173 <unknown>
#7 0x556cefed5d4b <unknown>
#8 0x556cefed69b1 <unknown>
#9 0x556cf03b493b <unknown>
#10 0x556cf03b883a <unknown>
#11 0x556cf039c692 <unknown>
#12 0x556cf03b93c4 <unknown>
#13 0x556cf03814cf <unknown>
#14 0x556cf03dd568 <unknown>
#15 0x556cf03dd746 <unknown>
#16 0x556cf03ee5f6 <unknown>
#17 0x7f4be9a6d1f5 <unknown>
[2025-05-07T02:57:46.062+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:57:46.077+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.943 seconds
[2025-05-07T02:58:16.824+0000] {processor.py:186} INFO - Started process (PID=1922) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:58:16.825+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:58:16.826+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:58:16.825+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:58:17.844+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:58:17.843+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561ecf89d78a <unknown>
#1 0x561ecf3400a0 <unknown>
#2 0x561ecf37a1ff <unknown>
#3 0x561ecf375c0f <unknown>
#4 0x561ecf3c5d75 <unknown>
#5 0x561ecf3c5296 <unknown>
#6 0x561ecf3b7173 <unknown>
#7 0x561ecf383d4b <unknown>
#8 0x561ecf3849b1 <unknown>
#9 0x561ecf86293b <unknown>
#10 0x561ecf86683a <unknown>
#11 0x561ecf84a692 <unknown>
#12 0x561ecf8673c4 <unknown>
#13 0x561ecf82f4cf <unknown>
#14 0x561ecf88b568 <unknown>
#15 0x561ecf88b746 <unknown>
#16 0x561ecf89c5f6 <unknown>
#17 0x7fcca896e1f5 <unknown>
[2025-05-07T02:58:17.845+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:58:17.856+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.036 seconds
[2025-05-07T02:58:49.093+0000] {processor.py:186} INFO - Started process (PID=1958) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:58:49.094+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:58:49.095+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:58:49.094+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:58:44.645+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:58:44.644+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556a5e0cd78a <unknown>
#1 0x556a5db700a0 <unknown>
#2 0x556a5dbaa1ff <unknown>
#3 0x556a5dba5c0f <unknown>
#4 0x556a5dbf5d75 <unknown>
#5 0x556a5dbf5296 <unknown>
#6 0x556a5dbe7173 <unknown>
#7 0x556a5dbb3d4b <unknown>
#8 0x556a5dbb49b1 <unknown>
#9 0x556a5e09293b <unknown>
#10 0x556a5e09683a <unknown>
#11 0x556a5e07a692 <unknown>
#12 0x556a5e0973c4 <unknown>
#13 0x556a5e05f4cf <unknown>
#14 0x556a5e0bb568 <unknown>
#15 0x556a5e0bb746 <unknown>
#16 0x556a5e0cc5f6 <unknown>
#17 0x7f922e71e1f5 <unknown>
[2025-05-07T02:58:44.646+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:58:44.659+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.130 seconds
[2025-05-07T02:59:15.518+0000] {processor.py:186} INFO - Started process (PID=1994) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:59:15.519+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:59:15.520+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:59:15.520+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:59:16.535+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:59:16.534+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561997aa978a <unknown>
#1 0x56199754c0a0 <unknown>
#2 0x5619975861ff <unknown>
#3 0x561997581c0f <unknown>
#4 0x5619975d1d75 <unknown>
#5 0x5619975d1296 <unknown>
#6 0x5619975c3173 <unknown>
#7 0x56199758fd4b <unknown>
#8 0x5619975909b1 <unknown>
#9 0x561997a6e93b <unknown>
#10 0x561997a7283a <unknown>
#11 0x561997a56692 <unknown>
#12 0x561997a733c4 <unknown>
#13 0x561997a3b4cf <unknown>
#14 0x561997a97568 <unknown>
#15 0x561997a97746 <unknown>
#16 0x561997aa85f6 <unknown>
#17 0x7fc9c8d831f5 <unknown>
[2025-05-07T02:59:16.535+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:59:16.557+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.045 seconds
[2025-05-07T02:59:47.219+0000] {processor.py:186} INFO - Started process (PID=2030) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T02:59:47.220+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T02:59:47.221+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:59:47.221+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T02:59:48.171+0000] {logging_mixin.py:190} INFO - [2025-05-07T02:59:48.170+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x558bf7c0f78a <unknown>
#1 0x558bf76b20a0 <unknown>
#2 0x558bf76ec1ff <unknown>
#3 0x558bf76e7c0f <unknown>
#4 0x558bf7737d75 <unknown>
#5 0x558bf7737296 <unknown>
#6 0x558bf7729173 <unknown>
#7 0x558bf76f5d4b <unknown>
#8 0x558bf76f69b1 <unknown>
#9 0x558bf7bd493b <unknown>
#10 0x558bf7bd883a <unknown>
#11 0x558bf7bbc692 <unknown>
#12 0x558bf7bd93c4 <unknown>
#13 0x558bf7ba14cf <unknown>
#14 0x558bf7bfd568 <unknown>
#15 0x558bf7bfd746 <unknown>
#16 0x558bf7c0e5f6 <unknown>
#17 0x7fbfaad361f5 <unknown>
[2025-05-07T02:59:48.172+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T02:59:48.188+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.974 seconds
[2025-05-07T03:00:19.281+0000] {processor.py:186} INFO - Started process (PID=2063) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:00:19.282+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:00:19.283+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:00:19.283+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:00:14.696+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:00:14.696+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55600165478a <unknown>
#1 0x5560010f70a0 <unknown>
#2 0x5560011311ff <unknown>
#3 0x55600112cc0f <unknown>
#4 0x55600117cd75 <unknown>
#5 0x55600117c296 <unknown>
#6 0x55600116e173 <unknown>
#7 0x55600113ad4b <unknown>
#8 0x55600113b9b1 <unknown>
#9 0x55600161993b <unknown>
#10 0x55600161d83a <unknown>
#11 0x556001601692 <unknown>
#12 0x55600161e3c4 <unknown>
#13 0x5560015e64cf <unknown>
#14 0x556001642568 <unknown>
#15 0x556001642746 <unknown>
#16 0x5560016535f6 <unknown>
#17 0x7f25fce3f1f5 <unknown>
[2025-05-07T03:00:14.697+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:00:14.904+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.187 seconds
[2025-05-07T03:00:45.509+0000] {processor.py:186} INFO - Started process (PID=2099) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:00:45.512+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:00:45.513+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:00:45.513+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:00:46.518+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:00:46.517+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a7af52a78a <unknown>
#1 0x55a7aefcd0a0 <unknown>
#2 0x55a7af0071ff <unknown>
#3 0x55a7af002c0f <unknown>
#4 0x55a7af052d75 <unknown>
#5 0x55a7af052296 <unknown>
#6 0x55a7af044173 <unknown>
#7 0x55a7af010d4b <unknown>
#8 0x55a7af0119b1 <unknown>
#9 0x55a7af4ef93b <unknown>
#10 0x55a7af4f383a <unknown>
#11 0x55a7af4d7692 <unknown>
#12 0x55a7af4f43c4 <unknown>
#13 0x55a7af4bc4cf <unknown>
#14 0x55a7af518568 <unknown>
#15 0x55a7af518746 <unknown>
#16 0x55a7af5295f6 <unknown>
#17 0x7f457b36f1f5 <unknown>
[2025-05-07T03:00:46.518+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:00:46.533+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.029 seconds
[2025-05-07T03:01:17.153+0000] {processor.py:186} INFO - Started process (PID=2135) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:01:17.154+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:01:17.155+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:01:17.155+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:01:18.117+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:01:18.116+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b3bb05278a <unknown>
#1 0x55b3baaf50a0 <unknown>
#2 0x55b3bab2f1ff <unknown>
#3 0x55b3bab2ac0f <unknown>
#4 0x55b3bab7ad75 <unknown>
#5 0x55b3bab7a296 <unknown>
#6 0x55b3bab6c173 <unknown>
#7 0x55b3bab38d4b <unknown>
#8 0x55b3bab399b1 <unknown>
#9 0x55b3bb01793b <unknown>
#10 0x55b3bb01b83a <unknown>
#11 0x55b3bafff692 <unknown>
#12 0x55b3bb01c3c4 <unknown>
#13 0x55b3bafe44cf <unknown>
#14 0x55b3bb040568 <unknown>
#15 0x55b3bb040746 <unknown>
#16 0x55b3bb0515f6 <unknown>
#17 0x7f96948111f5 <unknown>
[2025-05-07T03:01:18.117+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:01:18.134+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.987 seconds
[2025-05-07T03:01:49.501+0000] {processor.py:186} INFO - Started process (PID=2171) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:01:49.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:01:49.503+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:01:49.502+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:01:44.907+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:01:44.906+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5635e6e6a78a <unknown>
#1 0x5635e690d0a0 <unknown>
#2 0x5635e69471ff <unknown>
#3 0x5635e6942c0f <unknown>
#4 0x5635e6992d75 <unknown>
#5 0x5635e6992296 <unknown>
#6 0x5635e6984173 <unknown>
#7 0x5635e6950d4b <unknown>
#8 0x5635e69519b1 <unknown>
#9 0x5635e6e2f93b <unknown>
#10 0x5635e6e3383a <unknown>
#11 0x5635e6e17692 <unknown>
#12 0x5635e6e343c4 <unknown>
#13 0x5635e6dfc4cf <unknown>
#14 0x5635e6e58568 <unknown>
#15 0x5635e6e58746 <unknown>
#16 0x5635e6e695f6 <unknown>
#17 0x7f61155a91f5 <unknown>
[2025-05-07T03:01:44.907+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:01:44.925+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.983 seconds
[2025-05-07T03:02:15.595+0000] {processor.py:186} INFO - Started process (PID=2213) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:02:15.599+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:02:15.600+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:02:15.600+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:02:16.563+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:02:16.562+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56108a77378a <unknown>
#1 0x56108a2160a0 <unknown>
#2 0x56108a2501ff <unknown>
#3 0x56108a24bc0f <unknown>
#4 0x56108a29bd75 <unknown>
#5 0x56108a29b296 <unknown>
#6 0x56108a28d173 <unknown>
#7 0x56108a259d4b <unknown>
#8 0x56108a25a9b1 <unknown>
#9 0x56108a73893b <unknown>
#10 0x56108a73c83a <unknown>
#11 0x56108a720692 <unknown>
#12 0x56108a73d3c4 <unknown>
#13 0x56108a7054cf <unknown>
#14 0x56108a761568 <unknown>
#15 0x56108a761746 <unknown>
#16 0x56108a7725f6 <unknown>
#17 0x7f234d6f21f5 <unknown>
[2025-05-07T03:02:16.564+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:02:16.685+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.095 seconds
[2025-05-07T03:02:47.420+0000] {processor.py:186} INFO - Started process (PID=2249) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:02:47.421+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:02:47.422+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:02:47.422+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:02:48.512+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:02:48.511+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562b7a21c78a <unknown>
#1 0x562b79cbf0a0 <unknown>
#2 0x562b79cf91ff <unknown>
#3 0x562b79cf4c0f <unknown>
#4 0x562b79d44d75 <unknown>
#5 0x562b79d44296 <unknown>
#6 0x562b79d36173 <unknown>
#7 0x562b79d02d4b <unknown>
#8 0x562b79d039b1 <unknown>
#9 0x562b7a1e193b <unknown>
#10 0x562b7a1e583a <unknown>
#11 0x562b7a1c9692 <unknown>
#12 0x562b7a1e63c4 <unknown>
#13 0x562b7a1ae4cf <unknown>
#14 0x562b7a20a568 <unknown>
#15 0x562b7a20a746 <unknown>
#16 0x562b7a21b5f6 <unknown>
#17 0x7f36134581f5 <unknown>
[2025-05-07T03:02:48.513+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:02:48.536+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.121 seconds
[2025-05-07T03:03:18.584+0000] {processor.py:186} INFO - Started process (PID=2285) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:03:18.584+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:03:18.585+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:03:18.585+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:03:24.724+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:03:24.723+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55eb4b26b78a <unknown>
#1 0x55eb4ad0e0a0 <unknown>
#2 0x55eb4ad481ff <unknown>
#3 0x55eb4ad43c0f <unknown>
#4 0x55eb4ad93d75 <unknown>
#5 0x55eb4ad93296 <unknown>
#6 0x55eb4ad85173 <unknown>
#7 0x55eb4ad51d4b <unknown>
#8 0x55eb4ad529b1 <unknown>
#9 0x55eb4b23093b <unknown>
#10 0x55eb4b23483a <unknown>
#11 0x55eb4b218692 <unknown>
#12 0x55eb4b2353c4 <unknown>
#13 0x55eb4b1fd4cf <unknown>
#14 0x55eb4b259568 <unknown>
#15 0x55eb4b259746 <unknown>
#16 0x55eb4b26a5f6 <unknown>
#17 0x7fc6371831f5 <unknown>
[2025-05-07T03:03:24.724+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:03:24.736+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.101 seconds
[2025-05-07T03:03:59.880+0000] {processor.py:186} INFO - Started process (PID=2321) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:03:59.882+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:03:59.882+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:03:59.882+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:03:55.380+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:03:55.379+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e469cc378a <unknown>
#1 0x55e4697660a0 <unknown>
#2 0x55e4697a01ff <unknown>
#3 0x55e46979bc0f <unknown>
#4 0x55e4697ebd75 <unknown>
#5 0x55e4697eb296 <unknown>
#6 0x55e4697dd173 <unknown>
#7 0x55e4697a9d4b <unknown>
#8 0x55e4697aa9b1 <unknown>
#9 0x55e469c8893b <unknown>
#10 0x55e469c8c83a <unknown>
#11 0x55e469c70692 <unknown>
#12 0x55e469c8d3c4 <unknown>
#13 0x55e469c554cf <unknown>
#14 0x55e469cb1568 <unknown>
#15 0x55e469cb1746 <unknown>
#16 0x55e469cc25f6 <unknown>
#17 0x7ff77004c1f5 <unknown>
[2025-05-07T03:03:55.381+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:03:55.394+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.075 seconds
[2025-05-07T03:04:25.844+0000] {processor.py:186} INFO - Started process (PID=2358) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:04:25.846+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:04:25.847+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:04:25.847+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:04:26.978+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:04:26.977+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562c06b4378a <unknown>
#1 0x562c065e60a0 <unknown>
#2 0x562c066201ff <unknown>
#3 0x562c0661bc0f <unknown>
#4 0x562c0666bd75 <unknown>
#5 0x562c0666b296 <unknown>
#6 0x562c0665d173 <unknown>
#7 0x562c06629d4b <unknown>
#8 0x562c0662a9b1 <unknown>
#9 0x562c06b0893b <unknown>
#10 0x562c06b0c83a <unknown>
#11 0x562c06af0692 <unknown>
#12 0x562c06b0d3c4 <unknown>
#13 0x562c06ad54cf <unknown>
#14 0x562c06b31568 <unknown>
#15 0x562c06b31746 <unknown>
#16 0x562c06b425f6 <unknown>
#17 0x7f61d03831f5 <unknown>
[2025-05-07T03:04:26.979+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:04:26.990+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.151 seconds
[2025-05-07T03:04:57.200+0000] {processor.py:186} INFO - Started process (PID=2394) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:04:57.201+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:04:57.202+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:04:57.202+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:04:58.323+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:04:58.322+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 17, in __init__
    self._base_url = base_url.format(self._user_id)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557e2f4c578a <unknown>
#1 0x557e2ef680a0 <unknown>
#2 0x557e2efa21ff <unknown>
#3 0x557e2ef9dc0f <unknown>
#4 0x557e2efedd75 <unknown>
#5 0x557e2efed296 <unknown>
#6 0x557e2efdf173 <unknown>
#7 0x557e2efabd4b <unknown>
#8 0x557e2efac9b1 <unknown>
#9 0x557e2f48a93b <unknown>
#10 0x557e2f48e83a <unknown>
#11 0x557e2f472692 <unknown>
#12 0x557e2f48f3c4 <unknown>
#13 0x557e2f4574cf <unknown>
#14 0x557e2f4b3568 <unknown>
#15 0x557e2f4b3746 <unknown>
#16 0x557e2f4c45f6 <unknown>
#17 0x7f9c998df1f5 <unknown>
[2025-05-07T03:04:58.324+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:04:58.338+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.142 seconds
[2025-05-07T03:05:24.499+0000] {processor.py:186} INFO - Started process (PID=2429) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:05:24.499+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:05:24.500+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:05:24.500+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:05:25.412+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:05:25.411+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 20, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55aafe7db78a <unknown>
#1 0x55aafe27e0a0 <unknown>
#2 0x55aafe2b81ff <unknown>
#3 0x55aafe2b3c0f <unknown>
#4 0x55aafe303d75 <unknown>
#5 0x55aafe303296 <unknown>
#6 0x55aafe2f5173 <unknown>
#7 0x55aafe2c1d4b <unknown>
#8 0x55aafe2c29b1 <unknown>
#9 0x55aafe7a093b <unknown>
#10 0x55aafe7a483a <unknown>
#11 0x55aafe788692 <unknown>
#12 0x55aafe7a53c4 <unknown>
#13 0x55aafe76d4cf <unknown>
#14 0x55aafe7c9568 <unknown>
#15 0x55aafe7c9746 <unknown>
#16 0x55aafe7da5f6 <unknown>
#17 0x7f0f7ba361f5 <unknown>
[2025-05-07T03:05:25.413+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:05:25.564+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.071 seconds
[2025-05-07T03:05:54.579+0000] {processor.py:186} INFO - Started process (PID=2471) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:05:54.580+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:05:54.581+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:05:54.581+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:05:55.548+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:05:55.547+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 23, in __init__
    driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562a6817678a <unknown>
#1 0x562a67c190a0 <unknown>
#2 0x562a67c531ff <unknown>
#3 0x562a67c4ec0f <unknown>
#4 0x562a67c9ed75 <unknown>
#5 0x562a67c9e296 <unknown>
#6 0x562a67c90173 <unknown>
#7 0x562a67c5cd4b <unknown>
#8 0x562a67c5d9b1 <unknown>
#9 0x562a6813b93b <unknown>
#10 0x562a6813f83a <unknown>
#11 0x562a68123692 <unknown>
#12 0x562a681403c4 <unknown>
#13 0x562a681084cf <unknown>
#14 0x562a68164568 <unknown>
#15 0x562a68164746 <unknown>
#16 0x562a681755f6 <unknown>
#17 0x7f80ed40e1f5 <unknown>
[2025-05-07T03:05:55.549+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:05:55.564+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.989 seconds
[2025-05-07T03:06:24.751+0000] {processor.py:186} INFO - Started process (PID=2507) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:06:24.752+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:06:24.753+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:06:24.752+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:06:24.839+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:06:25.028+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:06:25.028+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T03:06:25.037+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:06:25.037+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T03:06:25.055+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.309 seconds
[2025-05-07T03:06:46.359+0000] {processor.py:186} INFO - Started process (PID=2513) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:06:46.360+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:06:46.361+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:06:46.360+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:06:46.828+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:06:46.827+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:06:46.828+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:06:46.842+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.488 seconds
[2025-05-07T03:07:16.962+0000] {processor.py:186} INFO - Started process (PID=2545) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:07:16.965+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:07:16.966+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:07:16.966+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:07:17.390+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:07:17.390+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:07:17.390+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:07:17.404+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.445 seconds
[2025-05-07T03:07:47.768+0000] {processor.py:186} INFO - Started process (PID=2572) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:07:47.769+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:07:47.770+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:07:47.769+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:07:48.243+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:07:48.242+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:07:48.243+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:07:48.258+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.494 seconds
[2025-05-07T03:08:18.972+0000] {processor.py:186} INFO - Started process (PID=2599) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:08:18.973+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:08:18.974+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:08:18.973+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:08:19.419+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:08:19.419+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:08:19.420+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:08:19.434+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.466 seconds
[2025-05-07T03:08:50.218+0000] {processor.py:186} INFO - Started process (PID=2626) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:08:50.226+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:08:50.226+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:08:50.226+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:08:45.079+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:08:45.078+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:08:45.079+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:08:45.103+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.457 seconds
[2025-05-07T03:09:15.338+0000] {processor.py:186} INFO - Started process (PID=2653) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:09:15.342+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:09:15.343+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:09:15.343+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:09:15.765+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:09:15.764+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:09:15.765+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:09:15.778+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.444 seconds
[2025-05-07T03:09:46.469+0000] {processor.py:186} INFO - Started process (PID=2680) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:09:46.470+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:09:46.470+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:09:46.470+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:09:46.876+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:09:46.876+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:09:46.877+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:09:46.889+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.425 seconds
[2025-05-07T03:10:28.187+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:10:28.188+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:10:28.190+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:10:28.190+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:10:28.611+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:10:28.610+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:10:28.611+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:10:28.626+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.446 seconds
[2025-05-07T03:10:59.189+0000] {processor.py:186} INFO - Started process (PID=82) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:10:59.191+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:10:59.193+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:10:59.193+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:10:59.849+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:10:59.849+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:10:59.850+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:10:59.864+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.681 seconds
[2025-05-07T03:11:30.205+0000] {processor.py:186} INFO - Started process (PID=115) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:11:30.210+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:11:30.214+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:11:30.214+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:11:30.078+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:11:30.077+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:11:30.078+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:11:30.093+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.395 seconds
[2025-05-07T03:12:00.282+0000] {processor.py:186} INFO - Started process (PID=142) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:12:05.374+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:12:05.376+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:12:05.375+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:12:00.214+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:12:00.214+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:12:00.214+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:12:00.237+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.455 seconds
[2025-05-07T03:12:25.163+0000] {processor.py:186} INFO - Started process (PID=169) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:12:25.163+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:12:25.165+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:12:25.164+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:12:25.584+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:12:25.583+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:12:25.584+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:12:25.604+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.446 seconds
[2025-05-07T03:13:26.686+0000] {processor.py:186} INFO - Started process (PID=49) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:13:26.687+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:13:26.690+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:13:26.690+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:13:27.194+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:13:27.193+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:13:27.194+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:13:27.210+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.531 seconds
[2025-05-07T03:14:00.788+0000] {processor.py:186} INFO - Started process (PID=82) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:14:00.800+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:14:00.802+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:14:00.802+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:13:55.689+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:13:55.689+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:13:55.689+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:13:55.702+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.501 seconds
[2025-05-07T03:14:25.784+0000] {processor.py:186} INFO - Started process (PID=103) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:14:25.786+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:14:25.788+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:14:25.788+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:14:20.649+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:14:20.649+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:14:20.650+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:14:20.667+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.465 seconds
[2025-05-07T03:14:55.747+0000] {processor.py:186} INFO - Started process (PID=138) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:14:55.750+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:14:55.751+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:14:55.751+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:14:50.681+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:14:50.680+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:14:50.681+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:14:50.694+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.537 seconds
[2025-05-07T03:15:25.835+0000] {processor.py:186} INFO - Started process (PID=163) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:15:25.835+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:15:25.837+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:15:25.837+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:15:20.763+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:15:20.762+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:15:20.763+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:15:20.775+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.526 seconds
[2025-05-07T03:15:55.930+0000] {processor.py:186} INFO - Started process (PID=190) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:15:55.946+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:15:55.948+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:15:55.947+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:15:50.795+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:15:50.795+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:15:50.795+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:15:50.809+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.466 seconds
[2025-05-07T03:16:25.821+0000] {processor.py:186} INFO - Started process (PID=221) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:16:25.822+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:16:25.824+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:16:25.823+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:16:20.578+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:16:20.577+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:16:20.578+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:16:20.597+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.362 seconds
[2025-05-07T03:16:55.899+0000] {processor.py:186} INFO - Started process (PID=245) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:16:55.901+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:16:55.903+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:16:55.902+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:16:50.653+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:16:50.653+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:16:50.654+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:16:50.757+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.443 seconds
[2025-05-07T03:17:25.933+0000] {processor.py:186} INFO - Started process (PID=272) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:17:25.935+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:17:25.937+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:17:25.936+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:17:20.798+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:17:20.797+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:17:20.798+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:17:20.811+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.465 seconds
[2025-05-07T03:17:50.913+0000] {processor.py:186} INFO - Started process (PID=298) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:17:50.915+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:17:50.918+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:17:50.917+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:17:45.814+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:17:45.814+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:17:45.815+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:17:45.827+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.500 seconds
[2025-05-07T03:18:16.154+0000] {processor.py:186} INFO - Started process (PID=325) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:18:16.155+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:18:16.156+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:18:16.156+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:18:11.002+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:18:11.002+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:18:11.003+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:18:11.017+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.451 seconds
[2025-05-07T03:29:13.328+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:29:13.329+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:29:13.332+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:29:13.331+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:29:13.833+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:29:13.832+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:29:13.834+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:29:13.853+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.532 seconds
[2025-05-07T03:29:47.105+0000] {processor.py:186} INFO - Started process (PID=82) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:29:47.106+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:29:47.108+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:29:47.108+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:29:42.019+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:29:42.019+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:29:42.020+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:29:42.034+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.525 seconds
[2025-05-07T03:30:17.340+0000] {processor.py:186} INFO - Started process (PID=109) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:30:17.342+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:30:17.344+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:30:17.343+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:30:12.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:30:12.092+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:30:12.093+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:30:12.109+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.367 seconds
[2025-05-07T03:30:42.176+0000] {processor.py:186} INFO - Started process (PID=146) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:30:42.177+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:30:42.179+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:30:42.179+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:30:42.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:30:42.647+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:30:42.648+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:30:42.660+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.488 seconds
[2025-05-07T03:31:17.251+0000] {processor.py:186} INFO - Started process (PID=176) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:31:17.253+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:31:17.254+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:31:17.254+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:31:12.090+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:31:12.090+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:31:12.090+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:31:12.104+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.443 seconds
[2025-05-07T03:31:42.124+0000] {processor.py:186} INFO - Started process (PID=197) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:31:42.124+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:31:42.126+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:31:42.126+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:31:42.057+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:31:42.056+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:31:42.057+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:31:42.070+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.447 seconds
[2025-05-07T03:33:58.184+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:33:58.185+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:33:58.188+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:33:58.188+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:33:58.646+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:33:58.645+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:33:58.646+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:33:58.664+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.486 seconds
[2025-05-07T03:34:29.362+0000] {processor.py:186} INFO - Started process (PID=82) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:34:29.364+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:34:29.366+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:34:29.366+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:34:29.795+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:34:29.794+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:34:29.795+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:34:29.808+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.452 seconds
[2025-05-07T03:35:02.746+0000] {processor.py:186} INFO - Started process (PID=115) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:35:02.748+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:35:02.750+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:35:02.749+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:34:57.578+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:34:57.577+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:34:57.578+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:34:57.593+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.443 seconds
[2025-05-07T03:35:32.622+0000] {processor.py:186} INFO - Started process (PID=145) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:35:32.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:35:32.625+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:35:32.625+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:35:27.482+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:35:27.481+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:35:27.482+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:35:27.494+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.478 seconds
[2025-05-07T03:35:57.810+0000] {processor.py:186} INFO - Started process (PID=169) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:35:57.826+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:35:57.828+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:35:57.827+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:35:52.660+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:35:52.659+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:35:52.660+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:35:52.674+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.460 seconds
[2025-05-07T03:36:27.721+0000] {processor.py:186} INFO - Started process (PID=196) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:36:27.726+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:36:27.728+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:36:27.727+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:36:22.560+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:36:22.560+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:36:22.560+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:36:22.574+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.451 seconds
[2025-05-07T03:36:57.865+0000] {processor.py:186} INFO - Started process (PID=223) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:36:57.866+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:36:57.867+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:36:57.867+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:36:52.636+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:36:52.635+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:36:52.636+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:36:52.658+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.386 seconds
[2025-05-07T03:37:27.786+0000] {processor.py:186} INFO - Started process (PID=250) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:37:27.787+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:37:27.789+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:37:27.789+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:37:22.607+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:37:22.606+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 28, in <module>
    scrape_task = x_login(X_USERNAME=Config.X_USERNAME,
  File "/opt/airflow/dags/tasks/x_login.py", line 9, in x_login
    browser = playwright.chromium.launch()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/sync_api/_generated.py", line 14494, in launch
    self._sync(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_sync_base.py", line 115, in _sync
    return task.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_browser_type.py", line 97, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
  File "/home/airflow/.local/lib/python3.10/site-packages/playwright/_impl/_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /home/airflow/.cache/ms-playwright/chromium_headless_shell-1169/chrome-linux/headless_shell
╔════════════════════════════════════════════════════════════╗
║ Looks like Playwright was just installed or updated.       ║
║ Please run the following command to download new browsers: ║
║                                                            ║
║     playwright install                                     ║
║                                                            ║
║ <3 Playwright Team                                         ║
╚════════════════════════════════════════════════════════════╝
[2025-05-07T03:37:22.607+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:37:22.735+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.545 seconds
[2025-05-07T03:44:00.803+0000] {processor.py:186} INFO - Started process (PID=56) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:44:00.804+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:44:00.806+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:00.806+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:44:07.855+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T03:44:08.183+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:44:08.252+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:08.252+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:x_login_dag
[2025-05-07T03:44:08.259+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:08.259+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:x_login_dag
[2025-05-07T03:44:08.264+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:08.264+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:x_login_dag
[2025-05-07T03:44:08.276+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:08.275+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T03:44:08.282+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:08.282+0000] {dag.py:3234} INFO - Creating ORM DAG for x_login_dag
[2025-05-07T03:44:08.283+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:08.283+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T03:44:08.284+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:08.284+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T03:44:08.298+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.997 seconds
[2025-05-07T03:44:39.172+0000] {processor.py:186} INFO - Started process (PID=189) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:44:39.173+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:44:39.174+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:39.174+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:44:45.994+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T03:44:46.280+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:44:46.299+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:46.299+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T03:44:46.307+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:46.307+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T03:44:46.309+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:44:46.309+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T03:44:46.319+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.653 seconds
[2025-05-07T03:45:18.496+0000] {processor.py:186} INFO - Started process (PID=315) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:45:18.496+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:45:18.498+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:18.498+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:45:19.143+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T03:45:19.435+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:45:19.451+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:19.450+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T03:45:19.458+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:19.458+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T03:45:19.460+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:19.460+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T03:45:19.470+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.079 seconds
[2025-05-07T03:45:49.688+0000] {processor.py:186} INFO - Started process (PID=444) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:45:49.689+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:45:49.691+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:49.691+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:45:55.782+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T03:45:56.082+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:45:56.100+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:56.100+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T03:45:56.108+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:56.108+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T03:45:56.110+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:56.110+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T03:45:56.120+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.944 seconds
[2025-05-07T03:45:57.044+0000] {processor.py:186} INFO - Started process (PID=576) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:45:57.045+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:45:57.047+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:45:57.046+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:46:19.660+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:46:19.656+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5586ef1dd78a <unknown>
#1 0x5586eec800a0 <unknown>
#2 0x5586eecba1ff <unknown>
#3 0x5586eecb5c0f <unknown>
#4 0x5586eed05d75 <unknown>
#5 0x5586eed05296 <unknown>
#6 0x5586eecf7173 <unknown>
#7 0x5586eecc3d4b <unknown>
#8 0x5586eecc49b1 <unknown>
#9 0x5586ef1a293b <unknown>
#10 0x5586ef1a683a <unknown>
#11 0x5586ef18a692 <unknown>
#12 0x5586ef1a73c4 <unknown>
#13 0x5586ef16f4cf <unknown>
#14 0x5586ef1cb568 <unknown>
#15 0x5586ef1cb746 <unknown>
#16 0x5586ef1dc5f6 <unknown>
#17 0x7f93d9aaa134 <unknown>
[2025-05-07T03:46:19.660+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:46:19.676+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 25.140 seconds
[2025-05-07T03:46:53.765+0000] {processor.py:186} INFO - Started process (PID=704) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:46:53.766+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:46:53.768+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:46:53.768+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:46:49.280+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:46:49.279+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x560940b8b78a <unknown>
#1 0x56094062e0a0 <unknown>
#2 0x5609406681ff <unknown>
#3 0x560940663c0f <unknown>
#4 0x5609406b3d75 <unknown>
#5 0x5609406b3296 <unknown>
#6 0x5609406a5173 <unknown>
#7 0x560940671d4b <unknown>
#8 0x5609406729b1 <unknown>
#9 0x560940b5093b <unknown>
#10 0x560940b5483a <unknown>
#11 0x560940b38692 <unknown>
#12 0x560940b553c4 <unknown>
#13 0x560940b1d4cf <unknown>
#14 0x560940b79568 <unknown>
#15 0x560940b79746 <unknown>
#16 0x560940b8a5f6 <unknown>
#17 0x7f520f230134 <unknown>
[2025-05-07T03:46:49.280+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:46:49.308+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.141 seconds
[2025-05-07T03:47:19.439+0000] {processor.py:186} INFO - Started process (PID=741) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:47:19.440+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:47:19.442+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:47:19.441+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:47:20.559+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:47:20.558+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ddd369c78a <unknown>
#1 0x55ddd313f0a0 <unknown>
#2 0x55ddd31791ff <unknown>
#3 0x55ddd3174c0f <unknown>
#4 0x55ddd31c4d75 <unknown>
#5 0x55ddd31c4296 <unknown>
#6 0x55ddd31b6173 <unknown>
#7 0x55ddd3182d4b <unknown>
#8 0x55ddd31839b1 <unknown>
#9 0x55ddd366193b <unknown>
#10 0x55ddd366583a <unknown>
#11 0x55ddd3649692 <unknown>
#12 0x55ddd36663c4 <unknown>
#13 0x55ddd362e4cf <unknown>
#14 0x55ddd368a568 <unknown>
#15 0x55ddd368a746 <unknown>
#16 0x55ddd369b5f6 <unknown>
#17 0x7f459b154134 <unknown>
[2025-05-07T03:47:20.560+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:47:20.573+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.138 seconds
[2025-05-07T03:47:50.746+0000] {processor.py:186} INFO - Started process (PID=777) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:47:50.749+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:47:50.750+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:47:50.750+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:47:51.787+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:47:51.786+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ca1687178a <unknown>
#1 0x55ca163140a0 <unknown>
#2 0x55ca1634e1ff <unknown>
#3 0x55ca16349c0f <unknown>
#4 0x55ca16399d75 <unknown>
#5 0x55ca16399296 <unknown>
#6 0x55ca1638b173 <unknown>
#7 0x55ca16357d4b <unknown>
#8 0x55ca163589b1 <unknown>
#9 0x55ca1683693b <unknown>
#10 0x55ca1683a83a <unknown>
#11 0x55ca1681e692 <unknown>
#12 0x55ca1683b3c4 <unknown>
#13 0x55ca168034cf <unknown>
#14 0x55ca1685f568 <unknown>
#15 0x55ca1685f746 <unknown>
#16 0x55ca168705f6 <unknown>
#17 0x7f09408db134 <unknown>
[2025-05-07T03:47:51.788+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:47:51.801+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.059 seconds
[2025-05-07T03:48:22.451+0000] {processor.py:186} INFO - Started process (PID=813) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:48:22.453+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:48:22.454+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:48:22.454+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:48:23.428+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:48:23.427+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5557e1fa178a <unknown>
#1 0x5557e1a440a0 <unknown>
#2 0x5557e1a7e1ff <unknown>
#3 0x5557e1a79c0f <unknown>
#4 0x5557e1ac9d75 <unknown>
#5 0x5557e1ac9296 <unknown>
#6 0x5557e1abb173 <unknown>
#7 0x5557e1a87d4b <unknown>
#8 0x5557e1a889b1 <unknown>
#9 0x5557e1f6693b <unknown>
#10 0x5557e1f6a83a <unknown>
#11 0x5557e1f4e692 <unknown>
#12 0x5557e1f6b3c4 <unknown>
#13 0x5557e1f334cf <unknown>
#14 0x5557e1f8f568 <unknown>
#15 0x5557e1f8f746 <unknown>
#16 0x5557e1fa05f6 <unknown>
#17 0x7f00c3fb1134 <unknown>
[2025-05-07T03:48:23.429+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:48:23.442+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.996 seconds
[2025-05-07T03:48:53.914+0000] {processor.py:186} INFO - Started process (PID=849) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:48:53.915+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:48:53.916+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:48:53.916+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:48:54.910+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:48:54.909+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5642a97b278a <unknown>
#1 0x5642a92550a0 <unknown>
#2 0x5642a928f1ff <unknown>
#3 0x5642a928ac0f <unknown>
#4 0x5642a92dad75 <unknown>
#5 0x5642a92da296 <unknown>
#6 0x5642a92cc173 <unknown>
#7 0x5642a9298d4b <unknown>
#8 0x5642a92999b1 <unknown>
#9 0x5642a977793b <unknown>
#10 0x5642a977b83a <unknown>
#11 0x5642a975f692 <unknown>
#12 0x5642a977c3c4 <unknown>
#13 0x5642a97444cf <unknown>
#14 0x5642a97a0568 <unknown>
#15 0x5642a97a0746 <unknown>
#16 0x5642a97b15f6 <unknown>
#17 0x7f546401d134 <unknown>
[2025-05-07T03:48:54.910+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:48:54.923+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.014 seconds
[2025-05-07T03:49:25.833+0000] {processor.py:186} INFO - Started process (PID=888) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:49:25.835+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:49:25.835+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:49:25.835+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:49:26.799+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:49:26.798+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x564b1b3ae78a <unknown>
#1 0x564b1ae510a0 <unknown>
#2 0x564b1ae8b1ff <unknown>
#3 0x564b1ae86c0f <unknown>
#4 0x564b1aed6d75 <unknown>
#5 0x564b1aed6296 <unknown>
#6 0x564b1aec8173 <unknown>
#7 0x564b1ae94d4b <unknown>
#8 0x564b1ae959b1 <unknown>
#9 0x564b1b37393b <unknown>
#10 0x564b1b37783a <unknown>
#11 0x564b1b35b692 <unknown>
#12 0x564b1b3783c4 <unknown>
#13 0x564b1b3404cf <unknown>
#14 0x564b1b39c568 <unknown>
#15 0x564b1b39c746 <unknown>
#16 0x564b1b3ad5f6 <unknown>
#17 0x7f524500e134 <unknown>
[2025-05-07T03:49:26.799+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:49:26.815+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.986 seconds
[2025-05-07T03:49:57.212+0000] {processor.py:186} INFO - Started process (PID=924) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:49:57.213+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:49:57.214+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:49:57.214+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:49:58.235+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:49:58.234+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5558bb38878a <unknown>
#1 0x5558bae2b0a0 <unknown>
#2 0x5558bae651ff <unknown>
#3 0x5558bae60c0f <unknown>
#4 0x5558baeb0d75 <unknown>
#5 0x5558baeb0296 <unknown>
#6 0x5558baea2173 <unknown>
#7 0x5558bae6ed4b <unknown>
#8 0x5558bae6f9b1 <unknown>
#9 0x5558bb34d93b <unknown>
#10 0x5558bb35183a <unknown>
#11 0x5558bb335692 <unknown>
#12 0x5558bb3523c4 <unknown>
#13 0x5558bb31a4cf <unknown>
#14 0x5558bb376568 <unknown>
#15 0x5558bb376746 <unknown>
#16 0x5558bb3875f6 <unknown>
#17 0x7f7da5939134 <unknown>
[2025-05-07T03:49:58.235+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:49:58.251+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.045 seconds
[2025-05-07T03:50:28.592+0000] {processor.py:186} INFO - Started process (PID=966) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:50:28.593+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:50:28.594+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:50:28.594+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:50:29.053+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:50:29.052+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5582be2c478a <unknown>
#1 0x5582bdd670a0 <unknown>
#2 0x5582bdda11ff <unknown>
#3 0x5582bdd9cc0f <unknown>
#4 0x5582bddecd75 <unknown>
#5 0x5582bddec296 <unknown>
#6 0x5582bddde173 <unknown>
#7 0x5582bddaad4b <unknown>
#8 0x5582bddab9b1 <unknown>
#9 0x5582be28993b <unknown>
#10 0x5582be28d83a <unknown>
#11 0x5582be271692 <unknown>
#12 0x5582be28e3c4 <unknown>
#13 0x5582be2564cf <unknown>
#14 0x5582be2b2568 <unknown>
#15 0x5582be2b2746 <unknown>
#16 0x5582be2c35f6 <unknown>
#17 0x7f059a711134 <unknown>
[2025-05-07T03:50:29.053+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:50:29.069+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.987 seconds
[2025-05-07T03:50:59.265+0000] {processor.py:186} INFO - Started process (PID=999) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:50:59.267+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:50:59.268+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:50:59.267+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:50:54.641+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:50:54.640+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5590098e578a <unknown>
#1 0x5590093880a0 <unknown>
#2 0x5590093c21ff <unknown>
#3 0x5590093bdc0f <unknown>
#4 0x55900940dd75 <unknown>
#5 0x55900940d296 <unknown>
#6 0x5590093ff173 <unknown>
#7 0x5590093cbd4b <unknown>
#8 0x5590093cc9b1 <unknown>
#9 0x5590098aa93b <unknown>
#10 0x5590098ae83a <unknown>
#11 0x559009892692 <unknown>
#12 0x5590098af3c4 <unknown>
#13 0x5590098774cf <unknown>
#14 0x5590098d3568 <unknown>
#15 0x5590098d3746 <unknown>
#16 0x5590098e45f6 <unknown>
#17 0x7f26c1dd4134 <unknown>
[2025-05-07T03:50:54.642+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:50:54.656+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.994 seconds
[2025-05-07T03:51:29.151+0000] {processor.py:186} INFO - Started process (PID=1035) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:51:29.152+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:51:29.153+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:51:29.153+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:51:24.621+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:51:24.619+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559f85b8878a <unknown>
#1 0x559f8562b0a0 <unknown>
#2 0x559f856651ff <unknown>
#3 0x559f85660c0f <unknown>
#4 0x559f856b0d75 <unknown>
#5 0x559f856b0296 <unknown>
#6 0x559f856a2173 <unknown>
#7 0x559f8566ed4b <unknown>
#8 0x559f8566f9b1 <unknown>
#9 0x559f85b4d93b <unknown>
#10 0x559f85b5183a <unknown>
#11 0x559f85b35692 <unknown>
#12 0x559f85b523c4 <unknown>
#13 0x559f85b1a4cf <unknown>
#14 0x559f85b76568 <unknown>
#15 0x559f85b76746 <unknown>
#16 0x559f85b875f6 <unknown>
#17 0x7f905cd96134 <unknown>
[2025-05-07T03:51:24.622+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:51:24.635+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.092 seconds
[2025-05-07T03:51:55.309+0000] {processor.py:186} INFO - Started process (PID=1074) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:51:55.312+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:51:55.313+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:51:55.313+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:51:56.238+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:51:56.237+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5640f015778a <unknown>
#1 0x5640efbfa0a0 <unknown>
#2 0x5640efc341ff <unknown>
#3 0x5640efc2fc0f <unknown>
#4 0x5640efc7fd75 <unknown>
#5 0x5640efc7f296 <unknown>
#6 0x5640efc71173 <unknown>
#7 0x5640efc3dd4b <unknown>
#8 0x5640efc3e9b1 <unknown>
#9 0x5640f011c93b <unknown>
#10 0x5640f012083a <unknown>
#11 0x5640f0104692 <unknown>
#12 0x5640f01213c4 <unknown>
#13 0x5640f00e94cf <unknown>
#14 0x5640f0145568 <unknown>
#15 0x5640f0145746 <unknown>
#16 0x5640f01565f6 <unknown>
#17 0x7fdc59d0e134 <unknown>
[2025-05-07T03:51:56.239+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:51:56.252+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.949 seconds
[2025-05-07T03:52:26.606+0000] {processor.py:186} INFO - Started process (PID=1110) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:52:26.609+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:52:26.610+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:52:26.610+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:52:27.502+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:52:27.501+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56272557b78a <unknown>
#1 0x56272501e0a0 <unknown>
#2 0x5627250581ff <unknown>
#3 0x562725053c0f <unknown>
#4 0x5627250a3d75 <unknown>
#5 0x5627250a3296 <unknown>
#6 0x562725095173 <unknown>
#7 0x562725061d4b <unknown>
#8 0x5627250629b1 <unknown>
#9 0x56272554093b <unknown>
#10 0x56272554483a <unknown>
#11 0x562725528692 <unknown>
#12 0x5627255453c4 <unknown>
#13 0x56272550d4cf <unknown>
#14 0x562725569568 <unknown>
#15 0x562725569746 <unknown>
#16 0x56272557a5f6 <unknown>
#17 0x7fe9ffb30134 <unknown>
[2025-05-07T03:52:27.502+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:52:27.517+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.916 seconds
[2025-05-07T03:52:58.171+0000] {processor.py:186} INFO - Started process (PID=1146) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:52:58.172+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:52:58.173+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:52:58.172+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:53:04.383+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:53:04.382+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561df682478a <unknown>
#1 0x561df62c70a0 <unknown>
#2 0x561df63011ff <unknown>
#3 0x561df62fcc0f <unknown>
#4 0x561df634cd75 <unknown>
#5 0x561df634c296 <unknown>
#6 0x561df633e173 <unknown>
#7 0x561df630ad4b <unknown>
#8 0x561df630b9b1 <unknown>
#9 0x561df67e993b <unknown>
#10 0x561df67ed83a <unknown>
#11 0x561df67d1692 <unknown>
#12 0x561df67ee3c4 <unknown>
#13 0x561df67b64cf <unknown>
#14 0x561df6812568 <unknown>
#15 0x561df6812746 <unknown>
#16 0x561df68235f6 <unknown>
#17 0x7f7c9199f134 <unknown>
[2025-05-07T03:53:04.384+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:53:04.399+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.128 seconds
[2025-05-07T03:53:39.512+0000] {processor.py:186} INFO - Started process (PID=1182) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:53:39.513+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:53:39.514+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:53:39.514+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:53:34.836+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:53:34.835+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f9f3fa978a <unknown>
#1 0x55f9f3a4c0a0 <unknown>
#2 0x55f9f3a861ff <unknown>
#3 0x55f9f3a81c0f <unknown>
#4 0x55f9f3ad1d75 <unknown>
#5 0x55f9f3ad1296 <unknown>
#6 0x55f9f3ac3173 <unknown>
#7 0x55f9f3a8fd4b <unknown>
#8 0x55f9f3a909b1 <unknown>
#9 0x55f9f3f6e93b <unknown>
#10 0x55f9f3f7283a <unknown>
#11 0x55f9f3f56692 <unknown>
#12 0x55f9f3f733c4 <unknown>
#13 0x55f9f3f3b4cf <unknown>
#14 0x55f9f3f97568 <unknown>
#15 0x55f9f3f97746 <unknown>
#16 0x55f9f3fa85f6 <unknown>
#17 0x7f1d571b1134 <unknown>
[2025-05-07T03:53:34.836+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:53:34.849+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.942 seconds
[2025-05-07T03:54:05.311+0000] {processor.py:186} INFO - Started process (PID=1226) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:54:05.320+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:54:05.321+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:54:05.321+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:54:06.317+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:54:06.316+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x565213a9b78a <unknown>
#1 0x56521353e0a0 <unknown>
#2 0x5652135781ff <unknown>
#3 0x565213573c0f <unknown>
#4 0x5652135c3d75 <unknown>
#5 0x5652135c3296 <unknown>
#6 0x5652135b5173 <unknown>
#7 0x565213581d4b <unknown>
#8 0x5652135829b1 <unknown>
#9 0x565213a6093b <unknown>
#10 0x565213a6483a <unknown>
#11 0x565213a48692 <unknown>
#12 0x565213a653c4 <unknown>
#13 0x565213a2d4cf <unknown>
#14 0x565213a89568 <unknown>
#15 0x565213a89746 <unknown>
#16 0x565213a9a5f6 <unknown>
#17 0x7efcb8b5b134 <unknown>
[2025-05-07T03:54:06.317+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:54:06.330+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.024 seconds
[2025-05-07T03:54:36.771+0000] {processor.py:186} INFO - Started process (PID=1262) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:54:36.773+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:54:36.773+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:54:36.773+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:54:37.717+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:54:37.716+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f3c7c5778a <unknown>
#1 0x55f3c76fa0a0 <unknown>
#2 0x55f3c77341ff <unknown>
#3 0x55f3c772fc0f <unknown>
#4 0x55f3c777fd75 <unknown>
#5 0x55f3c777f296 <unknown>
#6 0x55f3c7771173 <unknown>
#7 0x55f3c773dd4b <unknown>
#8 0x55f3c773e9b1 <unknown>
#9 0x55f3c7c1c93b <unknown>
#10 0x55f3c7c2083a <unknown>
#11 0x55f3c7c04692 <unknown>
#12 0x55f3c7c213c4 <unknown>
#13 0x55f3c7be94cf <unknown>
#14 0x55f3c7c45568 <unknown>
#15 0x55f3c7c45746 <unknown>
#16 0x55f3c7c565f6 <unknown>
#17 0x7f2a408cc134 <unknown>
[2025-05-07T03:54:37.717+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:54:37.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.965 seconds
[2025-05-07T03:55:08.144+0000] {processor.py:186} INFO - Started process (PID=1298) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:55:08.145+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:55:08.146+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:55:08.146+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:55:09.108+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:55:09.107+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ff17b3778a <unknown>
#1 0x55ff175da0a0 <unknown>
#2 0x55ff176141ff <unknown>
#3 0x55ff1760fc0f <unknown>
#4 0x55ff1765fd75 <unknown>
#5 0x55ff1765f296 <unknown>
#6 0x55ff17651173 <unknown>
#7 0x55ff1761dd4b <unknown>
#8 0x55ff1761e9b1 <unknown>
#9 0x55ff17afc93b <unknown>
#10 0x55ff17b0083a <unknown>
#11 0x55ff17ae4692 <unknown>
#12 0x55ff17b013c4 <unknown>
#13 0x55ff17ac94cf <unknown>
#14 0x55ff17b25568 <unknown>
#15 0x55ff17b25746 <unknown>
#16 0x55ff17b365f6 <unknown>
#17 0x7f7de1772134 <unknown>
[2025-05-07T03:55:09.109+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:55:09.122+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.982 seconds
[2025-05-07T03:55:39.327+0000] {processor.py:186} INFO - Started process (PID=1334) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:55:39.328+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:55:39.329+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:55:39.329+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:55:40.310+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:55:40.309+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e85168978a <unknown>
#1 0x55e85112c0a0 <unknown>
#2 0x55e8511661ff <unknown>
#3 0x55e851161c0f <unknown>
#4 0x55e8511b1d75 <unknown>
#5 0x55e8511b1296 <unknown>
#6 0x55e8511a3173 <unknown>
#7 0x55e85116fd4b <unknown>
#8 0x55e8511709b1 <unknown>
#9 0x55e85164e93b <unknown>
#10 0x55e85165283a <unknown>
#11 0x55e851636692 <unknown>
#12 0x55e8516533c4 <unknown>
#13 0x55e85161b4cf <unknown>
#14 0x55e851677568 <unknown>
#15 0x55e851677746 <unknown>
#16 0x55e8516885f6 <unknown>
#17 0x7f2725f2a134 <unknown>
[2025-05-07T03:55:40.311+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:55:40.325+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.003 seconds
[2025-05-07T03:56:10.359+0000] {processor.py:186} INFO - Started process (PID=1369) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:56:10.363+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:56:10.364+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:56:10.363+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:56:11.361+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:56:11.360+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5605763f778a <unknown>
#1 0x560575e9a0a0 <unknown>
#2 0x560575ed41ff <unknown>
#3 0x560575ecfc0f <unknown>
#4 0x560575f1fd75 <unknown>
#5 0x560575f1f296 <unknown>
#6 0x560575f11173 <unknown>
#7 0x560575eddd4b <unknown>
#8 0x560575ede9b1 <unknown>
#9 0x5605763bc93b <unknown>
#10 0x5605763c083a <unknown>
#11 0x5605763a4692 <unknown>
#12 0x5605763c13c4 <unknown>
#13 0x5605763894cf <unknown>
#14 0x5605763e5568 <unknown>
#15 0x5605763e5746 <unknown>
#16 0x5605763f65f6 <unknown>
#17 0x7f927baaf134 <unknown>
[2025-05-07T03:56:11.362+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:56:11.378+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.024 seconds
[2025-05-07T03:56:44.893+0000] {processor.py:186} INFO - Started process (PID=1405) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:56:44.893+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:56:44.894+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:56:44.894+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:56:40.219+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:56:40.218+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563d3a2b478a <unknown>
#1 0x563d39d570a0 <unknown>
#2 0x563d39d911ff <unknown>
#3 0x563d39d8cc0f <unknown>
#4 0x563d39ddcd75 <unknown>
#5 0x563d39ddc296 <unknown>
#6 0x563d39dce173 <unknown>
#7 0x563d39d9ad4b <unknown>
#8 0x563d39d9b9b1 <unknown>
#9 0x563d3a27993b <unknown>
#10 0x563d3a27d83a <unknown>
#11 0x563d3a261692 <unknown>
#12 0x563d3a27e3c4 <unknown>
#13 0x563d3a2464cf <unknown>
#14 0x563d3a2a2568 <unknown>
#15 0x563d3a2a2746 <unknown>
#16 0x563d3a2b35f6 <unknown>
#17 0x7facd2ad1134 <unknown>
[2025-05-07T03:56:40.220+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:56:40.238+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.948 seconds
[2025-05-07T03:57:14.856+0000] {processor.py:186} INFO - Started process (PID=1441) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:57:14.857+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:57:14.858+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:57:14.858+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:57:10.318+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:57:10.317+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5640ee97978a <unknown>
#1 0x5640ee41c0a0 <unknown>
#2 0x5640ee4561ff <unknown>
#3 0x5640ee451c0f <unknown>
#4 0x5640ee4a1d75 <unknown>
#5 0x5640ee4a1296 <unknown>
#6 0x5640ee493173 <unknown>
#7 0x5640ee45fd4b <unknown>
#8 0x5640ee4609b1 <unknown>
#9 0x5640ee93e93b <unknown>
#10 0x5640ee94283a <unknown>
#11 0x5640ee926692 <unknown>
#12 0x5640ee9433c4 <unknown>
#13 0x5640ee90b4cf <unknown>
#14 0x5640ee967568 <unknown>
#15 0x5640ee967746 <unknown>
#16 0x5640ee9785f6 <unknown>
#17 0x7fb6be083134 <unknown>
[2025-05-07T03:57:10.319+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:57:10.332+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.080 seconds
[2025-05-07T03:57:44.732+0000] {processor.py:186} INFO - Started process (PID=1477) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:57:44.732+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:57:44.733+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:57:44.733+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:57:40.139+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:57:40.138+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557c11bee78a <unknown>
#1 0x557c116910a0 <unknown>
#2 0x557c116cb1ff <unknown>
#3 0x557c116c6c0f <unknown>
#4 0x557c11716d75 <unknown>
#5 0x557c11716296 <unknown>
#6 0x557c11708173 <unknown>
#7 0x557c116d4d4b <unknown>
#8 0x557c116d59b1 <unknown>
#9 0x557c11bb393b <unknown>
#10 0x557c11bb783a <unknown>
#11 0x557c11b9b692 <unknown>
#12 0x557c11bb83c4 <unknown>
#13 0x557c11b804cf <unknown>
#14 0x557c11bdc568 <unknown>
#15 0x557c11bdc746 <unknown>
#16 0x557c11bed5f6 <unknown>
#17 0x7ff25b43a134 <unknown>
[2025-05-07T03:57:40.139+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:57:40.155+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.024 seconds
[2025-05-07T03:58:10.804+0000] {processor.py:186} INFO - Started process (PID=1519) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:58:10.805+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:58:10.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:58:10.805+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:58:11.782+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:58:11.781+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f47ca5c78a <unknown>
#1 0x55f47c4ff0a0 <unknown>
#2 0x55f47c5391ff <unknown>
#3 0x55f47c534c0f <unknown>
#4 0x55f47c584d75 <unknown>
#5 0x55f47c584296 <unknown>
#6 0x55f47c576173 <unknown>
#7 0x55f47c542d4b <unknown>
#8 0x55f47c5439b1 <unknown>
#9 0x55f47ca2193b <unknown>
#10 0x55f47ca2583a <unknown>
#11 0x55f47ca09692 <unknown>
#12 0x55f47ca263c4 <unknown>
#13 0x55f47c9ee4cf <unknown>
#14 0x55f47ca4a568 <unknown>
#15 0x55f47ca4a746 <unknown>
#16 0x55f47ca5b5f6 <unknown>
#17 0x7f12ce3a3134 <unknown>
[2025-05-07T03:58:11.783+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:58:11.796+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.998 seconds
[2025-05-07T03:58:45.074+0000] {processor.py:186} INFO - Started process (PID=1552) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:58:45.074+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:58:45.075+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:58:45.075+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:58:40.441+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:58:40.440+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5559a0ff478a <unknown>
#1 0x5559a0a970a0 <unknown>
#2 0x5559a0ad11ff <unknown>
#3 0x5559a0accc0f <unknown>
#4 0x5559a0b1cd75 <unknown>
#5 0x5559a0b1c296 <unknown>
#6 0x5559a0b0e173 <unknown>
#7 0x5559a0adad4b <unknown>
#8 0x5559a0adb9b1 <unknown>
#9 0x5559a0fb993b <unknown>
#10 0x5559a0fbd83a <unknown>
#11 0x5559a0fa1692 <unknown>
#12 0x5559a0fbe3c4 <unknown>
#13 0x5559a0f864cf <unknown>
#14 0x5559a0fe2568 <unknown>
#15 0x5559a0fe2746 <unknown>
#16 0x5559a0ff35f6 <unknown>
#17 0x7f9527853134 <unknown>
[2025-05-07T03:58:40.442+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:58:40.459+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.988 seconds
[2025-05-07T03:59:11.146+0000] {processor.py:186} INFO - Started process (PID=1591) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:11.147+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:59:11.148+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:59:11.148+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:12.156+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:59:12.155+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557b466e378a <unknown>
#1 0x557b461860a0 <unknown>
#2 0x557b461c01ff <unknown>
#3 0x557b461bbc0f <unknown>
#4 0x557b4620bd75 <unknown>
#5 0x557b4620b296 <unknown>
#6 0x557b461fd173 <unknown>
#7 0x557b461c9d4b <unknown>
#8 0x557b461ca9b1 <unknown>
#9 0x557b466a893b <unknown>
#10 0x557b466ac83a <unknown>
#11 0x557b46690692 <unknown>
#12 0x557b466ad3c4 <unknown>
#13 0x557b466754cf <unknown>
#14 0x557b466d1568 <unknown>
#15 0x557b466d1746 <unknown>
#16 0x557b466e25f6 <unknown>
#17 0x7fb526396134 <unknown>
[2025-05-07T03:59:12.156+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:12.170+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.030 seconds
[2025-05-07T03:59:13.173+0000] {processor.py:186} INFO - Started process (PID=1613) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:13.174+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:59:13.175+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:59:13.174+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:19.622+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T03:59:20.744+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:59:20.743+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55718337f78a <unknown>
#1 0x557182e220a0 <unknown>
#2 0x557182e5c1ff <unknown>
#3 0x557182e57c0f <unknown>
#4 0x557182ea7d75 <unknown>
#5 0x557182ea7296 <unknown>
#6 0x557182e99173 <unknown>
#7 0x557182e65d4b <unknown>
#8 0x557182e669b1 <unknown>
#9 0x55718334493b <unknown>
#10 0x55718334883a <unknown>
#11 0x55718332c692 <unknown>
#12 0x5571833493c4 <unknown>
#13 0x5571833114cf <unknown>
#14 0x55718336d568 <unknown>
#15 0x55718336d746 <unknown>
#16 0x55718337e5f6 <unknown>
#17 0x7f6e72bee134 <unknown>
[2025-05-07T03:59:20.745+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:20.758+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.593 seconds
[2025-05-07T03:59:51.754+0000] {processor.py:186} INFO - Started process (PID=1761) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:51.758+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T03:59:51.759+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:59:51.758+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:58.405+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T03:59:59.600+0000] {logging_mixin.py:190} INFO - [2025-05-07T03:59:59.599+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562cecbb478a <unknown>
#1 0x562cec6570a0 <unknown>
#2 0x562cec6911ff <unknown>
#3 0x562cec68cc0f <unknown>
#4 0x562cec6dcd75 <unknown>
#5 0x562cec6dc296 <unknown>
#6 0x562cec6ce173 <unknown>
#7 0x562cec69ad4b <unknown>
#8 0x562cec69b9b1 <unknown>
#9 0x562cecb7993b <unknown>
#10 0x562cecb7d83a <unknown>
#11 0x562cecb61692 <unknown>
#12 0x562cecb7e3c4 <unknown>
#13 0x562cecb464cf <unknown>
#14 0x562cecba2568 <unknown>
#15 0x562cecba2746 <unknown>
#16 0x562cecbb35f6 <unknown>
#17 0x7f8e10c81134 <unknown>
[2025-05-07T03:59:59.601+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T03:59:59.622+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.374 seconds
[2025-05-07T04:00:30.062+0000] {processor.py:186} INFO - Started process (PID=1917) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:00:30.065+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:00:30.067+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:00:30.066+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:00:36.129+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:00:37.384+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:00:37.383+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f5a42d778a <unknown>
#1 0x55f5a3d7a0a0 <unknown>
#2 0x55f5a3db41ff <unknown>
#3 0x55f5a3dafc0f <unknown>
#4 0x55f5a3dffd75 <unknown>
#5 0x55f5a3dff296 <unknown>
#6 0x55f5a3df1173 <unknown>
#7 0x55f5a3dbdd4b <unknown>
#8 0x55f5a3dbe9b1 <unknown>
#9 0x55f5a429c93b <unknown>
#10 0x55f5a42a083a <unknown>
#11 0x55f5a4284692 <unknown>
#12 0x55f5a42a13c4 <unknown>
#13 0x55f5a42694cf <unknown>
#14 0x55f5a42c5568 <unknown>
#15 0x55f5a42c5746 <unknown>
#16 0x55f5a42d65f6 <unknown>
#17 0x7fc397b5d134 <unknown>
[2025-05-07T04:00:37.385+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:00:37.400+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.838 seconds
[2025-05-07T04:01:08.184+0000] {processor.py:186} INFO - Started process (PID=2063) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:01:08.188+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:01:08.189+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:01:08.189+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:01:14.308+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:01:15.065+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:01:15.064+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5562e226378a <unknown>
#1 0x5562e1d060a0 <unknown>
#2 0x5562e1d401ff <unknown>
#3 0x5562e1d3bc0f <unknown>
#4 0x5562e1d8bd75 <unknown>
#5 0x5562e1d8b296 <unknown>
#6 0x5562e1d7d173 <unknown>
#7 0x5562e1d49d4b <unknown>
#8 0x5562e1d4a9b1 <unknown>
#9 0x5562e222893b <unknown>
#10 0x5562e222c83a <unknown>
#11 0x5562e2210692 <unknown>
#12 0x5562e222d3c4 <unknown>
#13 0x5562e21f54cf <unknown>
#14 0x5562e2251568 <unknown>
#15 0x5562e2251746 <unknown>
#16 0x5562e22625f6 <unknown>
#17 0x7ff0277c4134 <unknown>
[2025-05-07T04:01:15.066+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:01:15.084+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.908 seconds
[2025-05-07T04:01:45.915+0000] {processor.py:186} INFO - Started process (PID=2210) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:01:45.917+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:01:45.917+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:01:45.917+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:01:52.351+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:01:53.516+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:01:53.515+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562dc6a2778a <unknown>
#1 0x562dc64ca0a0 <unknown>
#2 0x562dc65041ff <unknown>
#3 0x562dc64ffc0f <unknown>
#4 0x562dc654fd75 <unknown>
#5 0x562dc654f296 <unknown>
#6 0x562dc6541173 <unknown>
#7 0x562dc650dd4b <unknown>
#8 0x562dc650e9b1 <unknown>
#9 0x562dc69ec93b <unknown>
#10 0x562dc69f083a <unknown>
#11 0x562dc69d4692 <unknown>
#12 0x562dc69f13c4 <unknown>
#13 0x562dc69b94cf <unknown>
#14 0x562dc6a15568 <unknown>
#15 0x562dc6a15746 <unknown>
#16 0x562dc6a265f6 <unknown>
#17 0x7f7aa10e8134 <unknown>
[2025-05-07T04:01:53.516+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:01:53.531+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.123 seconds
[2025-05-07T04:02:24.195+0000] {processor.py:186} INFO - Started process (PID=2364) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:02:24.196+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:02:24.197+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:02:24.197+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:02:35.287+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:02:30.933+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:02:30.932+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f1a424e78a <unknown>
#1 0x55f1a3cf10a0 <unknown>
#2 0x55f1a3d2b1ff <unknown>
#3 0x55f1a3d26c0f <unknown>
#4 0x55f1a3d76d75 <unknown>
#5 0x55f1a3d76296 <unknown>
#6 0x55f1a3d68173 <unknown>
#7 0x55f1a3d34d4b <unknown>
#8 0x55f1a3d359b1 <unknown>
#9 0x55f1a421393b <unknown>
#10 0x55f1a421783a <unknown>
#11 0x55f1a41fb692 <unknown>
#12 0x55f1a42183c4 <unknown>
#13 0x55f1a41e04cf <unknown>
#14 0x55f1a423c568 <unknown>
#15 0x55f1a423c746 <unknown>
#16 0x55f1a424d5f6 <unknown>
#17 0x7f6bd3568134 <unknown>
[2025-05-07T04:02:30.934+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:02:30.961+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.775 seconds
[2025-05-07T04:03:01.649+0000] {processor.py:186} INFO - Started process (PID=2519) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:03:01.650+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:03:01.652+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:03:01.652+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:03:08.376+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:03:09.490+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:03:09.489+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c7aa93178a <unknown>
#1 0x55c7aa3d40a0 <unknown>
#2 0x55c7aa40e1ff <unknown>
#3 0x55c7aa409c0f <unknown>
#4 0x55c7aa459d75 <unknown>
#5 0x55c7aa459296 <unknown>
#6 0x55c7aa44b173 <unknown>
#7 0x55c7aa417d4b <unknown>
#8 0x55c7aa4189b1 <unknown>
#9 0x55c7aa8f693b <unknown>
#10 0x55c7aa8fa83a <unknown>
#11 0x55c7aa8de692 <unknown>
#12 0x55c7aa8fb3c4 <unknown>
#13 0x55c7aa8c34cf <unknown>
#14 0x55c7aa91f568 <unknown>
#15 0x55c7aa91f746 <unknown>
#16 0x55c7aa9305f6 <unknown>
#17 0x7f986d864134 <unknown>
[2025-05-07T04:03:09.490+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:03:09.505+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.355 seconds
[2025-05-07T04:03:40.375+0000] {processor.py:186} INFO - Started process (PID=2665) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:03:40.376+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:03:40.377+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:03:40.377+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:03:41.004+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:03:41.992+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:03:41.991+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a71510878a <unknown>
#1 0x55a714bab0a0 <unknown>
#2 0x55a714be51ff <unknown>
#3 0x55a714be0c0f <unknown>
#4 0x55a714c30d75 <unknown>
#5 0x55a714c30296 <unknown>
#6 0x55a714c22173 <unknown>
#7 0x55a714beed4b <unknown>
#8 0x55a714bef9b1 <unknown>
#9 0x55a7150cd93b <unknown>
#10 0x55a7150d183a <unknown>
#11 0x55a7150b5692 <unknown>
#12 0x55a7150d23c4 <unknown>
#13 0x55a71509a4cf <unknown>
#14 0x55a7150f6568 <unknown>
#15 0x55a7150f6746 <unknown>
#16 0x55a7151075f6 <unknown>
#17 0x7f32d59c8134 <unknown>
[2025-05-07T04:03:41.993+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:03:42.006+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.740 seconds
[2025-05-07T04:04:12.064+0000] {processor.py:186} INFO - Started process (PID=2821) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:04:12.065+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:04:12.067+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:04:12.066+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:04:18.194+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:04:19.395+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:04:19.394+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559d68fed78a <unknown>
#1 0x559d68a900a0 <unknown>
#2 0x559d68aca1ff <unknown>
#3 0x559d68ac5c0f <unknown>
#4 0x559d68b15d75 <unknown>
#5 0x559d68b15296 <unknown>
#6 0x559d68b07173 <unknown>
#7 0x559d68ad3d4b <unknown>
#8 0x559d68ad49b1 <unknown>
#9 0x559d68fb293b <unknown>
#10 0x559d68fb683a <unknown>
#11 0x559d68f9a692 <unknown>
#12 0x559d68fb73c4 <unknown>
#13 0x559d68f7f4cf <unknown>
#14 0x559d68fdb568 <unknown>
#15 0x559d68fdb746 <unknown>
#16 0x559d68fec5f6 <unknown>
#17 0x7f723c850134 <unknown>
[2025-05-07T04:04:19.396+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:04:19.411+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.851 seconds
[2025-05-07T04:04:50.421+0000] {processor.py:186} INFO - Started process (PID=2964) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:04:50.423+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:04:50.424+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:04:50.423+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:04:51.016+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:04:52.347+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:04:52.346+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d6d3aff78a <unknown>
#1 0x55d6d35a20a0 <unknown>
#2 0x55d6d35dc1ff <unknown>
#3 0x55d6d35d7c0f <unknown>
#4 0x55d6d3627d75 <unknown>
#5 0x55d6d3627296 <unknown>
#6 0x55d6d3619173 <unknown>
#7 0x55d6d35e5d4b <unknown>
#8 0x55d6d35e69b1 <unknown>
#9 0x55d6d3ac493b <unknown>
#10 0x55d6d3ac883a <unknown>
#11 0x55d6d3aac692 <unknown>
#12 0x55d6d3ac93c4 <unknown>
#13 0x55d6d3a914cf <unknown>
#14 0x55d6d3aed568 <unknown>
#15 0x55d6d3aed746 <unknown>
#16 0x55d6d3afe5f6 <unknown>
#17 0x7f3497731134 <unknown>
[2025-05-07T04:04:52.348+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:04:52.363+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.047 seconds
[2025-05-07T04:05:22.395+0000] {processor.py:186} INFO - Started process (PID=3112) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:05:22.396+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:05:22.398+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:05:22.397+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:05:28.526+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:05:29.816+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:05:29.815+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55617c38278a <unknown>
#1 0x55617be250a0 <unknown>
#2 0x55617be5f1ff <unknown>
#3 0x55617be5ac0f <unknown>
#4 0x55617beaad75 <unknown>
#5 0x55617beaa296 <unknown>
#6 0x55617be9c173 <unknown>
#7 0x55617be68d4b <unknown>
#8 0x55617be699b1 <unknown>
#9 0x55617c34793b <unknown>
#10 0x55617c34b83a <unknown>
#11 0x55617c32f692 <unknown>
#12 0x55617c34c3c4 <unknown>
#13 0x55617c3144cf <unknown>
#14 0x55617c370568 <unknown>
#15 0x55617c370746 <unknown>
#16 0x55617c3815f6 <unknown>
#17 0x7f32a4ecc134 <unknown>
[2025-05-07T04:05:29.817+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:05:29.833+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.948 seconds
[2025-05-07T04:05:40.181+0000] {processor.py:186} INFO - Started process (PID=3248) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:05:40.181+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:05:40.182+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:05:40.182+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:05:41.147+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:05:41.146+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5586b04fe78a <unknown>
#1 0x5586affa10a0 <unknown>
#2 0x5586affdb1ff <unknown>
#3 0x5586affd6c0f <unknown>
#4 0x5586b0026d75 <unknown>
#5 0x5586b0026296 <unknown>
#6 0x5586b0018173 <unknown>
#7 0x5586affe4d4b <unknown>
#8 0x5586affe59b1 <unknown>
#9 0x5586b04c393b <unknown>
#10 0x5586b04c783a <unknown>
#11 0x5586b04ab692 <unknown>
#12 0x5586b04c83c4 <unknown>
#13 0x5586b04904cf <unknown>
#14 0x5586b04ec568 <unknown>
#15 0x5586b04ec746 <unknown>
#16 0x5586b04fd5f6 <unknown>
#17 0x7f8985c1b134 <unknown>
[2025-05-07T04:05:41.148+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:05:41.165+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.990 seconds
[2025-05-07T04:06:11.504+0000] {processor.py:186} INFO - Started process (PID=3281) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:06:11.505+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:06:11.506+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:06:11.506+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:06:12.501+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:06:12.500+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5636a972078a <unknown>
#1 0x5636a91c30a0 <unknown>
#2 0x5636a91fd1ff <unknown>
#3 0x5636a91f8c0f <unknown>
#4 0x5636a9248d75 <unknown>
#5 0x5636a9248296 <unknown>
#6 0x5636a923a173 <unknown>
#7 0x5636a9206d4b <unknown>
#8 0x5636a92079b1 <unknown>
#9 0x5636a96e593b <unknown>
#10 0x5636a96e983a <unknown>
#11 0x5636a96cd692 <unknown>
#12 0x5636a96ea3c4 <unknown>
#13 0x5636a96b24cf <unknown>
#14 0x5636a970e568 <unknown>
#15 0x5636a970e746 <unknown>
#16 0x5636a971f5f6 <unknown>
#17 0x7faf4b73f134 <unknown>
[2025-05-07T04:06:12.501+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:06:12.516+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.016 seconds
[2025-05-07T04:06:43.372+0000] {processor.py:186} INFO - Started process (PID=3317) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:06:43.373+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:06:43.374+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:06:43.374+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:06:44.356+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:06:44.355+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56446b4b678a <unknown>
#1 0x56446af590a0 <unknown>
#2 0x56446af931ff <unknown>
#3 0x56446af8ec0f <unknown>
#4 0x56446afded75 <unknown>
#5 0x56446afde296 <unknown>
#6 0x56446afd0173 <unknown>
#7 0x56446af9cd4b <unknown>
#8 0x56446af9d9b1 <unknown>
#9 0x56446b47b93b <unknown>
#10 0x56446b47f83a <unknown>
#11 0x56446b463692 <unknown>
#12 0x56446b4803c4 <unknown>
#13 0x56446b4484cf <unknown>
#14 0x56446b4a4568 <unknown>
#15 0x56446b4a4746 <unknown>
#16 0x56446b4b55f6 <unknown>
#17 0x7feee1b1a134 <unknown>
[2025-05-07T04:06:44.356+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:06:44.370+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.004 seconds
[2025-05-07T04:06:59.371+0000] {processor.py:186} INFO - Started process (PID=3351) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:06:59.372+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:06:59.373+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:06:59.373+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:07:10.838+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:07:06.488+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:07:06.487+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55afa234178a <unknown>
#1 0x55afa1de40a0 <unknown>
#2 0x55afa1e1e1ff <unknown>
#3 0x55afa1e19c0f <unknown>
#4 0x55afa1e69d75 <unknown>
#5 0x55afa1e69296 <unknown>
#6 0x55afa1e5b173 <unknown>
#7 0x55afa1e27d4b <unknown>
#8 0x55afa1e289b1 <unknown>
#9 0x55afa230693b <unknown>
#10 0x55afa230a83a <unknown>
#11 0x55afa22ee692 <unknown>
#12 0x55afa230b3c4 <unknown>
#13 0x55afa22d34cf <unknown>
#14 0x55afa232f568 <unknown>
#15 0x55afa232f746 <unknown>
#16 0x55afa23405f6 <unknown>
#17 0x7fcd6613d134 <unknown>
[2025-05-07T04:07:06.488+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:07:06.504+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.136 seconds
[2025-05-07T04:07:37.505+0000] {processor.py:186} INFO - Started process (PID=3492) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:07:37.509+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:07:37.510+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:07:37.510+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:07:44.013+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:07:45.193+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:07:45.193+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56503c57a78a <unknown>
#1 0x56503c01d0a0 <unknown>
#2 0x56503c0571ff <unknown>
#3 0x56503c052c0f <unknown>
#4 0x56503c0a2d75 <unknown>
#5 0x56503c0a2296 <unknown>
#6 0x56503c094173 <unknown>
#7 0x56503c060d4b <unknown>
#8 0x56503c0619b1 <unknown>
#9 0x56503c53f93b <unknown>
#10 0x56503c54383a <unknown>
#11 0x56503c527692 <unknown>
#12 0x56503c5443c4 <unknown>
#13 0x56503c50c4cf <unknown>
#14 0x56503c568568 <unknown>
#15 0x56503c568746 <unknown>
#16 0x56503c5795f6 <unknown>
#17 0x7f704c095134 <unknown>
[2025-05-07T04:07:45.194+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:07:45.208+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.205 seconds
[2025-05-07T04:08:15.721+0000] {processor.py:186} INFO - Started process (PID=3645) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:08:15.722+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:08:15.723+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:08:15.723+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:08:22.170+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:08:23.331+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:08:23.329+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5638f455e78a <unknown>
#1 0x5638f40010a0 <unknown>
#2 0x5638f403b1ff <unknown>
#3 0x5638f4036c0f <unknown>
#4 0x5638f4086d75 <unknown>
#5 0x5638f4086296 <unknown>
#6 0x5638f4078173 <unknown>
#7 0x5638f4044d4b <unknown>
#8 0x5638f40459b1 <unknown>
#9 0x5638f452393b <unknown>
#10 0x5638f452783a <unknown>
#11 0x5638f450b692 <unknown>
#12 0x5638f45283c4 <unknown>
#13 0x5638f44f04cf <unknown>
#14 0x5638f454c568 <unknown>
#15 0x5638f454c746 <unknown>
#16 0x5638f455d5f6 <unknown>
#17 0x7f949fb16134 <unknown>
[2025-05-07T04:08:23.333+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:08:23.355+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.139 seconds
[2025-05-07T04:08:56.008+0000] {processor.py:186} INFO - Started process (PID=3787) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:08:56.009+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:08:56.010+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:08:56.010+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:08:57.068+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:08:58.117+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:08:58.116+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56353a28378a <unknown>
#1 0x563539d260a0 <unknown>
#2 0x563539d601ff <unknown>
#3 0x563539d5bc0f <unknown>
#4 0x563539dabd75 <unknown>
#5 0x563539dab296 <unknown>
#6 0x563539d9d173 <unknown>
#7 0x563539d69d4b <unknown>
#8 0x563539d6a9b1 <unknown>
#9 0x56353a24893b <unknown>
#10 0x56353a24c83a <unknown>
#11 0x56353a230692 <unknown>
#12 0x56353a24d3c4 <unknown>
#13 0x56353a2154cf <unknown>
#14 0x56353a271568 <unknown>
#15 0x56353a271746 <unknown>
#16 0x56353a2825f6 <unknown>
#17 0x7f6edfe6a134 <unknown>
[2025-05-07T04:08:58.118+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:08:58.132+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.236 seconds
[2025-05-07T04:09:30.908+0000] {processor.py:186} INFO - Started process (PID=3939) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:09:30.910+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:09:30.911+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:09:30.911+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:09:32.217+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:09:33.406+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:09:33.405+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c34bf3378a <unknown>
#1 0x55c34b9d60a0 <unknown>
#2 0x55c34ba101ff <unknown>
#3 0x55c34ba0bc0f <unknown>
#4 0x55c34ba5bd75 <unknown>
#5 0x55c34ba5b296 <unknown>
#6 0x55c34ba4d173 <unknown>
#7 0x55c34ba19d4b <unknown>
#8 0x55c34ba1a9b1 <unknown>
#9 0x55c34bef893b <unknown>
#10 0x55c34befc83a <unknown>
#11 0x55c34bee0692 <unknown>
#12 0x55c34befd3c4 <unknown>
#13 0x55c34bec54cf <unknown>
#14 0x55c34bf21568 <unknown>
#15 0x55c34bf21746 <unknown>
#16 0x55c34bf325f6 <unknown>
#17 0x7f899b26c134 <unknown>
[2025-05-07T04:09:33.406+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:09:33.419+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.624 seconds
[2025-05-07T04:10:06.142+0000] {processor.py:186} INFO - Started process (PID=4088) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:10:06.143+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:10:06.144+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:10:06.144+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:10:07.071+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:10:08.242+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:10:08.241+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56364c16e78a <unknown>
#1 0x56364bc110a0 <unknown>
#2 0x56364bc4b1ff <unknown>
#3 0x56364bc46c0f <unknown>
#4 0x56364bc96d75 <unknown>
#5 0x56364bc96296 <unknown>
#6 0x56364bc88173 <unknown>
#7 0x56364bc54d4b <unknown>
#8 0x56364bc559b1 <unknown>
#9 0x56364c13393b <unknown>
#10 0x56364c13783a <unknown>
#11 0x56364c11b692 <unknown>
#12 0x56364c1383c4 <unknown>
#13 0x56364c1004cf <unknown>
#14 0x56364c15c568 <unknown>
#15 0x56364c15c746 <unknown>
#16 0x56364c16d5f6 <unknown>
#17 0x7f61deab0134 <unknown>
[2025-05-07T04:10:08.243+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:10:08.259+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.225 seconds
[2025-05-07T04:10:39.141+0000] {processor.py:186} INFO - Started process (PID=4237) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:10:39.144+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:10:39.145+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:10:39.145+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:10:51.076+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:10:46.701+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:10:46.699+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5646d4f1e78a <unknown>
#1 0x5646d49c10a0 <unknown>
#2 0x5646d49fb1ff <unknown>
#3 0x5646d49f6c0f <unknown>
#4 0x5646d4a46d75 <unknown>
#5 0x5646d4a46296 <unknown>
#6 0x5646d4a38173 <unknown>
#7 0x5646d4a04d4b <unknown>
#8 0x5646d4a059b1 <unknown>
#9 0x5646d4ee393b <unknown>
#10 0x5646d4ee783a <unknown>
#11 0x5646d4ecb692 <unknown>
#12 0x5646d4ee83c4 <unknown>
#13 0x5646d4eb04cf <unknown>
#14 0x5646d4f0c568 <unknown>
#15 0x5646d4f0c746 <unknown>
#16 0x5646d4f1d5f6 <unknown>
#17 0x7f2e3afb1134 <unknown>
[2025-05-07T04:10:46.702+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:10:46.715+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.584 seconds
[2025-05-07T04:11:21.194+0000] {processor.py:186} INFO - Started process (PID=4389) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:11:21.195+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:11:21.196+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:11:21.196+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:11:22.284+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:11:23.614+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:11:23.613+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563102bdf78a <unknown>
#1 0x5631026820a0 <unknown>
#2 0x5631026bc1ff <unknown>
#3 0x5631026b7c0f <unknown>
#4 0x563102707d75 <unknown>
#5 0x563102707296 <unknown>
#6 0x5631026f9173 <unknown>
#7 0x5631026c5d4b <unknown>
#8 0x5631026c69b1 <unknown>
#9 0x563102ba493b <unknown>
#10 0x563102ba883a <unknown>
#11 0x563102b8c692 <unknown>
#12 0x563102ba93c4 <unknown>
#13 0x563102b714cf <unknown>
#14 0x563102bcd568 <unknown>
#15 0x563102bcd746 <unknown>
#16 0x563102bde5f6 <unknown>
#17 0x7f989981b134 <unknown>
[2025-05-07T04:11:23.614+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:11:23.631+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.551 seconds
[2025-05-07T04:11:56.352+0000] {processor.py:186} INFO - Started process (PID=4538) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:11:56.354+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:11:56.355+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:11:56.354+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:12:01.026+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:12:01.636+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:12:01.635+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5654f2c8a78a <unknown>
#1 0x5654f272d0a0 <unknown>
#2 0x5654f27671ff <unknown>
#3 0x5654f2762c0f <unknown>
#4 0x5654f27b2d75 <unknown>
#5 0x5654f27b2296 <unknown>
#6 0x5654f27a4173 <unknown>
#7 0x5654f2770d4b <unknown>
#8 0x5654f27719b1 <unknown>
#9 0x5654f2c4f93b <unknown>
#10 0x5654f2c5383a <unknown>
#11 0x5654f2c37692 <unknown>
#12 0x5654f2c543c4 <unknown>
#13 0x5654f2c1c4cf <unknown>
#14 0x5654f2c78568 <unknown>
#15 0x5654f2c78746 <unknown>
#16 0x5654f2c895f6 <unknown>
#17 0x7fde13527134 <unknown>
[2025-05-07T04:12:01.637+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:12:01.652+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 11.915 seconds
[2025-05-07T04:12:32.567+0000] {processor.py:186} INFO - Started process (PID=4688) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:12:32.571+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:12:32.572+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:12:32.572+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:12:38.673+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:12:39.897+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:12:39.896+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a3c5c0f78a <unknown>
#1 0x55a3c56b20a0 <unknown>
#2 0x55a3c56ec1ff <unknown>
#3 0x55a3c56e7c0f <unknown>
#4 0x55a3c5737d75 <unknown>
#5 0x55a3c5737296 <unknown>
#6 0x55a3c5729173 <unknown>
#7 0x55a3c56f5d4b <unknown>
#8 0x55a3c56f69b1 <unknown>
#9 0x55a3c5bd493b <unknown>
#10 0x55a3c5bd883a <unknown>
#11 0x55a3c5bbc692 <unknown>
#12 0x55a3c5bd93c4 <unknown>
#13 0x55a3c5ba14cf <unknown>
#14 0x55a3c5bfd568 <unknown>
#15 0x55a3c5bfd746 <unknown>
#16 0x55a3c5c0e5f6 <unknown>
#17 0x7fa984902134 <unknown>
[2025-05-07T04:12:39.897+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:12:39.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.851 seconds
[2025-05-07T04:13:10.858+0000] {processor.py:186} INFO - Started process (PID=4835) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:13:10.859+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:13:10.860+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:13:10.860+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:13:16.327+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:13:17.514+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:13:17.513+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56381b03678a <unknown>
#1 0x56381aad90a0 <unknown>
#2 0x56381ab131ff <unknown>
#3 0x56381ab0ec0f <unknown>
#4 0x56381ab5ed75 <unknown>
#5 0x56381ab5e296 <unknown>
#6 0x56381ab50173 <unknown>
#7 0x56381ab1cd4b <unknown>
#8 0x56381ab1d9b1 <unknown>
#9 0x56381affb93b <unknown>
#10 0x56381afff83a <unknown>
#11 0x56381afe3692 <unknown>
#12 0x56381b0003c4 <unknown>
#13 0x56381afc84cf <unknown>
#14 0x56381b024568 <unknown>
#15 0x56381b024746 <unknown>
#16 0x56381b0355f6 <unknown>
#17 0x7fcf80135134 <unknown>
[2025-05-07T04:13:17.515+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:13:17.529+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.676 seconds
[2025-05-07T04:13:48.197+0000] {processor.py:186} INFO - Started process (PID=4985) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:13:48.198+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:13:48.199+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:13:48.199+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:13:54.274+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:13:55.440+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:13:55.439+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557dc754778a <unknown>
#1 0x557dc6fea0a0 <unknown>
#2 0x557dc70241ff <unknown>
#3 0x557dc701fc0f <unknown>
#4 0x557dc706fd75 <unknown>
#5 0x557dc706f296 <unknown>
#6 0x557dc7061173 <unknown>
#7 0x557dc702dd4b <unknown>
#8 0x557dc702e9b1 <unknown>
#9 0x557dc750c93b <unknown>
#10 0x557dc751083a <unknown>
#11 0x557dc74f4692 <unknown>
#12 0x557dc75113c4 <unknown>
#13 0x557dc74d94cf <unknown>
#14 0x557dc7535568 <unknown>
#15 0x557dc7535746 <unknown>
#16 0x557dc75465f6 <unknown>
#17 0x7ff82d518134 <unknown>
[2025-05-07T04:13:55.441+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:13:55.454+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.765 seconds
[2025-05-07T04:14:25.844+0000] {processor.py:186} INFO - Started process (PID=5134) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:14:25.848+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:14:25.849+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:14:25.849+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:14:31.303+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:14:32.552+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:14:32.551+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563b9764278a <unknown>
#1 0x563b970e50a0 <unknown>
#2 0x563b9711f1ff <unknown>
#3 0x563b9711ac0f <unknown>
#4 0x563b9716ad75 <unknown>
#5 0x563b9716a296 <unknown>
#6 0x563b9715c173 <unknown>
#7 0x563b97128d4b <unknown>
#8 0x563b971299b1 <unknown>
#9 0x563b9760793b <unknown>
#10 0x563b9760b83a <unknown>
#11 0x563b975ef692 <unknown>
#12 0x563b9760c3c4 <unknown>
#13 0x563b975d44cf <unknown>
#14 0x563b97630568 <unknown>
#15 0x563b97630746 <unknown>
#16 0x563b976415f6 <unknown>
#17 0x7f21cfabe134 <unknown>
[2025-05-07T04:14:32.553+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:14:32.565+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.722 seconds
[2025-05-07T04:15:03.445+0000] {processor.py:186} INFO - Started process (PID=5289) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:15:03.446+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:15:03.447+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:15:03.446+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:15:12.436+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:15:13.553+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:15:13.552+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x555c8cab678a <unknown>
#1 0x555c8c5590a0 <unknown>
#2 0x555c8c5931ff <unknown>
#3 0x555c8c58ec0f <unknown>
#4 0x555c8c5ded75 <unknown>
#5 0x555c8c5de296 <unknown>
#6 0x555c8c5d0173 <unknown>
#7 0x555c8c59cd4b <unknown>
#8 0x555c8c59d9b1 <unknown>
#9 0x555c8ca7b93b <unknown>
#10 0x555c8ca7f83a <unknown>
#11 0x555c8ca63692 <unknown>
#12 0x555c8ca803c4 <unknown>
#13 0x555c8ca484cf <unknown>
#14 0x555c8caa4568 <unknown>
#15 0x555c8caa4746 <unknown>
#16 0x555c8cab55f6 <unknown>
#17 0x7fa26fa8e134 <unknown>
[2025-05-07T04:15:13.554+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:15:13.566+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 11.125 seconds
[2025-05-07T04:15:43.616+0000] {processor.py:186} INFO - Started process (PID=5440) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:15:43.617+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:15:43.619+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:15:43.618+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:15:49.973+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:16:46.875+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:16:46.877+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:16:46.880+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:16:46.879+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:16:54.438+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:17:14.370+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:17:14.369+0000] {timeout.py:68} ERROR - Process timed out, PID: 55
[2025-05-07T04:17:44.631+0000] {processor.py:186} INFO - Started process (PID=276) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:17:44.632+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:17:44.633+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:17:44.633+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:17:51.576+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:17:54.351+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:17:54.349+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b60f4fd78a <unknown>
#1 0x55b60efa00a0 <unknown>
#2 0x55b60efda1ff <unknown>
#3 0x55b60efd5c0f <unknown>
#4 0x55b60f025d75 <unknown>
#5 0x55b60f025296 <unknown>
#6 0x55b60f017173 <unknown>
#7 0x55b60efe3d4b <unknown>
#8 0x55b60efe49b1 <unknown>
#9 0x55b60f4c293b <unknown>
#10 0x55b60f4c683a <unknown>
#11 0x55b60f4aa692 <unknown>
#12 0x55b60f4c73c4 <unknown>
#13 0x55b60f48f4cf <unknown>
#14 0x55b60f4eb568 <unknown>
#15 0x55b60f4eb746 <unknown>
#16 0x55b60f4fc5f6 <unknown>
#17 0x7fa28b134134 <unknown>
[2025-05-07T04:17:54.352+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:17:54.366+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 10.737 seconds
[2025-05-07T04:18:24.710+0000] {processor.py:186} INFO - Started process (PID=469) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:18:24.725+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:18:24.727+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:18:24.727+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:18:31.185+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:18:31.775+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:18:31.774+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5633f302878a <unknown>
#1 0x5633f2acb0a0 <unknown>
#2 0x5633f2b051ff <unknown>
#3 0x5633f2b00c0f <unknown>
#4 0x5633f2b50d75 <unknown>
#5 0x5633f2b50296 <unknown>
#6 0x5633f2b42173 <unknown>
#7 0x5633f2b0ed4b <unknown>
#8 0x5633f2b0f9b1 <unknown>
#9 0x5633f2fed93b <unknown>
#10 0x5633f2ff183a <unknown>
#11 0x5633f2fd5692 <unknown>
#12 0x5633f2ff23c4 <unknown>
#13 0x5633f2fba4cf <unknown>
#14 0x5633f3016568 <unknown>
#15 0x5633f3016746 <unknown>
#16 0x5633f30275f6 <unknown>
#17 0x7f0ecefa7134 <unknown>
[2025-05-07T04:18:31.776+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:18:31.799+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.095 seconds
[2025-05-07T04:19:01.846+0000] {processor.py:186} INFO - Started process (PID=613) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:19:01.847+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:19:01.849+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:19:01.849+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:19:07.966+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:19:09.067+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:19:09.066+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55871119978a <unknown>
#1 0x558710c3c0a0 <unknown>
#2 0x558710c761ff <unknown>
#3 0x558710c71c0f <unknown>
#4 0x558710cc1d75 <unknown>
#5 0x558710cc1296 <unknown>
#6 0x558710cb3173 <unknown>
#7 0x558710c7fd4b <unknown>
#8 0x558710c809b1 <unknown>
#9 0x55871115e93b <unknown>
#10 0x55871116283a <unknown>
#11 0x558711146692 <unknown>
#12 0x5587111633c4 <unknown>
#13 0x55871112b4cf <unknown>
#14 0x558711187568 <unknown>
#15 0x558711187746 <unknown>
#16 0x5587111985f6 <unknown>
#17 0x7f416b08f134 <unknown>
[2025-05-07T04:19:09.067+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:19:09.095+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.754 seconds
[2025-05-07T04:19:55.584+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:19:55.585+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:19:55.587+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:19:55.587+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:20:01.698+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:20:22.584+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:20:22.584+0000] {timeout.py:68} ERROR - Process timed out, PID: 55
[2025-05-07T04:20:53.290+0000] {processor.py:186} INFO - Started process (PID=286) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:20:53.302+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:20:53.305+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:20:53.305+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:20:59.780+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:21:02.792+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:21:02.789+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563935e8178a <unknown>
#1 0x5639359240a0 <unknown>
#2 0x56393595e1ff <unknown>
#3 0x563935959c0f <unknown>
#4 0x5639359a9d75 <unknown>
#5 0x5639359a9296 <unknown>
#6 0x56393599b173 <unknown>
#7 0x563935967d4b <unknown>
#8 0x5639359689b1 <unknown>
#9 0x563935e4693b <unknown>
#10 0x563935e4a83a <unknown>
#11 0x563935e2e692 <unknown>
#12 0x563935e4b3c4 <unknown>
#13 0x563935e134cf <unknown>
#14 0x563935e6f568 <unknown>
#15 0x563935e6f746 <unknown>
#16 0x563935e805f6 <unknown>
#17 0x7f0cd3b59134 <unknown>
[2025-05-07T04:21:02.793+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:21:02.807+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 10.528 seconds
[2025-05-07T04:21:37.235+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:21:37.236+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:21:37.238+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:21:37.238+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:21:37.797+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:21:39.007+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:21:39.005+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562bdb3a078a <unknown>
#1 0x562bdae430a0 <unknown>
#2 0x562bdae7d1ff <unknown>
#3 0x562bdae78c0f <unknown>
#4 0x562bdaec8d75 <unknown>
#5 0x562bdaec8296 <unknown>
#6 0x562bdaeba173 <unknown>
#7 0x562bdae86d4b <unknown>
#8 0x562bdae879b1 <unknown>
#9 0x562bdb36593b <unknown>
#10 0x562bdb36983a <unknown>
#11 0x562bdb34d692 <unknown>
#12 0x562bdb36a3c4 <unknown>
#13 0x562bdb3324cf <unknown>
#14 0x562bdb38e568 <unknown>
#15 0x562bdb38e746 <unknown>
#16 0x562bdb39f5f6 <unknown>
#17 0x7fab815f9134 <unknown>
[2025-05-07T04:21:39.007+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:21:39.024+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.912 seconds
[2025-05-07T04:22:09.133+0000] {processor.py:186} INFO - Started process (PID=638) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:22:09.134+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:22:09.136+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:22:09.136+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:22:16.034+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:22:22.392+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:22:22.391+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562838b1778a <unknown>
#1 0x5628385ba0a0 <unknown>
#2 0x5628385f41ff <unknown>
#3 0x5628385efc0f <unknown>
#4 0x56283863fd75 <unknown>
#5 0x56283863f296 <unknown>
#6 0x562838631173 <unknown>
#7 0x5628385fdd4b <unknown>
#8 0x5628385fe9b1 <unknown>
#9 0x562838adc93b <unknown>
#10 0x562838ae083a <unknown>
#11 0x562838ac4692 <unknown>
#12 0x562838ae13c4 <unknown>
#13 0x562838aa94cf <unknown>
#14 0x562838b05568 <unknown>
#15 0x562838b05746 <unknown>
#16 0x562838b165f6 <unknown>
#17 0x7f1e59499134 <unknown>
[2025-05-07T04:22:22.392+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:22:22.406+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.666 seconds
[2025-05-07T04:22:57.376+0000] {processor.py:186} INFO - Started process (PID=786) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:22:57.377+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:22:57.380+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:22:57.379+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:22:58.679+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:22:59.735+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:22:59.734+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557843bdf78a <unknown>
#1 0x5578436820a0 <unknown>
#2 0x5578436bc1ff <unknown>
#3 0x5578436b7c0f <unknown>
#4 0x557843707d75 <unknown>
#5 0x557843707296 <unknown>
#6 0x5578436f9173 <unknown>
#7 0x5578436c5d4b <unknown>
#8 0x5578436c69b1 <unknown>
#9 0x557843ba493b <unknown>
#10 0x557843ba883a <unknown>
#11 0x557843b8c692 <unknown>
#12 0x557843ba93c4 <unknown>
#13 0x557843b714cf <unknown>
#14 0x557843bcd568 <unknown>
#15 0x557843bcd746 <unknown>
#16 0x557843bde5f6 <unknown>
#17 0x7f4990f4c134 <unknown>
[2025-05-07T04:22:59.736+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:22:59.749+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.488 seconds
[2025-05-07T04:23:30.503+0000] {processor.py:186} INFO - Started process (PID=936) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:23:30.505+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:23:30.509+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:23:30.508+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:23:36.783+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:23:37.494+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:23:37.493+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d62f0af78a <unknown>
#1 0x55d62eb520a0 <unknown>
#2 0x55d62eb8c1ff <unknown>
#3 0x55d62eb87c0f <unknown>
#4 0x55d62ebd7d75 <unknown>
#5 0x55d62ebd7296 <unknown>
#6 0x55d62ebc9173 <unknown>
#7 0x55d62eb95d4b <unknown>
#8 0x55d62eb969b1 <unknown>
#9 0x55d62f07493b <unknown>
#10 0x55d62f07883a <unknown>
#11 0x55d62f05c692 <unknown>
#12 0x55d62f0793c4 <unknown>
#13 0x55d62f0414cf <unknown>
#14 0x55d62f09d568 <unknown>
#15 0x55d62f09d746 <unknown>
#16 0x55d62f0ae5f6 <unknown>
#17 0x7f6631c31134 <unknown>
[2025-05-07T04:23:37.495+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:23:37.509+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.015 seconds
[2025-05-07T04:24:08.239+0000] {processor.py:186} INFO - Started process (PID=1099) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:24:08.242+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:24:08.244+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:24:08.244+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:24:14.887+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:24:16.147+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:24:16.144+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f39760578a <unknown>
#1 0x55f3970a80a0 <unknown>
#2 0x55f3970e21ff <unknown>
#3 0x55f3970ddc0f <unknown>
#4 0x55f39712dd75 <unknown>
#5 0x55f39712d296 <unknown>
#6 0x55f39711f173 <unknown>
#7 0x55f3970ebd4b <unknown>
#8 0x55f3970ec9b1 <unknown>
#9 0x55f3975ca93b <unknown>
#10 0x55f3975ce83a <unknown>
#11 0x55f3975b2692 <unknown>
#12 0x55f3975cf3c4 <unknown>
#13 0x55f3975974cf <unknown>
#14 0x55f3975f3568 <unknown>
#15 0x55f3975f3746 <unknown>
#16 0x55f3976045f6 <unknown>
#17 0x7f3b589a0134 <unknown>
[2025-05-07T04:24:16.147+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:24:16.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.426 seconds
[2025-05-07T04:24:46.578+0000] {processor.py:186} INFO - Started process (PID=1253) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:24:46.587+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:24:46.589+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:24:46.589+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:24:52.245+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:24:53.361+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:24:53.359+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x564afa85178a <unknown>
#1 0x564afa2f40a0 <unknown>
#2 0x564afa32e1ff <unknown>
#3 0x564afa329c0f <unknown>
#4 0x564afa379d75 <unknown>
#5 0x564afa379296 <unknown>
#6 0x564afa36b173 <unknown>
#7 0x564afa337d4b <unknown>
#8 0x564afa3389b1 <unknown>
#9 0x564afa81693b <unknown>
#10 0x564afa81a83a <unknown>
#11 0x564afa7fe692 <unknown>
#12 0x564afa81b3c4 <unknown>
#13 0x564afa7e34cf <unknown>
#14 0x564afa83f568 <unknown>
#15 0x564afa83f746 <unknown>
#16 0x564afa8505f6 <unknown>
#17 0x7fd5e7a4e134 <unknown>
[2025-05-07T04:24:53.362+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:24:53.394+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.823 seconds
[2025-05-07T04:25:23.521+0000] {processor.py:186} INFO - Started process (PID=1392) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:25:23.522+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:25:23.524+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:25:23.524+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:25:29.586+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:25:30.770+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:25:30.769+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e25521878a <unknown>
#1 0x55e254cbb0a0 <unknown>
#2 0x55e254cf51ff <unknown>
#3 0x55e254cf0c0f <unknown>
#4 0x55e254d40d75 <unknown>
#5 0x55e254d40296 <unknown>
#6 0x55e254d32173 <unknown>
#7 0x55e254cfed4b <unknown>
#8 0x55e254cff9b1 <unknown>
#9 0x55e2551dd93b <unknown>
#10 0x55e2551e183a <unknown>
#11 0x55e2551c5692 <unknown>
#12 0x55e2551e23c4 <unknown>
#13 0x55e2551aa4cf <unknown>
#14 0x55e255206568 <unknown>
#15 0x55e255206746 <unknown>
#16 0x55e2552175f6 <unknown>
#17 0x7f0df6c43134 <unknown>
[2025-05-07T04:25:30.770+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:25:30.784+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.772 seconds
[2025-05-07T04:26:01.469+0000] {processor.py:186} INFO - Started process (PID=1547) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:26:01.470+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:26:01.472+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:26:01.471+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:26:07.533+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:26:08.781+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:26:08.780+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ab9a9ae78a <unknown>
#1 0x55ab9a4510a0 <unknown>
#2 0x55ab9a48b1ff <unknown>
#3 0x55ab9a486c0f <unknown>
#4 0x55ab9a4d6d75 <unknown>
#5 0x55ab9a4d6296 <unknown>
#6 0x55ab9a4c8173 <unknown>
#7 0x55ab9a494d4b <unknown>
#8 0x55ab9a4959b1 <unknown>
#9 0x55ab9a97393b <unknown>
#10 0x55ab9a97783a <unknown>
#11 0x55ab9a95b692 <unknown>
#12 0x55ab9a9783c4 <unknown>
#13 0x55ab9a9404cf <unknown>
#14 0x55ab9a99c568 <unknown>
#15 0x55ab9a99c746 <unknown>
#16 0x55ab9a9ad5f6 <unknown>
#17 0x7f1140a74134 <unknown>
[2025-05-07T04:26:08.782+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:26:08.797+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.336 seconds
[2025-05-07T04:28:09.315+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:28:09.318+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:28:09.326+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:28:09.325+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:28:16.641+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:28:36.811+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:28:36.810+0000] {timeout.py:68} ERROR - Process timed out, PID: 55
[2025-05-07T04:29:06.878+0000] {processor.py:186} INFO - Started process (PID=283) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:29:06.878+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:29:06.880+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:29:06.880+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:29:13.526+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:29:16.727+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:29:16.714+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ce447a278a <unknown>
#1 0x55ce442450a0 <unknown>
#2 0x55ce4427f1ff <unknown>
#3 0x55ce4427ac0f <unknown>
#4 0x55ce442cad75 <unknown>
#5 0x55ce442ca296 <unknown>
#6 0x55ce442bc173 <unknown>
#7 0x55ce44288d4b <unknown>
#8 0x55ce442899b1 <unknown>
#9 0x55ce4476793b <unknown>
#10 0x55ce4476b83a <unknown>
#11 0x55ce4474f692 <unknown>
#12 0x55ce4476c3c4 <unknown>
#13 0x55ce447344cf <unknown>
#14 0x55ce44790568 <unknown>
#15 0x55ce44790746 <unknown>
#16 0x55ce447a15f6 <unknown>
#17 0x7f029cb09134 <unknown>
[2025-05-07T04:29:16.728+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:29:16.739+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 10.869 seconds
[2025-05-07T04:29:48.139+0000] {processor.py:186} INFO - Started process (PID=485) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:29:48.140+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:29:48.142+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:29:48.141+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:29:48.962+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:29:50.202+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:29:50.201+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5630c502278a <unknown>
#1 0x5630c4ac50a0 <unknown>
#2 0x5630c4aff1ff <unknown>
#3 0x5630c4afac0f <unknown>
#4 0x5630c4b4ad75 <unknown>
#5 0x5630c4b4a296 <unknown>
#6 0x5630c4b3c173 <unknown>
#7 0x5630c4b08d4b <unknown>
#8 0x5630c4b099b1 <unknown>
#9 0x5630c4fe793b <unknown>
#10 0x5630c4feb83a <unknown>
#11 0x5630c4fcf692 <unknown>
#12 0x5630c4fec3c4 <unknown>
#13 0x5630c4fb44cf <unknown>
#14 0x5630c5010568 <unknown>
#15 0x5630c5010746 <unknown>
#16 0x5630c50215f6 <unknown>
#17 0x7f7f2203b134 <unknown>
[2025-05-07T04:29:50.203+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:29:50.217+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.193 seconds
[2025-05-07T04:30:23.042+0000] {processor.py:186} INFO - Started process (PID=624) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:30:23.043+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:30:23.045+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:30:23.044+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:30:23.609+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:30:24.776+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:30:24.775+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56187839d78a <unknown>
#1 0x561877e400a0 <unknown>
#2 0x561877e7a1ff <unknown>
#3 0x561877e75c0f <unknown>
#4 0x561877ec5d75 <unknown>
#5 0x561877ec5296 <unknown>
#6 0x561877eb7173 <unknown>
#7 0x561877e83d4b <unknown>
#8 0x561877e849b1 <unknown>
#9 0x56187836293b <unknown>
#10 0x56187836683a <unknown>
#11 0x56187834a692 <unknown>
#12 0x5618783673c4 <unknown>
#13 0x56187832f4cf <unknown>
#14 0x56187838b568 <unknown>
#15 0x56187838b746 <unknown>
#16 0x56187839c5f6 <unknown>
#17 0x7fb4c5fdf134 <unknown>
[2025-05-07T04:30:24.776+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:30:24.787+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.863 seconds
[2025-05-07T04:30:54.934+0000] {processor.py:186} INFO - Started process (PID=771) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:30:54.936+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:30:54.939+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:30:54.938+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:31:01.217+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:31:02.443+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:31:02.442+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5608c188178a <unknown>
#1 0x5608c13240a0 <unknown>
#2 0x5608c135e1ff <unknown>
#3 0x5608c1359c0f <unknown>
#4 0x5608c13a9d75 <unknown>
#5 0x5608c13a9296 <unknown>
#6 0x5608c139b173 <unknown>
#7 0x5608c1367d4b <unknown>
#8 0x5608c13689b1 <unknown>
#9 0x5608c184693b <unknown>
#10 0x5608c184a83a <unknown>
#11 0x5608c182e692 <unknown>
#12 0x5608c184b3c4 <unknown>
#13 0x5608c18134cf <unknown>
#14 0x5608c186f568 <unknown>
#15 0x5608c186f746 <unknown>
#16 0x5608c18805f6 <unknown>
#17 0x7fa97e4a7134 <unknown>
[2025-05-07T04:31:02.443+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:31:02.456+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.031 seconds
[2025-05-07T04:34:14.134+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:34:14.135+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:34:14.137+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:34:14.137+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:34:21.794+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:34:41.628+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:34:41.627+0000] {timeout.py:68} ERROR - Process timed out, PID: 55
[2025-05-07T04:35:13.488+0000] {processor.py:186} INFO - Started process (PID=259) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:35:13.489+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:35:13.491+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:35:13.490+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:35:14.742+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:35:15.873+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:35:15.872+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:35:16.825+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:35:16.821+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5558f82e5a8e <unknown>
#1 0x5558f7da2b0b <unknown>
#2 0x5558f7dd94ea <unknown>
#3 0x5558f7dd4aef <unknown>
#4 0x5558f7e28b18 <unknown>
#5 0x5558f7e169b3 <unknown>
#6 0x5558f7de0c59 <unknown>
#7 0x5558f7de1a08 <unknown>
#8 0x5558f82b240a <unknown>
#9 0x5558f82b585e <unknown>
#10 0x5558f82b5308 <unknown>
#11 0x5558f82b5ce5 <unknown>
#12 0x5558f829bb7b <unknown>
#13 0x5558f82b6050 <unknown>
#14 0x5558f8284ae9 <unknown>
#15 0x5558f82d4df5 <unknown>
#16 0x5558f82d4fdb <unknown>
#17 0x5558f82e4c05 <unknown>
#18 0x7f0588018134 <unknown>
[2025-05-07T04:35:16.825+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:35:16.841+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.476 seconds
[2025-05-07T04:35:46.996+0000] {processor.py:186} INFO - Started process (PID=442) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:35:47.013+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:35:47.014+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:35:47.014+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:35:53.264+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:35:58.705+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:35:58.705+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:35:53.862+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:35:53.861+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e0a4332a8e <unknown>
#1 0x55e0a3defb0b <unknown>
#2 0x55e0a3e264ea <unknown>
#3 0x55e0a3e21aef <unknown>
#4 0x55e0a3e75b18 <unknown>
#5 0x55e0a3e639b3 <unknown>
#6 0x55e0a3e2dc59 <unknown>
#7 0x55e0a3e2ea08 <unknown>
#8 0x55e0a42ff40a <unknown>
#9 0x55e0a430285e <unknown>
#10 0x55e0a4302308 <unknown>
#11 0x55e0a4302ce5 <unknown>
#12 0x55e0a42e8b7b <unknown>
#13 0x55e0a4303050 <unknown>
#14 0x55e0a42d1ae9 <unknown>
#15 0x55e0a4321df5 <unknown>
#16 0x55e0a4321fdb <unknown>
#17 0x55e0a4331c05 <unknown>
#18 0x7f64a81ff134 <unknown>
[2025-05-07T04:35:53.863+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:35:53.876+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.879 seconds
[2025-05-07T04:36:24.689+0000] {processor.py:186} INFO - Started process (PID=584) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:36:24.690+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:36:24.692+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:36:24.691+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:36:30.841+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:36:31.168+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:36:31.168+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:36:32.048+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:36:32.047+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b232f4fa8e <unknown>
#1 0x55b232a0cb0b <unknown>
#2 0x55b232a434ea <unknown>
#3 0x55b232a3eaef <unknown>
#4 0x55b232a92b18 <unknown>
#5 0x55b232a809b3 <unknown>
#6 0x55b232a4ac59 <unknown>
#7 0x55b232a4ba08 <unknown>
#8 0x55b232f1c40a <unknown>
#9 0x55b232f1f85e <unknown>
#10 0x55b232f1f308 <unknown>
#11 0x55b232f1fce5 <unknown>
#12 0x55b232f05b7b <unknown>
#13 0x55b232f20050 <unknown>
#14 0x55b232eeeae9 <unknown>
#15 0x55b232f3edf5 <unknown>
#16 0x55b232f3efdb <unknown>
#17 0x55b232f4ec05 <unknown>
#18 0x7fbbfb93f134 <unknown>
[2025-05-07T04:36:32.049+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:36:32.063+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.882 seconds
[2025-05-07T04:37:02.962+0000] {processor.py:186} INFO - Started process (PID=752) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:37:02.963+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:37:02.964+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:37:02.964+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:37:08.575+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:37:08.971+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:37:08.971+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:37:09.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:37:09.900+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561bf04fca8e <unknown>
#1 0x561beffb9b0b <unknown>
#2 0x561befff04ea <unknown>
#3 0x561beffebaef <unknown>
#4 0x561bf003fb18 <unknown>
#5 0x561bf002d9b3 <unknown>
#6 0x561befff7c59 <unknown>
#7 0x561befff8a08 <unknown>
#8 0x561bf04c940a <unknown>
#9 0x561bf04cc85e <unknown>
#10 0x561bf04cc308 <unknown>
#11 0x561bf04ccce5 <unknown>
#12 0x561bf04b2b7b <unknown>
#13 0x561bf04cd050 <unknown>
#14 0x561bf049bae9 <unknown>
#15 0x561bf04ebdf5 <unknown>
#16 0x561bf04ebfdb <unknown>
#17 0x561bf04fbc05 <unknown>
#18 0x7f552b5b8134 <unknown>
[2025-05-07T04:37:09.902+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:37:09.916+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.966 seconds
[2025-05-07T04:37:40.438+0000] {processor.py:186} INFO - Started process (PID=895) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:37:40.439+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:37:40.441+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:37:40.441+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:37:47.349+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:37:47.669+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:37:47.669+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:37:48.552+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:37:48.551+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5606a2809a8e <unknown>
#1 0x5606a22c6b0b <unknown>
#2 0x5606a22fd4ea <unknown>
#3 0x5606a22f8aef <unknown>
#4 0x5606a234cb18 <unknown>
#5 0x5606a233a9b3 <unknown>
#6 0x5606a2304c59 <unknown>
#7 0x5606a2305a08 <unknown>
#8 0x5606a27d640a <unknown>
#9 0x5606a27d985e <unknown>
#10 0x5606a27d9308 <unknown>
#11 0x5606a27d9ce5 <unknown>
#12 0x5606a27bfb7b <unknown>
#13 0x5606a27da050 <unknown>
#14 0x5606a27a8ae9 <unknown>
#15 0x5606a27f8df5 <unknown>
#16 0x5606a27f8fdb <unknown>
#17 0x5606a2808c05 <unknown>
#18 0x7efe4f399134 <unknown>
[2025-05-07T04:37:48.553+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:37:48.566+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.628 seconds
[2025-05-07T04:38:19.527+0000] {processor.py:186} INFO - Started process (PID=1047) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:38:19.531+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:38:19.533+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:38:19.533+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:38:25.974+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:38:26.182+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:38:26.182+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:38:27.016+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:38:27.015+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b02c936a8e <unknown>
#1 0x55b02c3f3b0b <unknown>
#2 0x55b02c42a4ea <unknown>
#3 0x55b02c425aef <unknown>
#4 0x55b02c479b18 <unknown>
#5 0x55b02c4679b3 <unknown>
#6 0x55b02c431c59 <unknown>
#7 0x55b02c432a08 <unknown>
#8 0x55b02c90340a <unknown>
#9 0x55b02c90685e <unknown>
#10 0x55b02c906308 <unknown>
#11 0x55b02c906ce5 <unknown>
#12 0x55b02c8ecb7b <unknown>
#13 0x55b02c907050 <unknown>
#14 0x55b02c8d5ae9 <unknown>
#15 0x55b02c925df5 <unknown>
#16 0x55b02c925fdb <unknown>
#17 0x55b02c935c05 <unknown>
#18 0x7ff331f59134 <unknown>
[2025-05-07T04:38:27.017+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:38:27.043+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.024 seconds
[2025-05-07T04:38:57.840+0000] {processor.py:186} INFO - Started process (PID=1202) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:38:57.842+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:38:57.843+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:38:57.843+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:39:08.906+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:39:03.614+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:39:03.614+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:39:04.442+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:39:04.441+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55721d148a8e <unknown>
#1 0x55721cc05b0b <unknown>
#2 0x55721cc3c4ea <unknown>
#3 0x55721cc37aef <unknown>
#4 0x55721cc8bb18 <unknown>
#5 0x55721cc799b3 <unknown>
#6 0x55721cc43c59 <unknown>
#7 0x55721cc44a08 <unknown>
#8 0x55721d11540a <unknown>
#9 0x55721d11885e <unknown>
#10 0x55721d118308 <unknown>
#11 0x55721d118ce5 <unknown>
#12 0x55721d0feb7b <unknown>
#13 0x55721d119050 <unknown>
#14 0x55721d0e7ae9 <unknown>
#15 0x55721d137df5 <unknown>
#16 0x55721d137fdb <unknown>
#17 0x55721d147c05 <unknown>
#18 0x7f171fb76134 <unknown>
[2025-05-07T04:39:04.442+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:39:04.458+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.625 seconds
[2025-05-07T04:39:34.548+0000] {processor.py:186} INFO - Started process (PID=1347) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:39:34.549+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:39:34.551+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:39:34.551+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:39:41.210+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:39:41.413+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:39:41.413+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:39:42.296+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:39:42.295+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5629bd373a8e <unknown>
#1 0x5629bce30b0b <unknown>
#2 0x5629bce674ea <unknown>
#3 0x5629bce62aef <unknown>
#4 0x5629bceb6b18 <unknown>
#5 0x5629bcea49b3 <unknown>
#6 0x5629bce6ec59 <unknown>
#7 0x5629bce6fa08 <unknown>
#8 0x5629bd34040a <unknown>
#9 0x5629bd34385e <unknown>
#10 0x5629bd343308 <unknown>
#11 0x5629bd343ce5 <unknown>
#12 0x5629bd329b7b <unknown>
#13 0x5629bd344050 <unknown>
#14 0x5629bd312ae9 <unknown>
#15 0x5629bd362df5 <unknown>
#16 0x5629bd362fdb <unknown>
#17 0x5629bd372c05 <unknown>
#18 0x7f4d16eb9134 <unknown>
[2025-05-07T04:39:42.296+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:39:42.314+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.279 seconds
[2025-05-07T04:40:13.326+0000] {processor.py:186} INFO - Started process (PID=1500) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:40:13.338+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:40:13.340+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:40:13.340+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:40:18.929+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:40:19.273+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:40:19.272+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:40:20.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:40:20.157+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x565161b98a8e <unknown>
#1 0x565161655b0b <unknown>
#2 0x56516168c4ea <unknown>
#3 0x565161687aef <unknown>
#4 0x5651616dbb18 <unknown>
#5 0x5651616c99b3 <unknown>
#6 0x565161693c59 <unknown>
#7 0x565161694a08 <unknown>
#8 0x565161b6540a <unknown>
#9 0x565161b6885e <unknown>
#10 0x565161b68308 <unknown>
#11 0x565161b68ce5 <unknown>
#12 0x565161b4eb7b <unknown>
#13 0x565161b69050 <unknown>
#14 0x565161b37ae9 <unknown>
#15 0x565161b87df5 <unknown>
#16 0x565161b87fdb <unknown>
#17 0x565161b97c05 <unknown>
#18 0x7f16ec843134 <unknown>
[2025-05-07T04:40:20.159+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:40:20.174+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.852 seconds
[2025-05-07T04:40:54.174+0000] {processor.py:186} INFO - Started process (PID=1643) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:40:54.175+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:40:54.177+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:40:54.177+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:40:55.127+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:40:55.465+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:40:55.465+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:40:56.431+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:40:56.430+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559a96b6aa8e <unknown>
#1 0x559a96627b0b <unknown>
#2 0x559a9665e4ea <unknown>
#3 0x559a96659aef <unknown>
#4 0x559a966adb18 <unknown>
#5 0x559a9669b9b3 <unknown>
#6 0x559a96665c59 <unknown>
#7 0x559a96666a08 <unknown>
#8 0x559a96b3740a <unknown>
#9 0x559a96b3a85e <unknown>
#10 0x559a96b3a308 <unknown>
#11 0x559a96b3ace5 <unknown>
#12 0x559a96b20b7b <unknown>
#13 0x559a96b3b050 <unknown>
#14 0x559a96b09ae9 <unknown>
#15 0x559a96b59df5 <unknown>
#16 0x559a96b59fdb <unknown>
#17 0x559a96b69c05 <unknown>
#18 0x7fcb3987d134 <unknown>
[2025-05-07T04:40:56.432+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:40:56.447+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.401 seconds
[2025-05-07T04:41:29.223+0000] {processor.py:186} INFO - Started process (PID=1785) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:41:29.224+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:41:29.226+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:41:29.226+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:41:30.243+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:41:30.563+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:41:30.563+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:41:31.393+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:41:31.392+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b9b36c9a8e <unknown>
#1 0x55b9b3186b0b <unknown>
#2 0x55b9b31bd4ea <unknown>
#3 0x55b9b31b8aef <unknown>
#4 0x55b9b320cb18 <unknown>
#5 0x55b9b31fa9b3 <unknown>
#6 0x55b9b31c4c59 <unknown>
#7 0x55b9b31c5a08 <unknown>
#8 0x55b9b369640a <unknown>
#9 0x55b9b369985e <unknown>
#10 0x55b9b3699308 <unknown>
#11 0x55b9b3699ce5 <unknown>
#12 0x55b9b367fb7b <unknown>
#13 0x55b9b369a050 <unknown>
#14 0x55b9b3668ae9 <unknown>
#15 0x55b9b36b8df5 <unknown>
#16 0x55b9b36b8fdb <unknown>
#17 0x55b9b36c8c05 <unknown>
#18 0x7f91ef374134 <unknown>
[2025-05-07T04:41:31.393+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:41:31.407+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.307 seconds
[2025-05-07T04:42:02.381+0000] {processor.py:186} INFO - Started process (PID=1937) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:42:02.385+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:42:02.386+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:42:02.386+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:42:08.972+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:42:14.403+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:42:14.403+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:42:09.710+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:42:09.709+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b2182c0a8e <unknown>
#1 0x55b217d7db0b <unknown>
#2 0x55b217db44ea <unknown>
#3 0x55b217dafaef <unknown>
#4 0x55b217e03b18 <unknown>
#5 0x55b217df19b3 <unknown>
#6 0x55b217dbbc59 <unknown>
#7 0x55b217dbca08 <unknown>
#8 0x55b21828d40a <unknown>
#9 0x55b21829085e <unknown>
#10 0x55b218290308 <unknown>
#11 0x55b218290ce5 <unknown>
#12 0x55b218276b7b <unknown>
#13 0x55b218291050 <unknown>
#14 0x55b21825fae9 <unknown>
#15 0x55b2182afdf5 <unknown>
#16 0x55b2182affdb <unknown>
#17 0x55b2182bfc05 <unknown>
#18 0x7fa40127c134 <unknown>
[2025-05-07T04:42:09.711+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:42:09.726+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.359 seconds
[2025-05-07T04:42:40.072+0000] {processor.py:186} INFO - Started process (PID=2081) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:42:40.074+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:42:40.079+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:42:40.079+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:42:47.252+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:42:47.617+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:42:47.617+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:42:48.499+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:42:48.498+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56379c1aca8e <unknown>
#1 0x56379bc69b0b <unknown>
#2 0x56379bca04ea <unknown>
#3 0x56379bc9baef <unknown>
#4 0x56379bcefb18 <unknown>
#5 0x56379bcdd9b3 <unknown>
#6 0x56379bca7c59 <unknown>
#7 0x56379bca8a08 <unknown>
#8 0x56379c17940a <unknown>
#9 0x56379c17c85e <unknown>
#10 0x56379c17c308 <unknown>
#11 0x56379c17cce5 <unknown>
#12 0x56379c162b7b <unknown>
#13 0x56379c17d050 <unknown>
#14 0x56379c14bae9 <unknown>
#15 0x56379c19bdf5 <unknown>
#16 0x56379c19bfdb <unknown>
#17 0x56379c1abc05 <unknown>
#18 0x7fbeaa1bc134 <unknown>
[2025-05-07T04:42:48.500+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:42:48.515+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.953 seconds
[2025-05-07T04:43:18.677+0000] {processor.py:186} INFO - Started process (PID=2226) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:43:18.681+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:43:18.683+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:43:18.682+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:43:24.526+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:43:24.698+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:43:24.697+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:43:25.579+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:43:25.578+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55573fbc9a8e <unknown>
#1 0x55573f686b0b <unknown>
#2 0x55573f6bd4ea <unknown>
#3 0x55573f6b8aef <unknown>
#4 0x55573f70cb18 <unknown>
#5 0x55573f6fa9b3 <unknown>
#6 0x55573f6c4c59 <unknown>
#7 0x55573f6c5a08 <unknown>
#8 0x55573fb9640a <unknown>
#9 0x55573fb9985e <unknown>
#10 0x55573fb99308 <unknown>
#11 0x55573fb99ce5 <unknown>
#12 0x55573fb7fb7b <unknown>
#13 0x55573fb9a050 <unknown>
#14 0x55573fb68ae9 <unknown>
#15 0x55573fbb8df5 <unknown>
#16 0x55573fbb8fdb <unknown>
#17 0x55573fbc8c05 <unknown>
#18 0x7f1b7bb2b134 <unknown>
[2025-05-07T04:43:25.579+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:43:25.593+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.918 seconds
[2025-05-07T04:43:59.415+0000] {processor.py:186} INFO - Started process (PID=2380) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:43:59.416+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:43:59.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:43:59.417+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:44:00.485+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:44:00.713+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:44:00.713+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:44:01.663+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:44:01.661+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55975d377a8e <unknown>
#1 0x55975ce34b0b <unknown>
#2 0x55975ce6b4ea <unknown>
#3 0x55975ce66aef <unknown>
#4 0x55975cebab18 <unknown>
#5 0x55975cea89b3 <unknown>
#6 0x55975ce72c59 <unknown>
#7 0x55975ce73a08 <unknown>
#8 0x55975d34440a <unknown>
#9 0x55975d34785e <unknown>
#10 0x55975d347308 <unknown>
#11 0x55975d347ce5 <unknown>
#12 0x55975d32db7b <unknown>
#13 0x55975d348050 <unknown>
#14 0x55975d316ae9 <unknown>
#15 0x55975d366df5 <unknown>
#16 0x55975d366fdb <unknown>
#17 0x55975d376c05 <unknown>
#18 0x7f94ddde7134 <unknown>
[2025-05-07T04:44:01.664+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:44:01.682+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.399 seconds
[2025-05-07T04:44:32.472+0000] {processor.py:186} INFO - Started process (PID=2522) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:44:32.473+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:44:32.475+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:44:32.474+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:44:44.497+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:44:39.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:44:39.050+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:44:39.982+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:44:39.982+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563f02153a8e <unknown>
#1 0x563f01c10b0b <unknown>
#2 0x563f01c474ea <unknown>
#3 0x563f01c42aef <unknown>
#4 0x563f01c96b18 <unknown>
#5 0x563f01c849b3 <unknown>
#6 0x563f01c4ec59 <unknown>
#7 0x563f01c4fa08 <unknown>
#8 0x563f0212040a <unknown>
#9 0x563f0212385e <unknown>
#10 0x563f02123308 <unknown>
#11 0x563f02123ce5 <unknown>
#12 0x563f02109b7b <unknown>
#13 0x563f02124050 <unknown>
#14 0x563f020f2ae9 <unknown>
#15 0x563f02142df5 <unknown>
#16 0x563f02142fdb <unknown>
#17 0x563f02152c05 <unknown>
#18 0x7face713d134 <unknown>
[2025-05-07T04:44:39.983+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:44:39.999+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.544 seconds
[2025-05-07T04:45:14.592+0000] {processor.py:186} INFO - Started process (PID=2674) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:45:14.593+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:45:14.595+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:45:14.595+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:45:15.475+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:45:15.899+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:45:15.898+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:45:16.799+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:45:16.798+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562635c34a8e <unknown>
#1 0x5626356f1b0b <unknown>
#2 0x5626357284ea <unknown>
#3 0x562635723aef <unknown>
#4 0x562635777b18 <unknown>
#5 0x5626357659b3 <unknown>
#6 0x56263572fc59 <unknown>
#7 0x562635730a08 <unknown>
#8 0x562635c0140a <unknown>
#9 0x562635c0485e <unknown>
#10 0x562635c04308 <unknown>
#11 0x562635c04ce5 <unknown>
#12 0x562635beab7b <unknown>
#13 0x562635c05050 <unknown>
#14 0x562635bd3ae9 <unknown>
#15 0x562635c23df5 <unknown>
#16 0x562635c23fdb <unknown>
#17 0x562635c33c05 <unknown>
#18 0x7f69147d4134 <unknown>
[2025-05-07T04:45:16.800+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:45:16.816+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.350 seconds
[2025-05-07T04:45:47.745+0000] {processor.py:186} INFO - Started process (PID=2826) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:45:47.749+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:45:47.751+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:45:47.751+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:45:53.751+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:45:53.982+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:45:53.982+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:45:54.359+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:45:54.358+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56520cc5aa8e <unknown>
#1 0x56520c717b0b <unknown>
#2 0x56520c74e4ea <unknown>
#3 0x56520c749aef <unknown>
#4 0x56520c79db18 <unknown>
#5 0x56520c78b9b3 <unknown>
#6 0x56520c755c59 <unknown>
#7 0x56520c756a08 <unknown>
#8 0x56520cc2740a <unknown>
#9 0x56520cc2a85e <unknown>
#10 0x56520cc2a308 <unknown>
#11 0x56520cc2ace5 <unknown>
#12 0x56520cc10b7b <unknown>
#13 0x56520cc2b050 <unknown>
#14 0x56520cbf9ae9 <unknown>
#15 0x56520cc49df5 <unknown>
#16 0x56520cc49fdb <unknown>
#17 0x56520cc59c05 <unknown>
#18 0x7fd76e803134 <unknown>
[2025-05-07T04:45:54.359+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:45:54.374+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.638 seconds
[2025-05-07T04:46:29.655+0000] {processor.py:186} INFO - Started process (PID=2961) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:46:29.656+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:46:29.659+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:46:29.658+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:46:30.810+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:46:31.115+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:46:31.115+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:46:31.891+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:46:31.890+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55fbfd026a8e <unknown>
#1 0x55fbfcae3b0b <unknown>
#2 0x55fbfcb1a4ea <unknown>
#3 0x55fbfcb15aef <unknown>
#4 0x55fbfcb69b18 <unknown>
#5 0x55fbfcb579b3 <unknown>
#6 0x55fbfcb21c59 <unknown>
#7 0x55fbfcb22a08 <unknown>
#8 0x55fbfcff340a <unknown>
#9 0x55fbfcff685e <unknown>
#10 0x55fbfcff6308 <unknown>
#11 0x55fbfcff6ce5 <unknown>
#12 0x55fbfcfdcb7b <unknown>
#13 0x55fbfcff7050 <unknown>
#14 0x55fbfcfc5ae9 <unknown>
#15 0x55fbfd015df5 <unknown>
#16 0x55fbfd015fdb <unknown>
#17 0x55fbfd025c05 <unknown>
#18 0x7fd3831be134 <unknown>
[2025-05-07T04:46:31.891+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:46:31.905+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.381 seconds
[2025-05-07T04:47:02.524+0000] {processor.py:186} INFO - Started process (PID=3108) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:47:02.525+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:47:02.527+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:47:02.527+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:47:08.588+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:47:09.010+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:47:09.010+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:47:09.390+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:47:09.388+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c26e2b8a8e <unknown>
#1 0x55c26dd75b0b <unknown>
#2 0x55c26ddac4ea <unknown>
#3 0x55c26dda7aef <unknown>
#4 0x55c26ddfbb18 <unknown>
#5 0x55c26dde99b3 <unknown>
#6 0x55c26ddb3c59 <unknown>
#7 0x55c26ddb4a08 <unknown>
#8 0x55c26e28540a <unknown>
#9 0x55c26e28885e <unknown>
#10 0x55c26e288308 <unknown>
#11 0x55c26e288ce5 <unknown>
#12 0x55c26e26eb7b <unknown>
#13 0x55c26e289050 <unknown>
#14 0x55c26e257ae9 <unknown>
#15 0x55c26e2a7df5 <unknown>
#16 0x55c26e2a7fdb <unknown>
#17 0x55c26e2b7c05 <unknown>
#18 0x7f8b239b2134 <unknown>
[2025-05-07T04:47:09.391+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:47:09.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.895 seconds
[2025-05-07T04:47:40.393+0000] {processor.py:186} INFO - Started process (PID=3256) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:47:40.394+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:47:40.396+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:47:40.396+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:47:46.984+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:47:47.323+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:47:47.323+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:47:48.205+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:47:48.202+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563e84425a8e <unknown>
#1 0x563e83ee2b0b <unknown>
#2 0x563e83f194ea <unknown>
#3 0x563e83f14aef <unknown>
#4 0x563e83f68b18 <unknown>
#5 0x563e83f569b3 <unknown>
#6 0x563e83f20c59 <unknown>
#7 0x563e83f21a08 <unknown>
#8 0x563e843f240a <unknown>
#9 0x563e843f585e <unknown>
#10 0x563e843f5308 <unknown>
#11 0x563e843f5ce5 <unknown>
#12 0x563e843dbb7b <unknown>
#13 0x563e843f6050 <unknown>
#14 0x563e843c4ae9 <unknown>
#15 0x563e84414df5 <unknown>
#16 0x563e84414fdb <unknown>
#17 0x563e84424c05 <unknown>
#18 0x7f55376b6134 <unknown>
[2025-05-07T04:47:48.205+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:47:48.220+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.335 seconds
[2025-05-07T04:48:18.845+0000] {processor.py:186} INFO - Started process (PID=3401) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:48:18.847+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:48:18.850+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:48:18.850+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:48:24.628+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:48:24.809+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:48:24.809+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:48:25.729+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:48:25.728+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563225509a8e <unknown>
#1 0x563224fc6b0b <unknown>
#2 0x563224ffd4ea <unknown>
#3 0x563224ff8aef <unknown>
#4 0x56322504cb18 <unknown>
#5 0x56322503a9b3 <unknown>
#6 0x563225004c59 <unknown>
#7 0x563225005a08 <unknown>
#8 0x5632254d640a <unknown>
#9 0x5632254d985e <unknown>
#10 0x5632254d9308 <unknown>
#11 0x5632254d9ce5 <unknown>
#12 0x5632254bfb7b <unknown>
#13 0x5632254da050 <unknown>
#14 0x5632254a8ae9 <unknown>
#15 0x5632254f8df5 <unknown>
#16 0x5632254f8fdb <unknown>
#17 0x563225508c05 <unknown>
#18 0x7f876f3d1134 <unknown>
[2025-05-07T04:48:25.730+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:48:25.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.903 seconds
[2025-05-07T04:48:56.481+0000] {processor.py:186} INFO - Started process (PID=3552) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:48:56.482+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:48:56.483+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:48:56.483+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:49:03.388+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:49:03.586+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:49:03.585+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:49:04.521+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:49:04.520+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562d99678a8e <unknown>
#1 0x562d99135b0b <unknown>
#2 0x562d9916c4ea <unknown>
#3 0x562d99167aef <unknown>
#4 0x562d991bbb18 <unknown>
#5 0x562d991a99b3 <unknown>
#6 0x562d99173c59 <unknown>
#7 0x562d99174a08 <unknown>
#8 0x562d9964540a <unknown>
#9 0x562d9964885e <unknown>
#10 0x562d99648308 <unknown>
#11 0x562d99648ce5 <unknown>
#12 0x562d9962eb7b <unknown>
#13 0x562d99649050 <unknown>
#14 0x562d99617ae9 <unknown>
#15 0x562d99667df5 <unknown>
#16 0x562d99667fdb <unknown>
#17 0x562d99677c05 <unknown>
#18 0x7f923f1fe134 <unknown>
[2025-05-07T04:49:04.521+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:49:04.536+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.567 seconds
[2025-05-07T04:49:34.501+0000] {processor.py:186} INFO - Started process (PID=3700) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:49:34.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:49:34.504+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:49:34.504+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:49:40.583+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:49:40.879+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:49:40.878+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:49:41.845+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:49:41.844+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5588d523ea8e <unknown>
#1 0x5588d4cfbb0b <unknown>
#2 0x5588d4d324ea <unknown>
#3 0x5588d4d2daef <unknown>
#4 0x5588d4d81b18 <unknown>
#5 0x5588d4d6f9b3 <unknown>
#6 0x5588d4d39c59 <unknown>
#7 0x5588d4d3aa08 <unknown>
#8 0x5588d520b40a <unknown>
#9 0x5588d520e85e <unknown>
#10 0x5588d520e308 <unknown>
#11 0x5588d520ece5 <unknown>
#12 0x5588d51f4b7b <unknown>
#13 0x5588d520f050 <unknown>
#14 0x5588d51ddae9 <unknown>
#15 0x5588d522ddf5 <unknown>
#16 0x5588d522dfdb <unknown>
#17 0x5588d523dc05 <unknown>
#18 0x7ff609853134 <unknown>
[2025-05-07T04:49:41.846+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:49:41.860+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.865 seconds
[2025-05-07T04:50:12.629+0000] {processor.py:186} INFO - Started process (PID=3853) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:50:12.630+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:50:12.632+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:50:12.632+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:50:18.852+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:50:19.044+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:50:19.044+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:50:25.055+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:50:25.054+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563df1004a8e <unknown>
#1 0x563df0ac1b0b <unknown>
#2 0x563df0af84ea <unknown>
#3 0x563df0af3aef <unknown>
#4 0x563df0b47b18 <unknown>
#5 0x563df0b359b3 <unknown>
#6 0x563df0affc59 <unknown>
#7 0x563df0b00a08 <unknown>
#8 0x563df0fd140a <unknown>
#9 0x563df0fd485e <unknown>
#10 0x563df0fd4308 <unknown>
#11 0x563df0fd4ce5 <unknown>
#12 0x563df0fbab7b <unknown>
#13 0x563df0fd5050 <unknown>
#14 0x563df0fa3ae9 <unknown>
#15 0x563df0ff3df5 <unknown>
#16 0x563df0ff3fdb <unknown>
#17 0x563df1003c05 <unknown>
#18 0x7f740375c134 <unknown>
[2025-05-07T04:50:25.056+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:50:25.070+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.824 seconds
[2025-05-07T04:50:55.243+0000] {processor.py:186} INFO - Started process (PID=4006) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:50:55.260+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:50:55.262+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:50:55.262+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:51:02.605+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:51:02.916+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:51:02.916+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:51:03.357+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:51:03.356+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d8f9fa0a8e <unknown>
#1 0x55d8f9a5db0b <unknown>
#2 0x55d8f9a944ea <unknown>
#3 0x55d8f9a8faef <unknown>
#4 0x55d8f9ae3b18 <unknown>
#5 0x55d8f9ad19b3 <unknown>
#6 0x55d8f9a9bc59 <unknown>
#7 0x55d8f9a9ca08 <unknown>
#8 0x55d8f9f6d40a <unknown>
#9 0x55d8f9f7085e <unknown>
#10 0x55d8f9f70308 <unknown>
#11 0x55d8f9f70ce5 <unknown>
#12 0x55d8f9f56b7b <unknown>
#13 0x55d8f9f71050 <unknown>
#14 0x55d8f9f3fae9 <unknown>
#15 0x55d8f9f8fdf5 <unknown>
#16 0x55d8f9f8ffdb <unknown>
#17 0x55d8f9f9fc05 <unknown>
#18 0x7f4e83556134 <unknown>
[2025-05-07T04:51:03.357+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:51:03.378+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.140 seconds
[2025-05-07T04:51:33.743+0000] {processor.py:186} INFO - Started process (PID=4163) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:51:33.745+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:51:33.746+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:51:33.746+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:51:40.180+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:51:40.368+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:51:40.368+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:51:41.298+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:51:41.298+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e6e518ca8e <unknown>
#1 0x55e6e4c49b0b <unknown>
#2 0x55e6e4c804ea <unknown>
#3 0x55e6e4c7baef <unknown>
#4 0x55e6e4ccfb18 <unknown>
#5 0x55e6e4cbd9b3 <unknown>
#6 0x55e6e4c87c59 <unknown>
#7 0x55e6e4c88a08 <unknown>
#8 0x55e6e515940a <unknown>
#9 0x55e6e515c85e <unknown>
#10 0x55e6e515c308 <unknown>
#11 0x55e6e515cce5 <unknown>
#12 0x55e6e5142b7b <unknown>
#13 0x55e6e515d050 <unknown>
#14 0x55e6e512bae9 <unknown>
#15 0x55e6e517bdf5 <unknown>
#16 0x55e6e517bfdb <unknown>
#17 0x55e6e518bc05 <unknown>
#18 0x7fd1e6804134 <unknown>
[2025-05-07T04:51:41.299+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:51:41.313+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.654 seconds
[2025-05-07T04:52:11.883+0000] {processor.py:186} INFO - Started process (PID=4323) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:52:11.883+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:52:11.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:52:11.886+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:52:18.264+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:52:18.585+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:52:18.585+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:52:19.411+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:52:19.410+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5559a8cfea8e <unknown>
#1 0x5559a87bbb0b <unknown>
#2 0x5559a87f24ea <unknown>
#3 0x5559a87edaef <unknown>
#4 0x5559a8841b18 <unknown>
#5 0x5559a882f9b3 <unknown>
#6 0x5559a87f9c59 <unknown>
#7 0x5559a87faa08 <unknown>
#8 0x5559a8ccb40a <unknown>
#9 0x5559a8cce85e <unknown>
#10 0x5559a8cce308 <unknown>
#11 0x5559a8ccece5 <unknown>
#12 0x5559a8cb4b7b <unknown>
#13 0x5559a8ccf050 <unknown>
#14 0x5559a8c9dae9 <unknown>
#15 0x5559a8ceddf5 <unknown>
#16 0x5559a8cedfdb <unknown>
#17 0x5559a8cfdc05 <unknown>
#18 0x7f85de8cd134 <unknown>
[2025-05-07T04:52:19.412+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:52:19.427+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.060 seconds
[2025-05-07T04:53:22.784+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:53:22.785+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:53:22.787+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:53:22.787+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:53:35.291+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:53:55.400+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:53:55.400+0000] {timeout.py:68} ERROR - Process timed out, PID: 55
[2025-05-07T04:54:30.341+0000] {processor.py:186} INFO - Started process (PID=263) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:54:30.342+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:54:30.344+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:54:30.344+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:54:31.429+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:54:32.554+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:54:32.554+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:54:33.130+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:54:33.129+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559152e09a8e <unknown>
#1 0x5591528c6b0b <unknown>
#2 0x5591528fd4ea <unknown>
#3 0x5591528f8aef <unknown>
#4 0x55915294cb18 <unknown>
#5 0x55915293a9b3 <unknown>
#6 0x559152904c59 <unknown>
#7 0x559152905a08 <unknown>
#8 0x559152dd640a <unknown>
#9 0x559152dd985e <unknown>
#10 0x559152dd9308 <unknown>
#11 0x559152dd9ce5 <unknown>
#12 0x559152dbfb7b <unknown>
#13 0x559152dda050 <unknown>
#14 0x559152da8ae9 <unknown>
#15 0x559152df8df5 <unknown>
#16 0x559152df8fdb <unknown>
#17 0x559152e08c05 <unknown>
#18 0x7fdf69270134 <unknown>
[2025-05-07T04:54:33.130+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:54:33.143+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.920 seconds
[2025-05-07T04:55:03.316+0000] {processor.py:186} INFO - Started process (PID=425) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:55:03.318+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:55:03.321+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:55:03.320+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:55:12.786+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:55:13.137+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:55:13.137+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:55:13.713+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:55:13.712+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562625512a8e <unknown>
#1 0x562624fcfb0b <unknown>
#2 0x5626250064ea <unknown>
#3 0x562625001aef <unknown>
#4 0x562625055b18 <unknown>
#5 0x5626250439b3 <unknown>
#6 0x56262500dc59 <unknown>
#7 0x56262500ea08 <unknown>
#8 0x5626254df40a <unknown>
#9 0x5626254e285e <unknown>
#10 0x5626254e2308 <unknown>
#11 0x5626254e2ce5 <unknown>
#12 0x5626254c8b7b <unknown>
#13 0x5626254e3050 <unknown>
#14 0x5626254b1ae9 <unknown>
#15 0x562625501df5 <unknown>
#16 0x562625501fdb <unknown>
#17 0x562625511c05 <unknown>
#18 0x7f5fe12f5134 <unknown>
[2025-05-07T04:55:13.714+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:55:13.727+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 11.417 seconds
[2025-05-07T04:55:44.241+0000] {processor.py:186} INFO - Started process (PID=578) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:55:44.243+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:55:44.246+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:55:44.246+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:55:55.619+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:55:50.330+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:55:50.330+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:55:50.897+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:55:50.896+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56148f407a8e <unknown>
#1 0x56148eec4b0b <unknown>
#2 0x56148eefb4ea <unknown>
#3 0x56148eef6aef <unknown>
#4 0x56148ef4ab18 <unknown>
#5 0x56148ef389b3 <unknown>
#6 0x56148ef02c59 <unknown>
#7 0x56148ef03a08 <unknown>
#8 0x56148f3d440a <unknown>
#9 0x56148f3d785e <unknown>
#10 0x56148f3d7308 <unknown>
#11 0x56148f3d7ce5 <unknown>
#12 0x56148f3bdb7b <unknown>
#13 0x56148f3d8050 <unknown>
#14 0x56148f3a6ae9 <unknown>
#15 0x56148f3f6df5 <unknown>
#16 0x56148f3f6fdb <unknown>
#17 0x56148f406c05 <unknown>
#18 0x7fd157742134 <unknown>
[2025-05-07T04:55:50.898+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:55:50.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.669 seconds
[2025-05-07T04:56:21.692+0000] {processor.py:186} INFO - Started process (PID=721) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:56:21.693+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:56:21.695+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:56:21.695+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:56:28.126+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:56:28.440+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:56:28.440+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:56:29.047+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:56:29.046+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562c9cc73a8e <unknown>
#1 0x562c9c730b0b <unknown>
#2 0x562c9c7674ea <unknown>
#3 0x562c9c762aef <unknown>
#4 0x562c9c7b6b18 <unknown>
#5 0x562c9c7a49b3 <unknown>
#6 0x562c9c76ec59 <unknown>
#7 0x562c9c76fa08 <unknown>
#8 0x562c9cc4040a <unknown>
#9 0x562c9cc4385e <unknown>
#10 0x562c9cc43308 <unknown>
#11 0x562c9cc43ce5 <unknown>
#12 0x562c9cc29b7b <unknown>
#13 0x562c9cc44050 <unknown>
#14 0x562c9cc12ae9 <unknown>
#15 0x562c9cc62df5 <unknown>
#16 0x562c9cc62fdb <unknown>
#17 0x562c9cc72c05 <unknown>
#18 0x7f9a9fbdb134 <unknown>
[2025-05-07T04:56:29.047+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:56:29.064+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.879 seconds
[2025-05-07T04:56:59.436+0000] {processor.py:186} INFO - Started process (PID=859) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:56:59.440+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:56:59.441+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:56:59.441+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:56:59.698+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:57:00.027+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:57:00.027+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:57:00.473+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:57:00.472+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557ffc433a8e <unknown>
#1 0x557ffbef0b0b <unknown>
#2 0x557ffbf274ea <unknown>
#3 0x557ffbf22aef <unknown>
#4 0x557ffbf76b18 <unknown>
#5 0x557ffbf649b3 <unknown>
#6 0x557ffbf2ec59 <unknown>
#7 0x557ffbf2fa08 <unknown>
#8 0x557ffc40040a <unknown>
#9 0x557ffc40385e <unknown>
#10 0x557ffc403308 <unknown>
#11 0x557ffc403ce5 <unknown>
#12 0x557ffc3e9b7b <unknown>
#13 0x557ffc404050 <unknown>
#14 0x557ffc3d2ae9 <unknown>
#15 0x557ffc422df5 <unknown>
#16 0x557ffc422fdb <unknown>
#17 0x557ffc432c05 <unknown>
#18 0x7f93aa7b5134 <unknown>
[2025-05-07T04:57:00.474+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:57:00.491+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.174 seconds
[2025-05-07T04:57:35.767+0000] {processor.py:186} INFO - Started process (PID=998) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:57:35.771+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:57:35.772+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:57:35.772+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:57:36.751+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:57:37.097+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:57:37.097+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:57:37.704+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:57:37.703+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x555f9f7a4a8e <unknown>
#1 0x555f9f261b0b <unknown>
#2 0x555f9f2984ea <unknown>
#3 0x555f9f293aef <unknown>
#4 0x555f9f2e7b18 <unknown>
#5 0x555f9f2d59b3 <unknown>
#6 0x555f9f29fc59 <unknown>
#7 0x555f9f2a0a08 <unknown>
#8 0x555f9f77140a <unknown>
#9 0x555f9f77485e <unknown>
#10 0x555f9f774308 <unknown>
#11 0x555f9f774ce5 <unknown>
#12 0x555f9f75ab7b <unknown>
#13 0x555f9f775050 <unknown>
#14 0x555f9f743ae9 <unknown>
#15 0x555f9f793df5 <unknown>
#16 0x555f9f793fdb <unknown>
#17 0x555f9f7a3c05 <unknown>
#18 0x7f1b896cc134 <unknown>
[2025-05-07T04:57:37.704+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:57:37.724+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.072 seconds
[2025-05-07T04:58:08.501+0000] {processor.py:186} INFO - Started process (PID=1144) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:58:08.510+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:58:08.511+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:58:08.511+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:58:15.243+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:58:20.678+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:58:20.678+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:58:15.640+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:58:15.639+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5570c5e2ca8e <unknown>
#1 0x5570c58e9b0b <unknown>
#2 0x5570c59204ea <unknown>
#3 0x5570c591baef <unknown>
#4 0x5570c596fb18 <unknown>
#5 0x5570c595d9b3 <unknown>
#6 0x5570c5927c59 <unknown>
#7 0x5570c5928a08 <unknown>
#8 0x5570c5df940a <unknown>
#9 0x5570c5dfc85e <unknown>
#10 0x5570c5dfc308 <unknown>
#11 0x5570c5dfcce5 <unknown>
#12 0x5570c5de2b7b <unknown>
#13 0x5570c5dfd050 <unknown>
#14 0x5570c5dcbae9 <unknown>
#15 0x5570c5e1bdf5 <unknown>
#16 0x5570c5e1bfdb <unknown>
#17 0x5570c5e2bc05 <unknown>
#18 0x7fde2a20c134 <unknown>
[2025-05-07T04:58:15.640+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:58:15.656+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.164 seconds
[2025-05-07T04:58:50.909+0000] {processor.py:186} INFO - Started process (PID=1286) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:58:50.911+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:58:50.912+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:58:50.912+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:58:51.905+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:58:52.305+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:58:52.305+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:58:52.895+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:58:52.894+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5590c209aa8e <unknown>
#1 0x5590c1b57b0b <unknown>
#2 0x5590c1b8e4ea <unknown>
#3 0x5590c1b89aef <unknown>
#4 0x5590c1bddb18 <unknown>
#5 0x5590c1bcb9b3 <unknown>
#6 0x5590c1b95c59 <unknown>
#7 0x5590c1b96a08 <unknown>
#8 0x5590c206740a <unknown>
#9 0x5590c206a85e <unknown>
#10 0x5590c206a308 <unknown>
#11 0x5590c206ace5 <unknown>
#12 0x5590c2050b7b <unknown>
#13 0x5590c206b050 <unknown>
#14 0x5590c2039ae9 <unknown>
#15 0x5590c2089df5 <unknown>
#16 0x5590c2089fdb <unknown>
#17 0x5590c2099c05 <unknown>
#18 0x7f24bcf8a134 <unknown>
[2025-05-07T04:58:52.895+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:58:52.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.115 seconds
[2025-05-07T04:59:23.598+0000] {processor.py:186} INFO - Started process (PID=1445) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T04:59:23.602+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T04:59:23.604+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:59:23.603+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T04:59:30.379+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T04:59:30.614+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:59:30.613+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T04:59:30.687+0000] {logging_mixin.py:190} INFO - [2025-05-07T04:59:30.686+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5612d0f89a8e <unknown>
#1 0x5612d0a46b0b <unknown>
#2 0x5612d0a7d4ea <unknown>
#3 0x5612d0a78aef <unknown>
#4 0x5612d0accb18 <unknown>
#5 0x5612d0aba9b3 <unknown>
#6 0x5612d0a84c59 <unknown>
#7 0x5612d0a85a08 <unknown>
#8 0x5612d0f5640a <unknown>
#9 0x5612d0f5985e <unknown>
#10 0x5612d0f59308 <unknown>
#11 0x5612d0f59ce5 <unknown>
#12 0x5612d0f3fb7b <unknown>
#13 0x5612d0f5a050 <unknown>
#14 0x5612d0f28ae9 <unknown>
#15 0x5612d0f78df5 <unknown>
#16 0x5612d0f78fdb <unknown>
#17 0x5612d0f88c05 <unknown>
#18 0x7fb6a372b134 <unknown>
[2025-05-07T04:59:30.688+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T04:59:30.704+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.105 seconds
[2025-05-07T05:00:05.882+0000] {processor.py:186} INFO - Started process (PID=1584) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:00:05.883+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:00:05.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:00:05.886+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:00:07.005+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:00:07.369+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:00:07.369+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:00:07.934+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:00:07.933+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c85aba3a8e <unknown>
#1 0x55c85a660b0b <unknown>
#2 0x55c85a6974ea <unknown>
#3 0x55c85a692aef <unknown>
#4 0x55c85a6e6b18 <unknown>
#5 0x55c85a6d49b3 <unknown>
#6 0x55c85a69ec59 <unknown>
#7 0x55c85a69fa08 <unknown>
#8 0x55c85ab7040a <unknown>
#9 0x55c85ab7385e <unknown>
#10 0x55c85ab73308 <unknown>
#11 0x55c85ab73ce5 <unknown>
#12 0x55c85ab59b7b <unknown>
#13 0x55c85ab74050 <unknown>
#14 0x55c85ab42ae9 <unknown>
#15 0x55c85ab92df5 <unknown>
#16 0x55c85ab92fdb <unknown>
#17 0x55c85aba2c05 <unknown>
#18 0x7f65da08f134 <unknown>
[2025-05-07T05:00:07.934+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:00:07.950+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.187 seconds
[2025-05-07T05:00:41.102+0000] {processor.py:186} INFO - Started process (PID=1721) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:00:41.103+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:00:41.105+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:00:41.105+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:00:41.542+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:00:41.848+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:00:41.847+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:00:42.412+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:00:42.411+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e34997ba8e <unknown>
#1 0x55e349438b0b <unknown>
#2 0x55e34946f4ea <unknown>
#3 0x55e34946aaef <unknown>
#4 0x55e3494beb18 <unknown>
#5 0x55e3494ac9b3 <unknown>
#6 0x55e349476c59 <unknown>
#7 0x55e349477a08 <unknown>
#8 0x55e34994840a <unknown>
#9 0x55e34994b85e <unknown>
#10 0x55e34994b308 <unknown>
#11 0x55e34994bce5 <unknown>
#12 0x55e349931b7b <unknown>
#13 0x55e34994c050 <unknown>
#14 0x55e34991aae9 <unknown>
#15 0x55e34996adf5 <unknown>
#16 0x55e34996afdb <unknown>
#17 0x55e34997ac05 <unknown>
#18 0x7f8ab30a9134 <unknown>
[2025-05-07T05:00:42.412+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:00:42.426+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.446 seconds
[2025-05-07T05:01:13.074+0000] {processor.py:186} INFO - Started process (PID=1868) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:01:13.075+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:01:13.077+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:01:13.077+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:01:19.630+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:01:19.946+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:01:19.946+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:01:20.521+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:01:20.520+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56371d762a8e <unknown>
#1 0x56371d21fb0b <unknown>
#2 0x56371d2564ea <unknown>
#3 0x56371d251aef <unknown>
#4 0x56371d2a5b18 <unknown>
#5 0x56371d2939b3 <unknown>
#6 0x56371d25dc59 <unknown>
#7 0x56371d25ea08 <unknown>
#8 0x56371d72f40a <unknown>
#9 0x56371d73285e <unknown>
#10 0x56371d732308 <unknown>
#11 0x56371d732ce5 <unknown>
#12 0x56371d718b7b <unknown>
#13 0x56371d733050 <unknown>
#14 0x56371d701ae9 <unknown>
#15 0x56371d751df5 <unknown>
#16 0x56371d751fdb <unknown>
#17 0x56371d761c05 <unknown>
#18 0x7fab2c7e3134 <unknown>
[2025-05-07T05:01:20.522+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:01:20.552+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.978 seconds
[2025-05-07T05:01:50.652+0000] {processor.py:186} INFO - Started process (PID=2013) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:01:50.653+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:01:50.655+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:01:50.654+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:01:57.167+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:01:57.454+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:01:57.454+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:01:58.031+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:01:58.030+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557b97749a8e <unknown>
#1 0x557b97206b0b <unknown>
#2 0x557b9723d4ea <unknown>
#3 0x557b97238aef <unknown>
#4 0x557b9728cb18 <unknown>
#5 0x557b9727a9b3 <unknown>
#6 0x557b97244c59 <unknown>
#7 0x557b97245a08 <unknown>
#8 0x557b9771640a <unknown>
#9 0x557b9771985e <unknown>
#10 0x557b97719308 <unknown>
#11 0x557b97719ce5 <unknown>
#12 0x557b976ffb7b <unknown>
#13 0x557b9771a050 <unknown>
#14 0x557b976e8ae9 <unknown>
#15 0x557b97738df5 <unknown>
#16 0x557b97738fdb <unknown>
#17 0x557b97748c05 <unknown>
#18 0x7f61136a9134 <unknown>
[2025-05-07T05:01:58.032+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:01:58.045+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.893 seconds
[2025-05-07T05:02:28.951+0000] {processor.py:186} INFO - Started process (PID=2148) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:02:28.952+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:02:28.954+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:02:28.953+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:02:35.772+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:02:41.115+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:02:41.115+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:02:36.083+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:02:36.082+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5587e81a0a8e <unknown>
#1 0x5587e7c5db0b <unknown>
#2 0x5587e7c944ea <unknown>
#3 0x5587e7c8faef <unknown>
#4 0x5587e7ce3b18 <unknown>
#5 0x5587e7cd19b3 <unknown>
#6 0x5587e7c9bc59 <unknown>
#7 0x5587e7c9ca08 <unknown>
#8 0x5587e816d40a <unknown>
#9 0x5587e817085e <unknown>
#10 0x5587e8170308 <unknown>
#11 0x5587e8170ce5 <unknown>
#12 0x5587e8156b7b <unknown>
#13 0x5587e8171050 <unknown>
#14 0x5587e813fae9 <unknown>
#15 0x5587e818fdf5 <unknown>
#16 0x5587e818ffdb <unknown>
#17 0x5587e819fc05 <unknown>
#18 0x7f4d01278134 <unknown>
[2025-05-07T05:02:36.083+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:02:36.101+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.160 seconds
[2025-05-07T05:03:06.403+0000] {processor.py:186} INFO - Started process (PID=2295) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:03:06.405+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:03:06.406+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:03:06.406+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:03:13.071+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:03:13.365+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:03:13.365+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:03:13.942+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:03:13.941+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a429429a8e <unknown>
#1 0x55a428ee6b0b <unknown>
#2 0x55a428f1d4ea <unknown>
#3 0x55a428f18aef <unknown>
#4 0x55a428f6cb18 <unknown>
#5 0x55a428f5a9b3 <unknown>
#6 0x55a428f24c59 <unknown>
#7 0x55a428f25a08 <unknown>
#8 0x55a4293f640a <unknown>
#9 0x55a4293f985e <unknown>
#10 0x55a4293f9308 <unknown>
#11 0x55a4293f9ce5 <unknown>
#12 0x55a4293dfb7b <unknown>
#13 0x55a4293fa050 <unknown>
#14 0x55a4293c8ae9 <unknown>
#15 0x55a429418df5 <unknown>
#16 0x55a429418fdb <unknown>
#17 0x55a429428c05 <unknown>
#18 0x7fc6b6987134 <unknown>
[2025-05-07T05:03:13.943+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:03:13.957+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.053 seconds
[2025-05-07T05:03:44.650+0000] {processor.py:186} INFO - Started process (PID=2438) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:03:44.652+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:03:44.654+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:03:44.653+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:03:50.854+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:03:51.079+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:03:51.078+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:03:51.655+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:03:51.654+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x560e29c30a8e <unknown>
#1 0x560e296edb0b <unknown>
#2 0x560e297244ea <unknown>
#3 0x560e2971faef <unknown>
#4 0x560e29773b18 <unknown>
#5 0x560e297619b3 <unknown>
#6 0x560e2972bc59 <unknown>
#7 0x560e2972ca08 <unknown>
#8 0x560e29bfd40a <unknown>
#9 0x560e29c0085e <unknown>
#10 0x560e29c00308 <unknown>
#11 0x560e29c00ce5 <unknown>
#12 0x560e29be6b7b <unknown>
#13 0x560e29c01050 <unknown>
#14 0x560e29bcfae9 <unknown>
#15 0x560e29c1fdf5 <unknown>
#16 0x560e29c1ffdb <unknown>
#17 0x560e29c2fc05 <unknown>
#18 0x7f3257226134 <unknown>
[2025-05-07T05:03:51.655+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:03:51.668+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.021 seconds
[2025-05-07T05:04:26.342+0000] {processor.py:186} INFO - Started process (PID=2577) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:04:26.345+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:04:26.347+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:04:26.347+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:04:27.380+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:04:27.736+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:04:27.736+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:04:28.323+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:04:28.323+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563ffbccaa8e <unknown>
#1 0x563ffb787b0b <unknown>
#2 0x563ffb7be4ea <unknown>
#3 0x563ffb7b9aef <unknown>
#4 0x563ffb80db18 <unknown>
#5 0x563ffb7fb9b3 <unknown>
#6 0x563ffb7c5c59 <unknown>
#7 0x563ffb7c6a08 <unknown>
#8 0x563ffbc9740a <unknown>
#9 0x563ffbc9a85e <unknown>
#10 0x563ffbc9a308 <unknown>
#11 0x563ffbc9ace5 <unknown>
#12 0x563ffbc80b7b <unknown>
#13 0x563ffbc9b050 <unknown>
#14 0x563ffbc69ae9 <unknown>
#15 0x563ffbcb9df5 <unknown>
#16 0x563ffbcb9fdb <unknown>
#17 0x563ffbcc9c05 <unknown>
#18 0x7f9efa9e4134 <unknown>
[2025-05-07T05:04:28.324+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:04:28.338+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.117 seconds
[2025-05-07T05:04:58.365+0000] {processor.py:186} INFO - Started process (PID=2727) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:04:58.367+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:04:58.369+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:04:58.368+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:05:05.165+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:05:05.338+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:05:05.338+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:05:05.903+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:05:05.902+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5580bdd7ba8e <unknown>
#1 0x5580bd838b0b <unknown>
#2 0x5580bd86f4ea <unknown>
#3 0x5580bd86aaef <unknown>
#4 0x5580bd8beb18 <unknown>
#5 0x5580bd8ac9b3 <unknown>
#6 0x5580bd876c59 <unknown>
#7 0x5580bd877a08 <unknown>
#8 0x5580bdd4840a <unknown>
#9 0x5580bdd4b85e <unknown>
#10 0x5580bdd4b308 <unknown>
#11 0x5580bdd4bce5 <unknown>
#12 0x5580bdd31b7b <unknown>
#13 0x5580bdd4c050 <unknown>
#14 0x5580bdd1aae9 <unknown>
#15 0x5580bdd6adf5 <unknown>
#16 0x5580bdd6afdb <unknown>
#17 0x5580bdd7ac05 <unknown>
#18 0x7feb80010134 <unknown>
[2025-05-07T05:05:05.904+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:05:05.917+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.053 seconds
[2025-05-07T05:05:36.560+0000] {processor.py:186} INFO - Started process (PID=2870) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:05:36.562+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:05:36.563+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:05:36.563+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:05:37.543+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:05:37.857+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:05:37.857+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:05:38.428+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:05:38.427+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556a136e2a8e <unknown>
#1 0x556a1319fb0b <unknown>
#2 0x556a131d64ea <unknown>
#3 0x556a131d1aef <unknown>
#4 0x556a13225b18 <unknown>
#5 0x556a132139b3 <unknown>
#6 0x556a131ddc59 <unknown>
#7 0x556a131dea08 <unknown>
#8 0x556a136af40a <unknown>
#9 0x556a136b285e <unknown>
#10 0x556a136b2308 <unknown>
#11 0x556a136b2ce5 <unknown>
#12 0x556a13698b7b <unknown>
#13 0x556a136b3050 <unknown>
#14 0x556a13681ae9 <unknown>
#15 0x556a136d1df5 <unknown>
#16 0x556a136d1fdb <unknown>
#17 0x556a136e1c05 <unknown>
#18 0x7f99ce44d134 <unknown>
[2025-05-07T05:05:38.429+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:05:38.443+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.008 seconds
[2025-05-07T05:06:08.890+0000] {processor.py:186} INFO - Started process (PID=3008) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:06:08.891+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:06:08.893+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:06:08.893+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:06:15.153+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:06:15.348+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:06:15.348+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:06:15.917+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:06:15.916+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56064b25ca8e <unknown>
#1 0x56064ad19b0b <unknown>
#2 0x56064ad504ea <unknown>
#3 0x56064ad4baef <unknown>
#4 0x56064ad9fb18 <unknown>
#5 0x56064ad8d9b3 <unknown>
#6 0x56064ad57c59 <unknown>
#7 0x56064ad58a08 <unknown>
#8 0x56064b22940a <unknown>
#9 0x56064b22c85e <unknown>
#10 0x56064b22c308 <unknown>
#11 0x56064b22cce5 <unknown>
#12 0x56064b212b7b <unknown>
#13 0x56064b22d050 <unknown>
#14 0x56064b1fbae9 <unknown>
#15 0x56064b24bdf5 <unknown>
#16 0x56064b24bfdb <unknown>
#17 0x56064b25bc05 <unknown>
#18 0x7f67b167f134 <unknown>
[2025-05-07T05:06:15.917+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:06:15.931+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.545 seconds
[2025-05-07T05:06:46.643+0000] {processor.py:186} INFO - Started process (PID=3154) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:06:46.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:06:46.646+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:06:46.646+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:06:47.604+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:06:47.954+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:06:47.954+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:06:48.519+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:06:48.518+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b28ccdaa8e <unknown>
#1 0x55b28c797b0b <unknown>
#2 0x55b28c7ce4ea <unknown>
#3 0x55b28c7c9aef <unknown>
#4 0x55b28c81db18 <unknown>
#5 0x55b28c80b9b3 <unknown>
#6 0x55b28c7d5c59 <unknown>
#7 0x55b28c7d6a08 <unknown>
#8 0x55b28cca740a <unknown>
#9 0x55b28ccaa85e <unknown>
#10 0x55b28ccaa308 <unknown>
#11 0x55b28ccaace5 <unknown>
#12 0x55b28cc90b7b <unknown>
#13 0x55b28ccab050 <unknown>
#14 0x55b28cc79ae9 <unknown>
#15 0x55b28ccc9df5 <unknown>
#16 0x55b28ccc9fdb <unknown>
#17 0x55b28ccd9c05 <unknown>
#18 0x7f9853668134 <unknown>
[2025-05-07T05:06:48.520+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:06:48.536+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.019 seconds
[2025-05-07T05:07:19.415+0000] {processor.py:186} INFO - Started process (PID=3299) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:07:19.417+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:07:19.419+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:07:19.419+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:07:26.146+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:07:26.368+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:07:26.368+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:07:26.450+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:07:26.449+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5582d0395a8e <unknown>
#1 0x5582cfe52b0b <unknown>
#2 0x5582cfe894ea <unknown>
#3 0x5582cfe84aef <unknown>
#4 0x5582cfed8b18 <unknown>
#5 0x5582cfec69b3 <unknown>
#6 0x5582cfe90c59 <unknown>
#7 0x5582cfe91a08 <unknown>
#8 0x5582d036240a <unknown>
#9 0x5582d036585e <unknown>
#10 0x5582d0365308 <unknown>
#11 0x5582d0365ce5 <unknown>
#12 0x5582d034bb7b <unknown>
#13 0x5582d0366050 <unknown>
#14 0x5582d0334ae9 <unknown>
#15 0x5582d0384df5 <unknown>
#16 0x5582d0384fdb <unknown>
#17 0x5582d0394c05 <unknown>
#18 0x7fdfed824134 <unknown>
[2025-05-07T05:07:26.450+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:07:26.470+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.067 seconds
[2025-05-07T05:07:56.636+0000] {processor.py:186} INFO - Started process (PID=3439) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:07:56.637+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:07:56.639+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:07:56.638+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:07:57.554+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:07:57.867+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:07:57.867+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:07:58.456+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:07:58.455+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5557d185ea8e <unknown>
#1 0x5557d131bb0b <unknown>
#2 0x5557d13524ea <unknown>
#3 0x5557d134daef <unknown>
#4 0x5557d13a1b18 <unknown>
#5 0x5557d138f9b3 <unknown>
#6 0x5557d1359c59 <unknown>
#7 0x5557d135aa08 <unknown>
#8 0x5557d182b40a <unknown>
#9 0x5557d182e85e <unknown>
#10 0x5557d182e308 <unknown>
#11 0x5557d182ece5 <unknown>
#12 0x5557d1814b7b <unknown>
#13 0x5557d182f050 <unknown>
#14 0x5557d17fdae9 <unknown>
#15 0x5557d184ddf5 <unknown>
#16 0x5557d184dfdb <unknown>
#17 0x5557d185dc05 <unknown>
#18 0x7efe84353134 <unknown>
[2025-05-07T05:07:58.456+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:07:58.476+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.960 seconds
[2025-05-07T05:08:31.714+0000] {processor.py:186} INFO - Started process (PID=3576) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:08:31.716+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:08:31.717+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:08:31.717+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:08:32.835+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:08:33.059+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:08:33.059+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:08:33.628+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:08:33.628+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ef8c079a8e <unknown>
#1 0x55ef8bb36b0b <unknown>
#2 0x55ef8bb6d4ea <unknown>
#3 0x55ef8bb68aef <unknown>
#4 0x55ef8bbbcb18 <unknown>
#5 0x55ef8bbaa9b3 <unknown>
#6 0x55ef8bb74c59 <unknown>
#7 0x55ef8bb75a08 <unknown>
#8 0x55ef8c04640a <unknown>
#9 0x55ef8c04985e <unknown>
#10 0x55ef8c049308 <unknown>
#11 0x55ef8c049ce5 <unknown>
#12 0x55ef8c02fb7b <unknown>
#13 0x55ef8c04a050 <unknown>
#14 0x55ef8c018ae9 <unknown>
#15 0x55ef8c068df5 <unknown>
#16 0x55ef8c068fdb <unknown>
#17 0x55ef8c078c05 <unknown>
#18 0x7f34e4fbc134 <unknown>
[2025-05-07T05:08:33.629+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:08:33.645+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.047 seconds
[2025-05-07T05:09:03.686+0000] {processor.py:186} INFO - Started process (PID=3728) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:09:03.687+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:09:03.688+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:09:03.688+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:09:10.799+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:09:11.111+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:09:11.111+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:09:16.795+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:09:16.794+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55facc1bba8e <unknown>
#1 0x55facbc78b0b <unknown>
#2 0x55facbcaf4ea <unknown>
#3 0x55facbcaaaef <unknown>
#4 0x55facbcfeb18 <unknown>
#5 0x55facbcec9b3 <unknown>
#6 0x55facbcb6c59 <unknown>
#7 0x55facbcb7a08 <unknown>
#8 0x55facc18840a <unknown>
#9 0x55facc18b85e <unknown>
#10 0x55facc18b308 <unknown>
#11 0x55facc18bce5 <unknown>
#12 0x55facc171b7b <unknown>
#13 0x55facc18c050 <unknown>
#14 0x55facc15aae9 <unknown>
#15 0x55facc1aadf5 <unknown>
#16 0x55facc1aafdb <unknown>
#17 0x55facc1bac05 <unknown>
#18 0x7f92a71b1134 <unknown>
[2025-05-07T05:09:16.796+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:09:16.811+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.507 seconds
[2025-05-07T05:09:47.055+0000] {processor.py:186} INFO - Started process (PID=3867) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:09:41.431+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:09:41.433+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:09:41.433+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:09:47.628+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:09:47.979+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:09:47.978+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:09:48.575+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:09:48.574+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561f51c3ca8e <unknown>
#1 0x561f516f9b0b <unknown>
#2 0x561f517304ea <unknown>
#3 0x561f5172baef <unknown>
#4 0x561f5177fb18 <unknown>
#5 0x561f5176d9b3 <unknown>
#6 0x561f51737c59 <unknown>
#7 0x561f51738a08 <unknown>
#8 0x561f51c0940a <unknown>
#9 0x561f51c0c85e <unknown>
#10 0x561f51c0c308 <unknown>
#11 0x561f51c0cce5 <unknown>
#12 0x561f51bf2b7b <unknown>
#13 0x561f51c0d050 <unknown>
#14 0x561f51bdbae9 <unknown>
#15 0x561f51c2bdf5 <unknown>
#16 0x561f51c2bfdb <unknown>
#17 0x561f51c3bc05 <unknown>
#18 0x7f3dbf974134 <unknown>
[2025-05-07T05:09:48.576+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:09:48.592+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.658 seconds
[2025-05-07T05:10:18.717+0000] {processor.py:186} INFO - Started process (PID=4013) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:10:18.718+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:10:18.720+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:10:18.720+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:10:25.780+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:10:26.129+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:10:26.129+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:10:26.705+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:10:26.704+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557f5fd82a8e <unknown>
#1 0x557f5f83fb0b <unknown>
#2 0x557f5f8764ea <unknown>
#3 0x557f5f871aef <unknown>
#4 0x557f5f8c5b18 <unknown>
#5 0x557f5f8b39b3 <unknown>
#6 0x557f5f87dc59 <unknown>
#7 0x557f5f87ea08 <unknown>
#8 0x557f5fd4f40a <unknown>
#9 0x557f5fd5285e <unknown>
#10 0x557f5fd52308 <unknown>
#11 0x557f5fd52ce5 <unknown>
#12 0x557f5fd38b7b <unknown>
#13 0x557f5fd53050 <unknown>
#14 0x557f5fd21ae9 <unknown>
#15 0x557f5fd71df5 <unknown>
#16 0x557f5fd71fdb <unknown>
#17 0x557f5fd81c05 <unknown>
#18 0x7fc25b083134 <unknown>
[2025-05-07T05:10:26.706+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:10:26.722+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.516 seconds
[2025-05-07T05:10:57.132+0000] {processor.py:186} INFO - Started process (PID=4151) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:10:57.133+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:10:57.135+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:10:57.135+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:10:57.992+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:10:58.323+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:10:58.323+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:10:58.909+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:10:58.908+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e8e9102a8e <unknown>
#1 0x55e8e8bbfb0b <unknown>
#2 0x55e8e8bf64ea <unknown>
#3 0x55e8e8bf1aef <unknown>
#4 0x55e8e8c45b18 <unknown>
#5 0x55e8e8c339b3 <unknown>
#6 0x55e8e8bfdc59 <unknown>
#7 0x55e8e8bfea08 <unknown>
#8 0x55e8e90cf40a <unknown>
#9 0x55e8e90d285e <unknown>
#10 0x55e8e90d2308 <unknown>
#11 0x55e8e90d2ce5 <unknown>
#12 0x55e8e90b8b7b <unknown>
#13 0x55e8e90d3050 <unknown>
#14 0x55e8e90a1ae9 <unknown>
#15 0x55e8e90f1df5 <unknown>
#16 0x55e8e90f1fdb <unknown>
#17 0x55e8e9101c05 <unknown>
#18 0x7f50964bf134 <unknown>
[2025-05-07T05:10:58.910+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:10:58.929+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.921 seconds
[2025-05-07T05:11:32.123+0000] {processor.py:186} INFO - Started process (PID=4298) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T05:11:32.140+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T05:11:32.142+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:11:32.142+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T05:11:33.151+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T05:11:33.332+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:11:33.331+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T05:11:33.906+0000] {logging_mixin.py:190} INFO - [2025-05-07T05:11:33.906+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c0e0826a8e <unknown>
#1 0x55c0e02e3b0b <unknown>
#2 0x55c0e031a4ea <unknown>
#3 0x55c0e0315aef <unknown>
#4 0x55c0e0369b18 <unknown>
#5 0x55c0e03579b3 <unknown>
#6 0x55c0e0321c59 <unknown>
#7 0x55c0e0322a08 <unknown>
#8 0x55c0e07f340a <unknown>
#9 0x55c0e07f685e <unknown>
#10 0x55c0e07f6308 <unknown>
#11 0x55c0e07f6ce5 <unknown>
#12 0x55c0e07dcb7b <unknown>
#13 0x55c0e07f7050 <unknown>
#14 0x55c0e07c5ae9 <unknown>
#15 0x55c0e0815df5 <unknown>
#16 0x55c0e0815fdb <unknown>
#17 0x55c0e0825c05 <unknown>
#18 0x7fc4fee40134 <unknown>
[2025-05-07T05:11:33.907+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T05:11:33.921+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.925 seconds
[2025-05-07T06:09:56.652+0000] {processor.py:186} INFO - Started process (PID=4440) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:09:56.653+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:09:56.656+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:09:56.656+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:10:08.690+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:10:13.293+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:10:13.292+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T06:10:13.862+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:10:13.861+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e8bca4ea8e <unknown>
#1 0x55e8bc50bb0b <unknown>
#2 0x55e8bc5424ea <unknown>
#3 0x55e8bc53daef <unknown>
#4 0x55e8bc591b18 <unknown>
#5 0x55e8bc57f9b3 <unknown>
#6 0x55e8bc549c59 <unknown>
#7 0x55e8bc54aa08 <unknown>
#8 0x55e8bca1b40a <unknown>
#9 0x55e8bca1e85e <unknown>
#10 0x55e8bca1e308 <unknown>
#11 0x55e8bca1ece5 <unknown>
#12 0x55e8bca04b7b <unknown>
#13 0x55e8bca1f050 <unknown>
#14 0x55e8bc9edae9 <unknown>
#15 0x55e8bca3ddf5 <unknown>
#16 0x55e8bca3dfdb <unknown>
#17 0x55e8bca4dc05 <unknown>
#18 0x7f82835b1134 <unknown>
[2025-05-07T06:10:13.863+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:10:13.883+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 17.237 seconds
[2025-05-07T06:10:44.498+0000] {processor.py:186} INFO - Started process (PID=4645) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:10:44.510+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:10:44.512+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:10:44.512+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:10:51.124+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:10:51.517+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:10:51.517+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T06:10:52.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:10:52.092+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55973419da8e <unknown>
#1 0x559733c5ab0b <unknown>
#2 0x559733c914ea <unknown>
#3 0x559733c8caef <unknown>
#4 0x559733ce0b18 <unknown>
#5 0x559733cce9b3 <unknown>
#6 0x559733c98c59 <unknown>
#7 0x559733c99a08 <unknown>
#8 0x55973416a40a <unknown>
#9 0x55973416d85e <unknown>
#10 0x55973416d308 <unknown>
#11 0x55973416dce5 <unknown>
#12 0x559734153b7b <unknown>
#13 0x55973416e050 <unknown>
#14 0x55973413cae9 <unknown>
#15 0x55973418cdf5 <unknown>
#16 0x55973418cfdb <unknown>
#17 0x55973419cc05 <unknown>
#18 0x7f3ccf217134 <unknown>
[2025-05-07T06:10:52.094+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:10:52.125+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.684 seconds
[2025-05-07T06:11:26.654+0000] {processor.py:186} INFO - Started process (PID=4783) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:11:26.656+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:11:26.658+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:11:26.658+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:11:27.598+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:11:27.921+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:11:27.921+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T06:11:28.495+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:11:28.494+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c4d0147a8e <unknown>
#1 0x55c4cfc04b0b <unknown>
#2 0x55c4cfc3b4ea <unknown>
#3 0x55c4cfc36aef <unknown>
#4 0x55c4cfc8ab18 <unknown>
#5 0x55c4cfc789b3 <unknown>
#6 0x55c4cfc42c59 <unknown>
#7 0x55c4cfc43a08 <unknown>
#8 0x55c4d011440a <unknown>
#9 0x55c4d011785e <unknown>
#10 0x55c4d0117308 <unknown>
#11 0x55c4d0117ce5 <unknown>
#12 0x55c4d00fdb7b <unknown>
#13 0x55c4d0118050 <unknown>
#14 0x55c4d00e6ae9 <unknown>
#15 0x55c4d0136df5 <unknown>
#16 0x55c4d0136fdb <unknown>
#17 0x55c4d0146c05 <unknown>
#18 0x7fd63a70e134 <unknown>
[2025-05-07T06:11:28.496+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:11:28.522+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.024 seconds
[2025-05-07T06:11:58.563+0000] {processor.py:186} INFO - Started process (PID=4938) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:11:58.564+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:11:58.566+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:11:58.566+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:12:05.188+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:12:05.535+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:12:05.535+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T06:12:06.134+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:12:06.133+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55bc79bc4a8e <unknown>
#1 0x55bc79681b0b <unknown>
#2 0x55bc796b84ea <unknown>
#3 0x55bc796b3aef <unknown>
#4 0x55bc79707b18 <unknown>
#5 0x55bc796f59b3 <unknown>
#6 0x55bc796bfc59 <unknown>
#7 0x55bc796c0a08 <unknown>
#8 0x55bc79b9140a <unknown>
#9 0x55bc79b9485e <unknown>
#10 0x55bc79b94308 <unknown>
#11 0x55bc79b94ce5 <unknown>
#12 0x55bc79b7ab7b <unknown>
#13 0x55bc79b95050 <unknown>
#14 0x55bc79b63ae9 <unknown>
#15 0x55bc79bb3df5 <unknown>
#16 0x55bc79bb3fdb <unknown>
#17 0x55bc79bc3c05 <unknown>
#18 0x7f8668189134 <unknown>
[2025-05-07T06:12:06.135+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:12:06.150+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.013 seconds
[2025-05-07T06:12:41.762+0000] {processor.py:186} INFO - Started process (PID=5085) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:12:41.763+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:12:41.765+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:12:41.765+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:12:42.134+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:12:42.528+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:12:42.528+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T06:12:43.095+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:12:43.094+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556cb5959a8e <unknown>
#1 0x556cb5416b0b <unknown>
#2 0x556cb544d4ea <unknown>
#3 0x556cb5448aef <unknown>
#4 0x556cb549cb18 <unknown>
#5 0x556cb548a9b3 <unknown>
#6 0x556cb5454c59 <unknown>
#7 0x556cb5455a08 <unknown>
#8 0x556cb592640a <unknown>
#9 0x556cb592985e <unknown>
#10 0x556cb5929308 <unknown>
#11 0x556cb5929ce5 <unknown>
#12 0x556cb590fb7b <unknown>
#13 0x556cb592a050 <unknown>
#14 0x556cb58f8ae9 <unknown>
#15 0x556cb5948df5 <unknown>
#16 0x556cb5948fdb <unknown>
#17 0x556cb5958c05 <unknown>
#18 0x7f5fb9d20134 <unknown>
[2025-05-07T06:12:43.096+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:12:43.114+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.508 seconds
[2025-05-07T06:13:13.897+0000] {processor.py:186} INFO - Started process (PID=5222) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:13:13.899+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:13:13.900+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:13:13.900+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:13:20.680+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:13:20.885+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:13:20.885+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T06:13:26.808+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:13:26.807+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56151175da8e <unknown>
#1 0x56151121ab0b <unknown>
#2 0x5615112514ea <unknown>
#3 0x56151124caef <unknown>
#4 0x5615112a0b18 <unknown>
#5 0x56151128e9b3 <unknown>
#6 0x561511258c59 <unknown>
#7 0x561511259a08 <unknown>
#8 0x56151172a40a <unknown>
#9 0x56151172d85e <unknown>
#10 0x56151172d308 <unknown>
#11 0x56151172dce5 <unknown>
#12 0x561511713b7b <unknown>
#13 0x56151172e050 <unknown>
#14 0x5615116fcae9 <unknown>
#15 0x56151174cdf5 <unknown>
#16 0x56151174cfdb <unknown>
#17 0x56151175cc05 <unknown>
#18 0x7fefbe6bc134 <unknown>
[2025-05-07T06:13:26.808+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:13:26.823+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.056 seconds
[2025-05-07T06:13:28.868+0000] {processor.py:186} INFO - Started process (PID=5355) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:13:28.869+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:13:28.870+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:13:28.870+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:13:35.987+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:13:36.210+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:13:36.284+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:13:36.284+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:13:36.291+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:13:36.291+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:13:36.293+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:13:36.293+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:13:36.312+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.875 seconds
[2025-05-07T06:14:11.898+0000] {processor.py:186} INFO - Started process (PID=5480) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:14:11.899+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:14:11.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:11.901+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:14:12.415+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:14:12.753+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:14:12.776+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:12.776+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:14:12.789+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:12.788+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:14:12.793+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:12.792+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:14:12.809+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.127 seconds
[2025-05-07T06:14:43.018+0000] {processor.py:186} INFO - Started process (PID=5603) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:14:43.027+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:14:43.037+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:43.036+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:14:51.961+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:14:53.466+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:14:53.564+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:53.564+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:14:53.625+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:53.625+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:14:53.630+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:14:53.630+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:14:53.668+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 11.687 seconds
[2025-05-07T06:15:24.595+0000] {processor.py:186} INFO - Started process (PID=5733) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:15:24.606+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:15:24.609+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:15:24.609+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:15:30.923+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:15:31.233+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:15:31.255+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:15:31.254+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:15:31.264+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:15:31.264+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:15:31.266+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:15:31.266+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:15:31.278+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.197 seconds
[2025-05-07T06:16:07.138+0000] {processor.py:186} INFO - Started process (PID=5856) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:16:07.140+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:16:07.145+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:07.144+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:16:08.274+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:16:08.589+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:16:08.610+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:08.609+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:16:08.620+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:08.619+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:16:08.622+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:08.622+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:16:08.633+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.739 seconds
[2025-05-07T06:16:39.007+0000] {processor.py:186} INFO - Started process (PID=5986) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:16:39.023+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:16:39.024+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:39.024+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:16:45.549+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:16:45.887+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:16:45.907+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:45.907+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:16:45.917+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:45.917+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:16:45.919+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:16:45.919+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:16:45.944+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.444 seconds
[2025-05-07T06:17:17.262+0000] {processor.py:186} INFO - Started process (PID=6111) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:17:17.263+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:17:17.264+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:17.264+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:17:18.257+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:17:18.446+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:17:18.472+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:18.471+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:17:18.487+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:18.486+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:17:18.489+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:18.489+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:17:18.501+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.479 seconds
[2025-05-07T06:17:49.223+0000] {processor.py:186} INFO - Started process (PID=6245) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:17:49.237+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:17:49.239+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:49.238+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:17:55.314+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:17:55.622+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:17:55.643+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:55.643+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:17:55.653+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:55.653+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:17:55.655+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:17:55.655+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:17:55.667+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.950 seconds
[2025-05-07T06:18:26.530+0000] {processor.py:186} INFO - Started process (PID=6371) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:18:26.543+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:18:26.545+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:18:26.544+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:18:32.333+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:18:32.663+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:18:32.701+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:18:32.700+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:18:32.714+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:18:32.714+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:18:32.720+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:18:32.719+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:18:32.734+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.218 seconds
[2025-05-07T06:19:07.218+0000] {processor.py:186} INFO - Started process (PID=6488) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:07.220+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:19:07.222+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:07.221+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:07.643+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:19:07.972+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:07.991+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:07.991+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:19:08.001+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:08.000+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:19:08.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:08.003+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:19:08.013+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.036 seconds
[2025-05-07T06:19:11.103+0000] {processor.py:186} INFO - Started process (PID=6598) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:11.104+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:19:11.106+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:11.106+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:17.453+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:19:17.648+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:17.671+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:17.670+0000] {dag.py:3211} INFO - Sync 2 DAGs
[2025-05-07T06:19:17.681+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:17.681+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:19:17.683+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:17.683+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:19:17.696+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.604 seconds
[2025-05-07T06:19:41.888+0000] {processor.py:186} INFO - Started process (PID=6720) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:41.889+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:19:41.891+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:41.891+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:47.824+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:19:48.048+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:19:48.042+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = x_videos_scraper(id="elonmusk",
NameError: name 'x_videos_scraper' is not defined. Did you mean: 'ins_videos_scraper'?
[2025-05-07T06:19:48.049+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:19:48.076+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.190 seconds
[2025-05-07T06:20:12.222+0000] {processor.py:186} INFO - Started process (PID=6853) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:12.223+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:20:12.226+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:12.225+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:18.869+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:20:19.147+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:19.146+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 37, in <module>
    scrape_task = x_videos_scraper(id="elonmusk",
NameError: name 'x_videos_scraper' is not defined. Did you mean: 'ins_videos_scraper'?
[2025-05-07T06:20:19.149+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:19.166+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.453 seconds
[2025-05-07T06:20:26.383+0000] {processor.py:186} INFO - Started process (PID=6961) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:26.384+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:20:26.385+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:26.385+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:32.475+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:20:32.790+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:32.877+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.877+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:x_videos_scraper_dag
[2025-05-07T06:20:32.897+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.897+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:x_videos_scraper_dag
[2025-05-07T06:20:32.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.901+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:x_videos_scraper_dag
[2025-05-07T06:20:32.907+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.907+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:20:32.917+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.917+0000] {dag.py:3234} INFO - Creating ORM DAG for x_videos_scraper_dag
[2025-05-07T06:20:32.917+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.917+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:20:32.920+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.920+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:20:32.921+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:32.920+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:20:32.933+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.559 seconds
[2025-05-07T06:20:44.048+0000] {processor.py:186} INFO - Started process (PID=7075) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:44.049+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:20:44.050+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:44.049+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:50.802+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:20:51.226+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:20:51.238+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:51.237+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:20:51.248+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:51.248+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:20:51.251+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:51.251+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:20:51.252+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:20:51.252+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:20:51.266+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.716 seconds
[2025-05-07T06:21:22.616+0000] {processor.py:186} INFO - Started process (PID=7199) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:21:22.617+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:21:22.618+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:21:22.618+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:21:24.443+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:21:24.652+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:21:24.689+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:21:24.689+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:21:24.705+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:21:24.704+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:21:24.707+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:21:24.707+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:21:24.708+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:21:24.708+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:21:24.725+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.352 seconds
[2025-05-07T06:21:55.396+0000] {processor.py:186} INFO - Started process (PID=7327) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:21:55.413+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:21:55.414+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:21:55.414+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:22:02.296+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:22:02.544+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:22:02.566+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:02.565+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:22:02.575+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:02.575+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:22:02.577+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:02.577+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:22:02.578+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:02.577+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:22:02.589+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.196 seconds
[2025-05-07T06:22:33.473+0000] {processor.py:186} INFO - Started process (PID=7453) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:22:33.484+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:22:33.485+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:33.485+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:22:40.050+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:22:40.356+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:22:40.377+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:40.377+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:22:40.386+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:40.386+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:22:40.389+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:40.389+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:22:40.390+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:22:40.390+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:22:40.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.445 seconds
[2025-05-07T06:23:10.778+0000] {processor.py:186} INFO - Started process (PID=7571) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:23:10.792+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:23:10.793+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:10.793+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:23:16.887+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:23:17.173+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:23:17.205+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:17.204+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:23:17.216+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:17.216+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:23:17.218+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:17.218+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:23:17.219+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:17.219+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:23:17.230+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.962 seconds
[2025-05-07T06:23:47.828+0000] {processor.py:186} INFO - Started process (PID=7690) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:23:47.829+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:23:47.830+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:47.829+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:23:48.636+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:23:48.941+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:23:48.965+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:48.965+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:23:48.975+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:48.975+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:23:48.977+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:48.977+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:23:48.978+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:23:48.978+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:23:48.989+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.411 seconds
[2025-05-07T06:24:22.775+0000] {processor.py:186} INFO - Started process (PID=7822) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:24:22.777+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:24:22.778+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:22.777+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:24:23.696+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:24:24.012+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:24:24.036+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:24.035+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:24:24.045+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:24.045+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:24:24.047+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:24.047+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:24:24.048+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:24.048+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:24:24.058+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.525 seconds
[2025-05-07T06:24:57.813+0000] {processor.py:186} INFO - Started process (PID=7942) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:24:57.817+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:24:57.819+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:57.818+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:24:58.113+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:24:58.435+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:24:58.459+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:58.458+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:24:58.470+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:58.469+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:24:58.472+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:58.472+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:24:58.473+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:24:58.473+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:24:58.484+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.916 seconds
[2025-05-07T06:25:28.580+0000] {processor.py:186} INFO - Started process (PID=8059) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:25:28.581+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:25:28.582+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:25:28.582+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:25:35.048+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:25:35.361+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:25:35.390+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:25:35.389+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:25:35.401+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:25:35.400+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:25:35.403+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:25:35.403+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:25:35.404+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:25:35.404+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:25:35.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.352 seconds
[2025-05-07T06:26:07.984+0000] {processor.py:186} INFO - Started process (PID=8183) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:26:07.985+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:26:07.986+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:07.985+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:26:08.909+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:26:09.285+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:26:09.307+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:09.307+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:26:09.317+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:09.317+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:26:09.320+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:09.320+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:26:09.321+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:09.320+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:26:09.332+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.595 seconds
[2025-05-07T06:26:43.022+0000] {processor.py:186} INFO - Started process (PID=8310) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:26:43.023+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:26:43.024+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:43.024+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:26:43.355+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:26:43.732+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:26:43.755+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:43.755+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:26:43.764+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:43.764+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:26:43.767+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:43.767+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:26:43.767+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:26:43.767+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:26:43.779+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.000 seconds
[2025-05-07T06:27:18.156+0000] {processor.py:186} INFO - Started process (PID=8429) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:27:18.157+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:27:18.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:27:18.158+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:27:27.215+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:27:27.418+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:27:27.443+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:27:27.442+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:27:27.454+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:27:27.454+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:27:27.457+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:27:27.457+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:27:27.458+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:27:27.458+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:27:27.468+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 16.055 seconds
[2025-05-07T06:28:03.236+0000] {processor.py:186} INFO - Started process (PID=8565) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:28:03.237+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:28:03.238+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:03.238+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:28:03.494+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:28:03.808+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:28:03.829+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:03.829+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:28:03.839+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:03.839+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:28:03.841+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:03.841+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:28:03.842+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:03.842+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:28:03.854+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.874 seconds
[2025-05-07T06:28:34.702+0000] {processor.py:186} INFO - Started process (PID=8682) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:28:34.705+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:28:34.706+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:34.706+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:28:40.904+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:28:41.260+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:28:41.281+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:41.281+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:28:41.290+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:41.290+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:28:41.292+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:41.292+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:28:41.293+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:28:41.293+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:28:41.307+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.109 seconds
[2025-05-07T06:29:11.838+0000] {processor.py:186} INFO - Started process (PID=8806) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:29:11.841+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:29:11.842+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:11.842+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:29:18.043+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:29:18.316+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:29:18.339+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:18.339+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:29:18.350+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:18.350+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:29:18.352+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:18.352+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:29:18.353+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:18.353+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:29:18.364+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.526 seconds
[2025-05-07T06:29:48.898+0000] {processor.py:186} INFO - Started process (PID=8932) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:29:48.900+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:29:48.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:48.901+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:29:55.783+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:29:56.126+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:29:56.159+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:56.159+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:29:56.171+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:56.171+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:29:56.174+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:56.174+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:29:56.175+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:29:56.175+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:29:56.190+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.796 seconds
[2025-05-07T06:30:28.478+0000] {processor.py:186} INFO - Started process (PID=9052) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:30:28.488+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:30:28.489+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:30:28.488+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:30:30.891+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:30:31.122+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:30:31.144+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:30:31.144+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:30:31.170+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:30:31.170+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:30:31.173+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:30:31.173+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:30:31.174+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:30:31.173+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:30:31.185+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.961 seconds
[2025-05-07T06:31:03.567+0000] {processor.py:186} INFO - Started process (PID=9182) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:31:03.567+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:31:03.568+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:03.568+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:31:04.533+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:31:04.865+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:31:04.887+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:04.887+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:31:04.899+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:04.899+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:31:04.902+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:04.902+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:31:04.903+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:04.903+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:31:04.924+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.605 seconds
[2025-05-07T06:31:38.468+0000] {processor.py:186} INFO - Started process (PID=9311) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:31:38.468+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:31:38.472+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:38.472+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:31:38.623+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:31:38.960+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:31:38.990+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:38.989+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:31:39.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:39.003+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:31:39.006+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:39.006+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:31:39.007+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:31:39.007+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:31:39.020+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.800 seconds
[2025-05-07T06:32:13.569+0000] {processor.py:186} INFO - Started process (PID=9434) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:32:13.581+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:32:13.582+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:13.582+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:32:14.585+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:32:14.906+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:32:14.939+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:14.939+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:32:14.948+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:14.948+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:32:14.950+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:14.950+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:32:14.951+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:14.951+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:32:14.962+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.635 seconds
[2025-05-07T06:32:45.110+0000] {processor.py:186} INFO - Started process (PID=9552) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:32:45.110+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:32:45.111+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:45.111+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:32:51.186+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:32:51.508+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:32:51.532+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:51.532+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:32:51.542+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:51.542+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:32:51.544+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:51.544+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:32:51.545+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:32:51.545+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:32:51.557+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.945 seconds
[2025-05-07T06:33:21.756+0000] {processor.py:186} INFO - Started process (PID=9684) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:33:21.757+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:33:21.759+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:21.758+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:33:28.290+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:33:33.832+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:33:28.115+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:28.115+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:33:28.126+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:28.126+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:33:28.129+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:28.129+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:33:28.130+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:28.130+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:33:28.141+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.393 seconds
[2025-05-07T06:33:58.729+0000] {processor.py:186} INFO - Started process (PID=9804) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:33:58.730+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:33:58.731+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:58.731+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:33:59.071+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:33:59.371+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:33:59.396+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:59.396+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:33:59.406+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:59.406+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:33:59.408+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:59.408+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:33:59.409+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:33:59.409+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:33:59.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.944 seconds
[2025-05-07T06:34:29.503+0000] {processor.py:186} INFO - Started process (PID=9927) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:34:29.504+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:34:29.505+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:34:29.505+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:34:36.211+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:34:36.555+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:34:36.584+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:34:36.584+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:34:36.597+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:34:36.597+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:34:36.600+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:34:36.600+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:34:36.600+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:34:36.600+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:34:36.612+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.612 seconds
[2025-05-07T06:35:06.712+0000] {processor.py:186} INFO - Started process (PID=10052) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:35:06.713+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:35:06.714+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:06.714+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:35:13.266+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:35:18.856+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:35:18.881+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:18.880+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:35:18.891+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:18.891+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:35:18.895+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:18.895+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:35:18.895+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:18.895+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:35:18.906+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.455 seconds
[2025-05-07T06:35:53.843+0000] {processor.py:186} INFO - Started process (PID=10181) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:35:53.844+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:35:53.845+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:53.845+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:35:54.787+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:35:55.156+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:35:55.182+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:55.181+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:35:55.192+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:55.191+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:35:55.194+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:55.194+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:35:55.195+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:35:55.195+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:35:55.207+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.612 seconds
[2025-05-07T06:36:25.981+0000] {processor.py:186} INFO - Started process (PID=10320) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:36:25.985+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:36:25.986+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:36:25.985+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:36:31.989+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:36:32.310+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:36:32.332+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:36:32.332+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:36:32.341+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:36:32.341+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:36:32.343+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:36:32.343+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:36:32.344+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:36:32.344+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:36:32.356+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.887 seconds
[2025-05-07T06:37:03.155+0000] {processor.py:186} INFO - Started process (PID=10439) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:37:03.156+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:37:03.157+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:03.157+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:37:09.213+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:37:09.530+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:37:09.552+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:09.552+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:37:09.562+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:09.562+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:37:09.566+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:09.566+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:37:09.566+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:09.566+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:37:09.577+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.425 seconds
[2025-05-07T06:37:40.258+0000] {processor.py:186} INFO - Started process (PID=10560) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:37:40.259+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:37:40.260+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:40.260+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:37:47.549+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:37:47.761+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:37:47.784+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:47.784+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:37:47.796+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:47.796+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:37:47.798+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:47.798+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:37:47.799+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:37:47.799+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:37:47.810+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.066 seconds
[2025-05-07T06:38:19.199+0000] {processor.py:186} INFO - Started process (PID=10684) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:38:19.200+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:38:19.201+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:19.201+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:38:20.709+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:38:21.025+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:38:21.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:21.051+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:38:21.061+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:21.061+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:38:21.064+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:21.064+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:38:21.065+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:21.065+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:38:21.077+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.127 seconds
[2025-05-07T06:38:51.786+0000] {processor.py:186} INFO - Started process (PID=10816) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:38:51.788+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:38:51.789+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:51.789+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:38:57.807+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:38:58.111+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:38:58.132+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:58.131+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:38:58.140+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:58.140+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:38:58.142+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:58.142+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:38:58.143+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:38:58.143+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:38:58.154+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.876 seconds
[2025-05-07T06:39:34.244+0000] {processor.py:186} INFO - Started process (PID=10942) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:39:34.253+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:39:34.254+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:39:34.254+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:39:35.370+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:39:35.602+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:39:35.625+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:39:35.625+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:39:35.636+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:39:35.635+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:39:35.638+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:39:35.638+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:39:35.639+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:39:35.639+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:39:35.650+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.655 seconds
[2025-05-07T06:40:05.787+0000] {processor.py:186} INFO - Started process (PID=11069) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:40:05.802+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:40:05.803+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:05.803+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:40:12.430+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:40:12.769+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:40:12.801+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:12.801+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:40:12.811+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:12.811+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:40:12.814+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:12.813+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:40:12.815+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:12.814+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:40:12.826+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.541 seconds
[2025-05-07T06:40:43.339+0000] {processor.py:186} INFO - Started process (PID=11197) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:40:43.343+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:40:43.344+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:43.344+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:40:49.136+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:40:49.430+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:40:49.453+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:49.453+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:40:49.462+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:49.462+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:40:49.464+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:49.464+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:40:49.465+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:40:49.465+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:40:49.476+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.148 seconds
[2025-05-07T06:41:20.156+0000] {processor.py:186} INFO - Started process (PID=11317) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:41:20.157+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:41:20.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:41:20.158+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:41:27.397+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:41:27.685+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:41:27.708+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:41:27.707+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:41:27.718+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:41:27.718+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:41:27.720+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:41:27.720+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:41:27.721+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:41:27.721+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:41:27.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.077 seconds
[2025-05-07T06:41:58.472+0000] {processor.py:186} INFO - Started process (PID=11436) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:41:58.473+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:41:58.474+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:41:58.474+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:42:04.024+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:42:04.304+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:42:04.326+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:04.325+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:42:04.335+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:04.335+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:42:04.338+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:04.337+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:42:04.338+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:04.338+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:42:04.349+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.889 seconds
[2025-05-07T06:42:33.939+0000] {processor.py:186} INFO - Started process (PID=11568) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:42:33.941+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:42:33.943+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:33.942+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:42:40.070+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:42:40.384+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:42:40.406+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:40.406+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:42:40.415+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:40.415+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:42:40.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:40.417+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:42:40.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:42:40.418+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:42:40.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.000 seconds
[2025-05-07T06:43:10.681+0000] {processor.py:186} INFO - Started process (PID=11690) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:43:10.685+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:43:10.686+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:10.686+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:43:16.766+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:43:17.048+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:43:17.071+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:17.071+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:43:17.080+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:17.080+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:43:17.084+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:17.084+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:43:17.085+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:17.085+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:43:17.095+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.920 seconds
[2025-05-07T06:43:47.660+0000] {processor.py:186} INFO - Started process (PID=11813) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:43:47.661+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:43:47.662+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:47.662+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:43:59.518+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:43:54.067+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:43:54.090+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:54.090+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:43:54.099+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:54.099+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:43:54.101+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:54.101+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:43:54.102+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:43:54.102+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:43:54.112+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.468 seconds
[2025-05-07T06:44:24.222+0000] {processor.py:186} INFO - Started process (PID=11945) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:44:24.223+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:44:24.224+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:44:24.224+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:44:30.771+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:44:31.066+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:44:31.088+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:44:31.088+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:44:31.097+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:44:31.097+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:44:31.099+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:44:31.099+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:44:31.100+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:44:31.100+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:44:31.111+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.399 seconds
[2025-05-07T06:45:01.672+0000] {processor.py:186} INFO - Started process (PID=12072) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:45:01.673+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:45:01.674+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:01.674+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:45:07.800+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:45:08.129+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:45:08.162+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:08.162+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:45:08.172+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:08.172+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:45:08.175+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:08.175+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:45:08.176+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:08.176+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:45:08.188+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.029 seconds
[2025-05-07T06:45:39.685+0000] {processor.py:186} INFO - Started process (PID=12192) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:45:39.686+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:45:39.687+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:39.687+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:45:39.859+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:45:40.192+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:45:40.215+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:40.215+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:45:40.225+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:40.225+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:45:40.227+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:40.227+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:45:40.228+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:45:40.228+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:45:40.239+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.810 seconds
[2025-05-07T06:46:10.369+0000] {processor.py:186} INFO - Started process (PID=12320) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:46:10.372+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:46:10.374+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:10.374+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:46:17.719+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:46:18.040+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:46:18.063+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:18.063+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:46:18.072+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:18.072+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:46:18.074+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:18.074+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:46:18.075+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:18.074+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:46:18.086+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.228 seconds
[2025-05-07T06:46:49.002+0000] {processor.py:186} INFO - Started process (PID=12452) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:46:49.007+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:46:49.008+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:49.008+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:46:55.237+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:46:55.548+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:46:55.571+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:55.571+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:46:55.581+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:55.581+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:46:55.583+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:55.583+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:46:55.584+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:46:55.584+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:46:55.595+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.597 seconds
[2025-05-07T06:47:30.025+0000] {processor.py:186} INFO - Started process (PID=12577) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:47:30.026+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:47:30.027+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:47:30.027+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:47:30.471+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:47:30.775+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:47:30.798+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:47:30.798+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:47:30.809+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:47:30.808+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:47:30.811+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:47:30.811+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:47:30.811+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:47:30.811+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:47:30.822+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.050 seconds
[2025-05-07T06:47:59.444+0000] {processor.py:186} INFO - Started process (PID=12709) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:47:59.446+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:47:59.446+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:47:59.446+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:48:06.342+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:48:06.503+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:48:06.526+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:06.525+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:48:06.534+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:06.534+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:48:06.536+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:06.536+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:48:06.537+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:06.537+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:48:06.547+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.609 seconds
[2025-05-07T06:48:40.035+0000] {processor.py:186} INFO - Started process (PID=12837) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:48:40.037+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:48:40.038+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:40.038+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:48:40.775+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:48:41.083+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:48:41.103+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:41.103+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:48:41.111+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:41.111+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:48:41.113+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:41.113+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:48:41.114+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:48:41.114+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:48:41.125+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.349 seconds
[2025-05-07T06:49:15.243+0000] {processor.py:186} INFO - Started process (PID=12962) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:49:15.244+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:49:15.245+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:15.244+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:49:15.438+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:49:15.735+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:49:15.758+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:15.758+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:49:15.769+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:15.769+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:49:15.772+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:15.772+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:49:15.773+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:15.773+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:49:15.785+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.799 seconds
[2025-05-07T06:49:50.111+0000] {processor.py:186} INFO - Started process (PID=13082) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:49:50.115+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:49:50.116+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:50.116+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:49:51.134+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:49:51.433+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:49:51.455+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:51.455+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:49:51.464+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:51.464+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:49:51.466+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:51.466+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:49:51.467+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:49:51.467+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:49:51.477+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.625 seconds
[2025-05-07T06:50:21.669+0000] {processor.py:186} INFO - Started process (PID=13211) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:50:21.671+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:50:21.672+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:50:21.672+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:50:28.119+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:50:28.291+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:50:28.311+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:50:28.310+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:50:28.319+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:50:28.319+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:50:28.321+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:50:28.321+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:50:28.322+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:50:28.322+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:50:28.332+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.165 seconds
[2025-05-07T06:50:58.888+0000] {processor.py:186} INFO - Started process (PID=13330) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:50:58.890+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:50:58.891+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:50:58.890+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:51:04.842+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:51:05.131+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:51:05.152+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:05.151+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:51:05.161+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:05.161+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:51:05.163+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:05.163+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:51:05.164+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:05.164+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:51:05.175+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.289 seconds
[2025-05-07T06:51:36.122+0000] {processor.py:186} INFO - Started process (PID=13462) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:51:36.123+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:51:36.124+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:36.124+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:51:42.209+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:51:42.556+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:51:42.579+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:42.578+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:51:42.589+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:42.589+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:51:42.592+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:42.591+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:51:42.592+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:51:42.592+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:51:42.604+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.989 seconds
[2025-05-07T06:52:13.528+0000] {processor.py:186} INFO - Started process (PID=13589) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:52:13.529+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:52:13.530+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:13.530+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:52:25.456+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:52:20.050+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:52:20.077+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:20.076+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:52:20.089+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:20.088+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:52:20.091+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:20.091+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:52:20.091+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:20.091+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:52:20.102+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.585 seconds
[2025-05-07T06:52:50.945+0000] {processor.py:186} INFO - Started process (PID=13719) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:52:50.949+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:52:50.950+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:50.950+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:52:57.074+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:52:57.366+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:52:57.389+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:57.389+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:52:57.399+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:57.399+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:52:57.401+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:57.401+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:52:57.402+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:52:57.402+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:52:57.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.974 seconds
[2025-05-07T06:53:28.217+0000] {processor.py:186} INFO - Started process (PID=13846) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:53:28.218+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:53:28.219+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:53:28.218+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:53:34.343+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:53:34.651+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:53:34.673+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:53:34.673+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:53:34.684+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:53:34.684+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:53:34.688+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:53:34.687+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:53:34.689+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:53:34.689+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:53:34.703+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.993 seconds
[2025-05-07T06:54:05.000+0000] {processor.py:186} INFO - Started process (PID=13976) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:54:05.002+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:54:05.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:05.002+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:54:11.452+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:54:11.783+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:54:11.809+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:11.808+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:54:11.819+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:11.819+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:54:11.821+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:11.821+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:54:11.822+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:11.822+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:54:11.833+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.342 seconds
[2025-05-07T06:54:42.653+0000] {processor.py:186} INFO - Started process (PID=14106) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:54:42.655+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:54:42.655+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:42.655+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:54:49.193+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:54:49.498+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:54:49.522+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:49.522+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:54:49.532+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:49.532+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:54:49.535+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:49.535+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:54:49.536+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:54:49.535+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:54:49.546+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.397 seconds
[2025-05-07T06:55:20.579+0000] {processor.py:186} INFO - Started process (PID=14229) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:55:20.581+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:55:20.582+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:20.581+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:55:20.851+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:55:21.207+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:55:21.229+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:21.229+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:55:21.238+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:21.238+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:55:21.241+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:21.241+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:55:21.242+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:21.242+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:55:21.253+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.934 seconds
[2025-05-07T06:55:51.720+0000] {processor.py:186} INFO - Started process (PID=14352) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:55:51.721+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:55:51.722+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:51.722+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:55:58.076+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:55:58.385+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:55:58.413+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:58.412+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:55:58.425+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:58.425+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:55:58.429+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:58.428+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:55:58.430+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:55:58.430+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:55:58.448+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.233 seconds
[2025-05-07T06:56:30.908+0000] {processor.py:186} INFO - Started process (PID=14472) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:56:30.910+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:56:30.911+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:56:30.911+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:56:31.878+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:56:32.196+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:56:32.223+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:56:32.222+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:56:32.233+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:56:32.232+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:56:32.235+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:56:32.235+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:56:32.236+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:56:32.236+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:56:32.246+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.597 seconds
[2025-05-07T06:57:02.708+0000] {processor.py:186} INFO - Started process (PID=14595) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:57:02.709+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:57:02.710+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:02.710+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:57:08.997+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:57:09.294+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:57:09.317+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:09.317+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:57:09.327+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:09.326+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:57:09.329+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:09.329+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:57:09.330+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:09.330+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:57:09.341+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.140 seconds
[2025-05-07T06:57:40.937+0000] {processor.py:186} INFO - Started process (PID=14713) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:57:40.939+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:57:40.940+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:40.940+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:57:41.717+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:57:42.002+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:57:42.023+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:42.022+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:57:42.032+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:42.032+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:57:42.034+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:42.034+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:57:42.035+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:57:42.035+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:57:42.045+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.371 seconds
[2025-05-07T06:58:13.010+0000] {processor.py:186} INFO - Started process (PID=14838) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:58:13.011+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:58:13.012+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:58:13.012+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:58:20.413+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:58:25.863+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:58:25.893+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:58:25.892+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:58:25.904+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:58:25.904+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:58:25.907+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:58:25.907+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:58:25.908+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:58:25.908+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:58:25.921+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.149 seconds
[2025-05-07T06:58:56.478+0000] {processor.py:186} INFO - Started process (PID=14964) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:58:56.482+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:58:56.483+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:58:56.483+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:59:03.349+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:59:03.673+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:59:03.694+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:03.694+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:59:03.703+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:03.703+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:59:03.706+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:03.705+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:59:03.706+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:03.706+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:59:03.717+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.745 seconds
[2025-05-07T06:59:36.142+0000] {processor.py:186} INFO - Started process (PID=15087) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T06:59:36.143+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T06:59:36.144+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:36.144+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T06:59:36.447+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T06:59:36.729+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T06:59:36.752+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:36.752+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T06:59:36.763+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:36.763+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T06:59:36.766+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:36.766+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T06:59:36.767+0000] {logging_mixin.py:190} INFO - [2025-05-07T06:59:36.766+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T06:59:36.777+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.893 seconds
[2025-05-07T07:00:06.835+0000] {processor.py:186} INFO - Started process (PID=15211) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:00:06.840+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:00:06.840+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:06.840+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:00:10.861+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:00:11.191+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:00:11.228+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:11.228+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:00:11.238+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:11.238+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:00:11.241+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:11.241+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:00:11.241+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:11.241+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:00:11.253+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 12.180 seconds
[2025-05-07T07:00:41.273+0000] {processor.py:186} INFO - Started process (PID=15329) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:00:41.283+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:00:41.284+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:41.283+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:00:41.542+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:00:41.895+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:00:41.919+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:41.919+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:00:41.930+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:41.929+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:00:41.932+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:41.932+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:00:41.933+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:00:41.933+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:00:41.944+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.932 seconds
[2025-05-07T07:01:12.016+0000] {processor.py:186} INFO - Started process (PID=15455) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:01:12.017+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:01:12.018+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:12.017+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:01:18.616+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:01:18.938+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:01:18.961+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:18.960+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:01:18.970+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:18.970+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:01:18.972+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:18.972+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:01:18.973+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:18.972+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:01:18.984+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.468 seconds
[2025-05-07T07:01:49.990+0000] {processor.py:186} INFO - Started process (PID=15577) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:01:49.991+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:01:49.992+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:49.992+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:01:56.295+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:01:56.483+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:01:56.505+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:56.505+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:01:56.516+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:56.516+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:01:56.518+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:56.518+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:01:56.519+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:01:56.519+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:01:56.530+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.546 seconds
[2025-05-07T07:02:27.527+0000] {processor.py:186} INFO - Started process (PID=15704) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:02:27.529+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:02:27.530+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:02:27.530+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:02:34.174+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:02:34.477+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:02:34.500+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:02:34.500+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:02:34.510+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:02:34.510+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:02:34.512+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:02:34.512+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:02:34.513+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:02:34.513+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:02:34.523+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.501 seconds
[2025-05-07T07:03:06.545+0000] {processor.py:186} INFO - Started process (PID=15830) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:03:06.547+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:03:06.549+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:06.548+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:03:08.656+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:03:09.024+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:03:09.080+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:09.080+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:03:09.091+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:09.091+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:03:09.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:09.093+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:03:09.094+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:09.094+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:03:09.106+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.836 seconds
[2025-05-07T07:03:39.152+0000] {processor.py:186} INFO - Started process (PID=15963) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:03:39.152+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:03:39.153+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:39.153+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:03:47.899+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:03:48.210+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:03:48.242+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:48.241+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:03:48.252+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:48.252+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:03:48.255+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:48.254+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:03:48.255+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:03:48.255+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:03:48.266+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.890 seconds
[2025-05-07T07:04:18.676+0000] {processor.py:186} INFO - Started process (PID=16094) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:04:18.677+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:04:18.679+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:04:18.678+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:04:25.174+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:04:25.511+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:04:25.534+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:04:25.534+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:04:25.543+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:04:25.543+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:04:25.545+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:04:25.545+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:04:25.546+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:04:25.546+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:04:25.557+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.389 seconds
[2025-05-07T07:05:01.603+0000] {processor.py:186} INFO - Started process (PID=16223) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:05:01.606+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:05:01.607+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:01.607+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:05:02.098+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:05:02.197+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:05:02.220+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:02.219+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:05:02.229+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:02.229+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:05:02.231+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:02.231+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:05:02.232+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:02.232+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:05:02.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.904 seconds
[2025-05-07T07:05:36.702+0000] {processor.py:186} INFO - Started process (PID=16336) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:05:36.703+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:05:36.705+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:36.704+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:05:37.523+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:05:37.810+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:05:37.833+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:37.833+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:05:37.842+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:37.842+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:05:37.845+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:37.845+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:05:37.846+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:05:37.846+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:05:37.856+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.419 seconds
[2025-05-07T07:06:08.288+0000] {processor.py:186} INFO - Started process (PID=16462) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:06:08.292+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:06:08.293+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:08.293+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:06:15.376+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:06:15.591+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:06:15.614+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:15.614+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:06:15.624+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:15.624+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:06:15.626+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:15.626+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:06:15.627+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:15.627+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:06:15.639+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.848 seconds
[2025-05-07T07:06:51.927+0000] {processor.py:186} INFO - Started process (PID=16589) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:06:51.928+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:06:51.929+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:51.929+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:06:52.837+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:06:53.143+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:06:53.164+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:53.164+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:06:53.173+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:53.173+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:06:53.175+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:53.175+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:06:53.176+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:06:53.176+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:06:53.187+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.521 seconds
[2025-05-07T07:07:24.143+0000] {processor.py:186} INFO - Started process (PID=16717) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:07:24.144+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:07:24.145+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:07:24.145+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:07:30.140+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:07:30.453+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:07:30.475+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:07:30.474+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:07:30.484+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:07:30.484+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:07:30.486+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:07:30.486+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:07:30.487+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:07:30.487+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:07:30.497+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.861 seconds
[2025-05-07T07:08:00.838+0000] {processor.py:186} INFO - Started process (PID=16846) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:08:00.839+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:08:00.840+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:00.840+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:08:06.887+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:08:07.216+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:08:07.236+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:07.236+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:08:07.246+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:07.246+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:08:07.248+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:07.248+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:08:07.249+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:07.249+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:08:07.262+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.432 seconds
[2025-05-07T07:08:37.962+0000] {processor.py:186} INFO - Started process (PID=16968) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:08:37.966+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:08:37.966+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:37.966+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:08:44.115+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:08:44.454+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:08:44.481+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:44.481+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:08:44.491+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:44.491+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:08:44.493+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:44.493+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:08:44.494+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:08:44.494+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:08:44.505+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.060 seconds
[2025-05-07T07:09:15.122+0000] {processor.py:186} INFO - Started process (PID=17093) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:09:15.125+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:09:15.126+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:09:15.126+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:09:21.515+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:09:26.979+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:09:26.999+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:09:26.999+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:09:27.007+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:09:27.007+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:09:27.009+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:09:27.009+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:09:27.010+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:09:27.010+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:09:27.021+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.139 seconds
[2025-05-07T07:09:57.727+0000] {processor.py:186} INFO - Started process (PID=17222) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:09:57.728+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:09:57.730+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:09:57.729+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:10:04.247+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:10:04.557+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:10:04.577+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:04.577+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:10:04.587+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:04.587+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:10:04.589+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:04.589+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:10:04.590+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:04.590+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:10:04.610+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.384 seconds
[2025-05-07T07:10:35.255+0000] {processor.py:186} INFO - Started process (PID=17348) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:10:35.258+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:10:35.259+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:35.259+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:10:41.833+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:10:41.646+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:10:41.669+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:41.669+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:10:41.678+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:41.678+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:10:41.680+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:41.680+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:10:41.681+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:10:41.680+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:10:41.691+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.439 seconds
[2025-05-07T07:11:12.175+0000] {processor.py:186} INFO - Started process (PID=17468) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:11:12.176+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:11:12.177+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:12.177+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:11:12.740+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:11:13.060+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:11:13.081+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:13.081+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:11:13.091+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:13.090+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:11:13.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:13.093+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:11:13.094+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:13.093+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:11:13.121+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.209 seconds
[2025-05-07T07:11:43.972+0000] {processor.py:186} INFO - Started process (PID=17602) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:11:43.974+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:11:43.975+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:43.975+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:11:50.573+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:11:50.848+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:11:50.871+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:50.870+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:11:50.881+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:50.881+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:11:50.883+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:50.883+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:11:50.884+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:11:50.883+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:11:50.894+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.425 seconds
[2025-05-07T07:12:21.452+0000] {processor.py:186} INFO - Started process (PID=17725) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:12:21.453+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:12:21.454+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:12:21.454+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:12:27.514+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:12:27.853+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:12:27.875+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:12:27.874+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:12:27.884+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:12:27.883+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:12:27.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:12:27.885+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:12:27.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:12:27.886+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:12:27.897+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.447 seconds
[2025-05-07T07:12:58.574+0000] {processor.py:186} INFO - Started process (PID=17864) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:12:58.575+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:12:58.577+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:12:58.576+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:13:05.232+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:13:05.612+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:13:05.637+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:05.637+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:13:05.647+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:05.647+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:13:05.650+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:05.650+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:13:05.651+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:05.651+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:13:05.662+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.594 seconds
[2025-05-07T07:13:36.352+0000] {processor.py:186} INFO - Started process (PID=17985) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:13:36.356+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:13:36.357+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:36.357+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:13:42.460+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:13:42.743+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:13:42.766+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:42.766+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:13:42.776+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:42.776+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:13:42.779+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:42.779+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:13:42.780+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:13:42.780+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:13:42.791+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.451 seconds
[2025-05-07T07:14:13.273+0000] {processor.py:186} INFO - Started process (PID=18105) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:14:13.284+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:14:13.286+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:14:13.285+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:14:19.285+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:14:19.604+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:14:19.623+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:14:19.623+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:14:19.633+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:14:19.633+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:14:19.635+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:14:19.635+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:14:19.636+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:14:19.636+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:14:19.654+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.882 seconds
[2025-05-07T07:14:50.389+0000] {processor.py:186} INFO - Started process (PID=18231) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:14:50.391+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:14:50.391+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:14:50.391+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:14:57.162+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:15:02.729+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:15:02.756+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:02.756+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:15:02.769+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:02.768+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:15:02.771+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:02.771+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:15:02.772+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:02.772+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:14:57.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.639 seconds
[2025-05-07T07:15:27.210+0000] {processor.py:186} INFO - Started process (PID=18361) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:15:27.211+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:15:27.212+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:27.212+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:15:33.714+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:15:34.027+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:15:34.050+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:34.050+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:15:34.059+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:34.059+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:15:34.061+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:34.061+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:15:34.061+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:15:34.061+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:15:34.072+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.376 seconds
[2025-05-07T07:16:04.729+0000] {processor.py:186} INFO - Started process (PID=18484) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:16:04.731+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:16:04.732+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:04.731+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:16:11.475+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:16:11.797+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:16:11.820+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:11.819+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:16:11.831+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:11.831+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:16:11.833+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:11.833+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:16:11.834+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:11.834+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:16:11.845+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.625 seconds
[2025-05-07T07:16:47.783+0000] {processor.py:186} INFO - Started process (PID=18619) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:16:47.784+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:16:47.785+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:47.785+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:16:48.001+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:16:48.300+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:16:48.323+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:48.322+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:16:48.331+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:48.331+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:16:48.333+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:48.333+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:16:48.333+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:16:48.333+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:16:48.343+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.827 seconds
[2025-05-07T07:17:19.030+0000] {processor.py:186} INFO - Started process (PID=18740) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:17:19.031+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:17:19.033+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:17:19.032+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:17:25.557+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:17:25.875+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:17:25.898+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:17:25.898+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:17:25.909+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:17:25.908+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:17:25.911+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:17:25.911+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:17:25.912+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:17:25.912+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:17:25.924+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.404 seconds
[2025-05-07T07:17:56.626+0000] {processor.py:186} INFO - Started process (PID=18868) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:17:56.638+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:17:56.640+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:17:56.639+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:18:02.752+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:18:03.061+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:18:03.084+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:03.083+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:18:03.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:03.093+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:18:03.096+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:03.095+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:18:03.096+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:03.096+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:18:03.115+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.498 seconds
[2025-05-07T07:18:33.565+0000] {processor.py:186} INFO - Started process (PID=18990) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:18:33.566+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:18:33.567+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:33.567+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:18:40.494+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:18:40.839+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:18:40.864+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:40.864+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:18:40.874+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:40.874+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:18:40.877+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:40.877+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:18:40.878+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:18:40.878+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:18:40.889+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.826 seconds
[2025-05-07T07:19:11.805+0000] {processor.py:186} INFO - Started process (PID=19125) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:19:11.806+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:19:11.807+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:11.807+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:19:21.021+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:19:21.270+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:19:21.293+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:21.293+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:19:21.302+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:21.302+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:19:21.304+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:21.304+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:19:21.305+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:21.305+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:19:21.316+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 10.525 seconds
[2025-05-07T07:19:52.174+0000] {processor.py:186} INFO - Started process (PID=19254) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:19:52.175+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:19:52.176+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:52.175+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:19:58.187+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:19:58.462+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:19:58.483+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:58.483+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:19:58.493+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:58.493+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:19:58.495+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:58.495+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:19:58.496+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:19:58.495+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:19:58.508+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.339 seconds
[2025-05-07T07:20:28.985+0000] {processor.py:186} INFO - Started process (PID=19378) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:20:28.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:20:28.988+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:20:28.988+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:20:36.028+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:20:36.588+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:20:36.615+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:20:36.614+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:20:36.626+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:20:36.626+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:20:36.629+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:20:36.628+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:20:36.630+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:20:36.629+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:20:36.645+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.163 seconds
[2025-05-07T07:21:08.146+0000] {processor.py:186} INFO - Started process (PID=19515) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:21:08.146+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:21:08.148+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:08.148+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:21:09.317+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:21:09.678+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:21:09.702+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:09.702+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:21:09.711+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:09.711+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:21:09.713+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:09.713+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:21:09.714+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:09.714+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:21:09.724+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.855 seconds
[2025-05-07T07:21:40.328+0000] {processor.py:186} INFO - Started process (PID=19655) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:21:40.332+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:21:40.333+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:40.332+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:21:46.938+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:21:47.234+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:21:47.254+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:47.254+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:21:47.263+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:47.263+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:21:47.265+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:47.265+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:21:47.266+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:21:47.266+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:21:47.277+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.457 seconds
[2025-05-07T07:22:23.324+0000] {processor.py:186} INFO - Started process (PID=19779) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:22:23.327+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:22:23.330+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:23.329+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:22:24.556+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:22:24.764+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:22:24.786+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:24.785+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:22:24.796+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:24.796+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:22:24.798+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:24.798+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:22:24.799+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:24.799+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:22:24.809+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.764 seconds
[2025-05-07T07:22:58.247+0000] {processor.py:186} INFO - Started process (PID=19906) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:22:58.248+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:22:58.249+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:58.248+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:22:58.598+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:22:58.893+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:22:58.914+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:58.914+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:22:58.922+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:58.922+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:22:58.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:58.924+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:22:58.925+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:22:58.925+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:22:58.935+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.957 seconds
[2025-05-07T07:23:33.498+0000] {processor.py:186} INFO - Started process (PID=20040) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:23:33.499+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:23:33.500+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:23:33.500+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:23:34.255+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:23:34.564+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:23:34.584+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:23:34.584+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:23:34.594+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:23:34.594+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:23:34.596+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:23:34.596+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:23:34.597+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:23:34.597+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:23:34.607+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.381 seconds
[2025-05-07T07:24:05.306+0000] {processor.py:186} INFO - Started process (PID=20161) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:24:05.310+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:24:05.311+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:05.311+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:24:11.810+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:24:12.125+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:24:12.149+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:12.148+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:24:12.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:12.158+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:24:12.161+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:12.161+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:24:12.162+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:12.162+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:24:12.173+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.367 seconds
[2025-05-07T07:24:43.522+0000] {processor.py:186} INFO - Started process (PID=20274) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:24:43.523+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:24:43.524+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:43.524+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:24:43.885+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:24:44.241+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:24:44.263+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:44.263+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:24:44.273+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:44.273+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:24:44.275+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:44.275+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:24:44.276+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:24:44.276+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:24:44.287+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.040 seconds
[2025-05-07T07:25:14.306+0000] {processor.py:186} INFO - Started process (PID=20401) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:25:14.307+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:25:14.308+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:25:14.308+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:25:24.770+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:25:25.078+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:25:25.102+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:25:25.102+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:25:25.112+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:25:25.112+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:25:25.114+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:25:25.114+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:25:25.115+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:25:25.115+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:25:25.127+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 11.828 seconds
[2025-05-07T07:25:58.747+0000] {processor.py:186} INFO - Started process (PID=20535) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:25:58.749+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:25:58.750+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:25:58.750+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:26:00.005+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:26:00.307+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:26:00.328+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:00.328+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:26:00.338+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:00.338+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:26:00.340+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:00.340+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:26:00.341+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:00.341+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:26:00.353+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.882 seconds
[2025-05-07T07:26:31.302+0000] {processor.py:186} INFO - Started process (PID=20675) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:26:31.307+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:26:31.308+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:31.307+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:26:37.842+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:26:38.152+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:26:38.172+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:38.172+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:26:38.181+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:38.181+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:26:38.184+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:38.184+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:26:38.184+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:26:38.184+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:26:38.195+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.393 seconds
[2025-05-07T07:27:08.741+0000] {processor.py:186} INFO - Started process (PID=20794) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:27:08.742+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:27:08.744+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:08.743+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:27:11.080+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:27:11.392+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:27:11.414+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:11.413+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:27:11.422+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:11.422+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:27:11.424+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:11.424+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:27:11.425+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:11.425+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:27:11.435+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.976 seconds
[2025-05-07T07:27:42.081+0000] {processor.py:186} INFO - Started process (PID=20933) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:27:42.082+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:27:42.082+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:42.082+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:27:53.890+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:27:48.430+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:27:48.451+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:48.451+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:27:48.460+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:48.460+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:27:48.462+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:48.462+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:27:48.463+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:27:48.463+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:27:48.473+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.399 seconds
[2025-05-07T07:28:19.052+0000] {processor.py:186} INFO - Started process (PID=21051) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:28:19.054+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:28:19.054+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:19.054+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:28:19.860+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:28:20.170+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:28:20.193+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:20.192+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:28:20.202+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:20.202+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:28:20.205+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:20.205+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:28:20.206+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:20.206+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:28:20.233+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.451 seconds
[2025-05-07T07:28:50.845+0000] {processor.py:186} INFO - Started process (PID=21171) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:28:50.846+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:28:50.847+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:50.847+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:28:57.556+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:28:57.871+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:28:57.909+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:57.908+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:28:57.920+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:57.920+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:28:57.923+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:57.923+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:28:57.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:28:57.924+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:28:57.936+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.595 seconds
[2025-05-07T07:29:28.643+0000] {processor.py:186} INFO - Started process (PID=21302) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:29:28.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:29:28.645+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:29:28.645+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:29:34.265+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:29:34.559+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:29:34.583+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:29:34.582+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:29:34.592+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:29:34.592+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:29:34.594+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:29:34.594+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:29:34.595+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:29:34.595+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:29:34.607+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.967 seconds
[2025-05-07T07:30:09.212+0000] {processor.py:186} INFO - Started process (PID=21422) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:30:09.215+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:30:09.216+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:09.216+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:30:10.082+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:30:10.383+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:30:10.407+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:10.406+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:30:10.415+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:10.415+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:30:10.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:10.417+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:30:10.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:10.418+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:30:10.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.494 seconds
[2025-05-07T07:30:40.981+0000] {processor.py:186} INFO - Started process (PID=21555) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:30:40.982+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:30:40.984+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:40.983+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:30:47.705+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:30:47.999+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:30:48.022+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:48.022+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:30:48.033+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:48.033+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:30:48.035+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:48.035+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:30:48.036+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:30:48.036+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:30:48.047+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.581 seconds
[2025-05-07T07:31:19.118+0000] {processor.py:186} INFO - Started process (PID=21683) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:31:19.119+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:31:19.120+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:19.119+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:31:20.275+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:31:20.511+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:31:20.545+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:20.545+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:31:20.556+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:20.556+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:31:20.558+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:20.558+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:31:20.559+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:20.559+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:31:20.571+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.729 seconds
[2025-05-07T07:31:50.945+0000] {processor.py:186} INFO - Started process (PID=21802) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:31:50.946+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:31:50.947+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:50.947+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:31:58.040+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:31:58.336+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:31:58.359+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:58.358+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:31:58.368+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:58.368+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:31:58.371+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:58.371+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:31:58.371+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:31:58.371+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:31:58.381+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.939 seconds
[2025-05-07T07:32:34.239+0000] {processor.py:186} INFO - Started process (PID=21940) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:32:34.240+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:32:34.241+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:32:34.241+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:32:35.139+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:32:35.372+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:32:35.393+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:32:35.393+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:32:35.402+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:32:35.402+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:32:35.405+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:32:35.405+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:32:35.406+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:32:35.406+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:32:35.416+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.456 seconds
[2025-05-07T07:33:09.412+0000] {processor.py:186} INFO - Started process (PID=22073) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:33:09.413+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:33:09.414+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:09.413+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:33:10.134+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:33:10.440+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:33:10.465+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:10.465+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:33:10.474+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:10.474+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:33:10.476+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:10.476+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:33:10.477+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:10.477+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:33:10.487+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.354 seconds
[2025-05-07T07:33:40.962+0000] {processor.py:186} INFO - Started process (PID=22197) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:33:40.963+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:33:40.964+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:40.964+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:33:47.711+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:33:47.954+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:33:47.976+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:47.976+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:33:47.986+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:47.986+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:33:47.988+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:47.988+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:33:47.989+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:33:47.989+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:33:48.000+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.542 seconds
[2025-05-07T07:34:18.489+0000] {processor.py:186} INFO - Started process (PID=22319) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:34:18.490+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:34:18.491+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:34:18.491+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:34:25.013+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:34:25.330+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:34:25.351+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:34:25.351+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:34:25.360+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:34:25.360+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:34:25.362+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:34:25.362+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:34:25.363+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:34:25.363+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:34:25.373+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.889 seconds
[2025-05-07T07:34:59.578+0000] {processor.py:186} INFO - Started process (PID=22445) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:34:59.579+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:34:59.580+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:34:59.580+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:35:01.106+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:35:01.386+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:35:01.408+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:01.407+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:35:01.419+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:01.418+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:35:01.421+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:01.421+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:35:01.422+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:01.422+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:35:01.432+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.138 seconds
[2025-05-07T07:35:32.225+0000] {processor.py:186} INFO - Started process (PID=22594) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:35:32.226+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:35:32.227+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:32.227+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:35:38.349+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:35:38.670+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:35:38.699+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:38.698+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:35:38.707+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:38.707+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:35:38.709+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:38.709+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:35:38.710+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:35:38.710+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:35:38.720+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.000 seconds
[2025-05-07T07:36:09.256+0000] {processor.py:186} INFO - Started process (PID=22711) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:36:09.257+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:36:09.258+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:09.258+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:36:16.638+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:36:16.914+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:36:16.935+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:16.934+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:36:16.944+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:16.944+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:36:16.946+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:16.946+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:36:16.947+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:16.947+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:36:16.957+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.711 seconds
[2025-05-07T07:36:49.860+0000] {processor.py:186} INFO - Started process (PID=22852) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:36:49.861+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:36:49.862+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:49.862+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:36:51.413+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:36:51.616+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:36:51.637+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:51.637+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:36:51.646+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:51.646+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:36:51.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:51.648+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:36:51.649+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:36:51.649+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:36:51.659+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.081 seconds
[2025-05-07T07:37:22.372+0000] {processor.py:186} INFO - Started process (PID=22982) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:37:22.374+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:37:22.375+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:37:22.375+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:37:29.047+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:37:29.370+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:37:29.397+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:37:29.396+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:37:34.681+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:37:34.680+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:37:34.684+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:37:34.684+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:37:34.686+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:37:34.686+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:37:34.707+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.573 seconds
[2025-05-07T07:38:09.931+0000] {processor.py:186} INFO - Started process (PID=23105) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:38:09.932+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:38:09.933+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:09.932+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:38:11.333+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:38:11.640+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:38:11.660+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:11.660+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:38:11.668+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:11.668+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:38:11.670+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:11.670+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:38:11.671+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:11.671+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:38:11.683+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.035 seconds
[2025-05-07T07:38:42.429+0000] {processor.py:186} INFO - Started process (PID=23245) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:38:42.432+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:38:42.433+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:42.433+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:38:49.076+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:38:49.418+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:38:49.440+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:49.439+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:38:49.450+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:49.449+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:38:49.452+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:49.452+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:38:49.453+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:38:49.453+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:38:49.466+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.548 seconds
[2025-05-07T07:39:20.477+0000] {processor.py:186} INFO - Started process (PID=23373) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:39:20.481+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:39:20.482+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:39:20.482+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:39:26.709+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:39:27.007+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:39:27.029+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:39:27.029+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:39:27.038+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:39:27.038+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:39:27.040+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:39:27.040+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:39:27.041+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:39:27.041+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:39:27.051+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.073 seconds
[2025-05-07T07:39:57.911+0000] {processor.py:186} INFO - Started process (PID=23501) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:39:57.912+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:39:57.913+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:39:57.913+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:40:10.170+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:40:04.697+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:40:04.719+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:04.718+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:40:04.729+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:04.728+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:40:04.731+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:04.731+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:40:04.732+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:04.732+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:40:04.743+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.841 seconds
[2025-05-07T07:40:40.210+0000] {processor.py:186} INFO - Started process (PID=23624) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:40:40.212+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:40:40.213+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:40.212+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:40:41.974+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:40:42.383+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:40:42.416+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:42.416+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:40:42.426+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:42.425+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:40:42.429+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:42.429+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:40:42.430+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:40:42.430+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:40:42.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.520 seconds
[2025-05-07T07:41:15.288+0000] {processor.py:186} INFO - Started process (PID=23756) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:41:15.289+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:41:15.290+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:15.290+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:41:16.683+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:41:17.004+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:41:17.042+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:17.042+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:41:17.055+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:17.055+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:41:17.058+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:17.057+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:41:17.058+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:17.058+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:41:17.070+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.060 seconds
[2025-05-07T07:41:47.741+0000] {processor.py:186} INFO - Started process (PID=23896) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:41:47.742+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:41:47.743+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:47.743+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:42:00.159+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:41:54.743+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:41:54.790+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:54.789+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:41:54.799+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:54.799+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:41:54.801+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:54.801+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:41:54.801+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:41:54.801+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:41:54.813+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.088 seconds
[2025-05-07T07:42:25.816+0000] {processor.py:186} INFO - Started process (PID=24017) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:42:25.817+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:42:25.818+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:42:25.818+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:42:32.598+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:42:32.918+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:42:32.939+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:42:32.939+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:42:32.948+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:42:32.948+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:42:32.951+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:42:32.950+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:42:32.951+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:42:32.951+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:42:32.962+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.659 seconds
[2025-05-07T07:43:02.988+0000] {processor.py:186} INFO - Started process (PID=24150) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:43:02.989+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:43:02.990+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:02.990+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:43:09.563+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:43:09.769+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:43:09.791+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:09.791+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:43:09.801+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:09.801+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:43:09.803+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:09.803+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:43:09.804+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:09.804+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:43:09.815+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.331 seconds
[2025-05-07T07:43:39.912+0000] {processor.py:186} INFO - Started process (PID=24274) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:43:39.914+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:43:39.915+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:39.915+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:43:46.048+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:43:46.363+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:43:46.386+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:46.385+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:43:46.394+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:46.394+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:43:46.397+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:46.397+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:43:46.398+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:43:46.398+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:43:46.408+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.495 seconds
[2025-05-07T07:44:20.520+0000] {processor.py:186} INFO - Started process (PID=24403) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:44:20.521+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:44:20.522+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:44:20.522+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:44:22.367+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:44:22.748+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:44:22.803+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:44:22.803+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:44:22.813+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:44:22.813+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:44:22.816+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:44:22.815+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:44:22.816+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:44:22.816+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:44:22.828+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.598 seconds
[2025-05-07T07:44:53.141+0000] {processor.py:186} INFO - Started process (PID=24547) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:44:53.143+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:44:53.144+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:44:53.143+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:44:59.975+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:45:00.404+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:45:00.443+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:00.442+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:45:00.452+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:00.452+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:45:00.455+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:00.455+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:45:00.456+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:00.456+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:45:00.469+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.335 seconds
[2025-05-07T07:45:31.287+0000] {processor.py:186} INFO - Started process (PID=24677) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:45:31.288+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:45:31.289+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:31.288+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:45:38.063+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:45:38.324+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:45:38.348+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:38.348+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:45:38.357+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:38.357+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:45:38.360+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:38.359+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:45:38.360+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:45:38.360+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:45:38.371+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.580 seconds
[2025-05-07T07:46:10.549+0000] {processor.py:186} INFO - Started process (PID=24815) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:46:10.551+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:46:10.552+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:10.552+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:46:11.175+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:46:11.353+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:46:11.375+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:11.374+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:46:11.383+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:11.383+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:46:11.385+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:11.385+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:46:11.386+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:11.386+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:46:11.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.133 seconds
[2025-05-07T07:46:45.646+0000] {processor.py:186} INFO - Started process (PID=24952) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:46:45.650+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:46:45.651+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:45.651+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:46:46.388+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:46:46.703+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:46:46.726+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:46.726+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:46:46.736+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:46.736+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:46:46.739+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:46.739+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:46:46.740+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:46:46.739+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:46:46.751+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.386 seconds
[2025-05-07T07:47:17.387+0000] {processor.py:186} INFO - Started process (PID=25078) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:47:17.388+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:47:17.389+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:47:17.389+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:47:24.026+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:47:24.358+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:47:24.384+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:47:24.384+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:47:24.396+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:47:24.396+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:47:24.398+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:47:24.398+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:47:24.399+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:47:24.399+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:47:24.411+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.522 seconds
[2025-05-07T07:47:54.697+0000] {processor.py:186} INFO - Started process (PID=25212) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:47:54.698+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:47:54.699+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:47:54.699+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:48:01.578+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:48:01.887+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:48:01.913+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:01.912+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:48:01.922+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:01.921+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:48:01.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:01.924+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:48:01.925+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:01.925+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:48:01.936+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.245 seconds
[2025-05-07T07:48:35.763+0000] {processor.py:186} INFO - Started process (PID=25342) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:48:35.764+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:48:35.765+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:35.765+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:48:36.187+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:48:36.499+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:48:36.521+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:36.521+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:48:36.530+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:36.530+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:48:36.533+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:36.532+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:48:36.534+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:48:36.533+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:48:36.545+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.061 seconds
[2025-05-07T07:49:06.613+0000] {processor.py:186} INFO - Started process (PID=25465) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:49:06.614+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:49:06.616+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:06.615+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:49:12.592+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:49:12.963+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:49:12.992+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:12.991+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:49:13.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:13.003+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:49:13.007+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:13.007+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:49:13.009+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:13.008+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:49:13.022+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.919 seconds
[2025-05-07T07:49:45.866+0000] {processor.py:186} INFO - Started process (PID=25593) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:49:45.867+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:49:45.868+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:45.868+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:49:46.593+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:49:46.927+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:49:46.951+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:46.951+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:49:46.961+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:46.960+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:49:46.963+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:46.963+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:49:46.964+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:49:46.964+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:49:46.975+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.391 seconds
[2025-05-07T07:50:21.085+0000] {processor.py:186} INFO - Started process (PID=25716) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:50:21.086+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:50:21.087+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:21.087+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:50:21.398+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:50:21.719+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:50:21.743+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:21.743+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:50:21.753+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:21.753+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:50:21.755+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:21.755+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:50:21.756+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:21.756+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:50:21.766+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.965 seconds
[2025-05-07T07:50:50.448+0000] {processor.py:186} INFO - Started process (PID=25843) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:50:50.452+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:50:50.453+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:50.453+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:50:57.032+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:50:57.347+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:50:57.370+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:57.370+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:50:57.379+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:57.379+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:50:57.382+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:57.382+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:50:57.383+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:50:57.383+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:50:57.408+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.469 seconds
[2025-05-07T07:51:27.844+0000] {processor.py:186} INFO - Started process (PID=25964) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:51:27.846+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:51:27.846+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:51:27.846+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:51:34.433+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:51:34.729+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:51:34.751+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:51:34.750+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:51:34.761+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:51:34.760+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:51:34.763+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:51:34.763+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:51:34.764+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:51:34.764+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:51:34.775+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.432 seconds
[2025-05-07T07:52:05.213+0000] {processor.py:186} INFO - Started process (PID=26082) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:52:05.214+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:52:05.215+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:05.215+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:52:11.433+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:52:11.772+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:52:11.794+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:11.794+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:52:11.803+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:11.803+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:52:11.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:11.805+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:52:11.806+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:11.806+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:52:11.816+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.605 seconds
[2025-05-07T07:52:42.164+0000] {processor.py:186} INFO - Started process (PID=26209) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:52:42.165+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:52:42.166+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:42.166+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:52:49.042+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:52:49.248+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:52:49.271+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:49.270+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:52:49.280+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:49.279+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:52:49.282+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:49.282+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:52:49.282+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:52:49.282+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:52:49.293+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.637 seconds
[2025-05-07T07:53:21.278+0000] {processor.py:186} INFO - Started process (PID=26343) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:53:21.280+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:53:21.281+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:53:21.281+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:53:25.654+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:53:31.267+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:53:31.289+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:53:31.289+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:53:31.298+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:53:31.298+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:53:31.301+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:53:31.301+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:53:31.302+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:53:31.302+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:53:31.313+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 11.042 seconds
[2025-05-07T07:54:01.946+0000] {processor.py:186} INFO - Started process (PID=26490) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:54:01.947+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:54:01.948+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:01.947+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:54:08.270+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:54:08.606+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:54:08.628+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:08.628+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T07:54:08.638+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:08.638+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T07:54:08.640+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:08.640+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T07:54:08.641+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:08.641+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T07:54:08.652+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.208 seconds
[2025-05-07T07:54:20.005+0000] {processor.py:186} INFO - Started process (PID=26606) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:54:20.006+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:54:20.007+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:20.007+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:54:26.371+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:54:27.898+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:27.898+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:54:28.510+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:54:28.507+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5650ad5bca8e <unknown>
#1 0x5650ad079b0b <unknown>
#2 0x5650ad0b04ea <unknown>
#3 0x5650ad0abaef <unknown>
#4 0x5650ad0ffb18 <unknown>
#5 0x5650ad0ed9b3 <unknown>
#6 0x5650ad0b7c59 <unknown>
#7 0x5650ad0b8a08 <unknown>
#8 0x5650ad58940a <unknown>
#9 0x5650ad58c85e <unknown>
#10 0x5650ad58c308 <unknown>
#11 0x5650ad58cce5 <unknown>
#12 0x5650ad572b7b <unknown>
#13 0x5650ad58d050 <unknown>
#14 0x5650ad55bae9 <unknown>
#15 0x5650ad5abdf5 <unknown>
#16 0x5650ad5abfdb <unknown>
#17 0x5650ad5bbc05 <unknown>
#18 0x7fa3c48a6134 <unknown>
[2025-05-07T07:54:28.512+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:54:28.536+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.539 seconds
[2025-05-07T07:55:01.505+0000] {processor.py:186} INFO - Started process (PID=26800) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:55:01.506+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:55:01.508+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:55:01.507+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:55:02.280+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:55:02.640+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:55:02.639+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:55:03.242+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:55:03.240+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5626baee3a8e <unknown>
#1 0x5626ba9a0b0b <unknown>
#2 0x5626ba9d74ea <unknown>
#3 0x5626ba9d2aef <unknown>
#4 0x5626baa26b18 <unknown>
#5 0x5626baa149b3 <unknown>
#6 0x5626ba9dec59 <unknown>
#7 0x5626ba9dfa08 <unknown>
#8 0x5626baeb040a <unknown>
#9 0x5626baeb385e <unknown>
#10 0x5626baeb3308 <unknown>
#11 0x5626baeb3ce5 <unknown>
#12 0x5626bae99b7b <unknown>
#13 0x5626baeb4050 <unknown>
#14 0x5626bae82ae9 <unknown>
#15 0x5626baed2df5 <unknown>
#16 0x5626baed2fdb <unknown>
#17 0x5626baee2c05 <unknown>
#18 0x7fc282b3d134 <unknown>
[2025-05-07T07:55:03.242+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:55:03.256+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.037 seconds
[2025-05-07T07:55:34.090+0000] {processor.py:186} INFO - Started process (PID=26947) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:55:34.091+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:55:34.092+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:55:34.092+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:55:40.157+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:55:40.513+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:55:40.513+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:55:41.087+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:55:41.086+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55bd09dc8a8e <unknown>
#1 0x55bd09885b0b <unknown>
#2 0x55bd098bc4ea <unknown>
#3 0x55bd098b7aef <unknown>
#4 0x55bd0990bb18 <unknown>
#5 0x55bd098f99b3 <unknown>
#6 0x55bd098c3c59 <unknown>
#7 0x55bd098c4a08 <unknown>
#8 0x55bd09d9540a <unknown>
#9 0x55bd09d9885e <unknown>
#10 0x55bd09d98308 <unknown>
#11 0x55bd09d98ce5 <unknown>
#12 0x55bd09d7eb7b <unknown>
#13 0x55bd09d99050 <unknown>
#14 0x55bd09d67ae9 <unknown>
#15 0x55bd09db7df5 <unknown>
#16 0x55bd09db7fdb <unknown>
#17 0x55bd09dc7c05 <unknown>
#18 0x7fe466f52134 <unknown>
[2025-05-07T07:55:41.088+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:55:41.105+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.512 seconds
[2025-05-07T07:56:17.018+0000] {processor.py:186} INFO - Started process (PID=27109) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:56:17.020+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:56:17.021+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:56:17.020+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:56:22.953+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:56:23.262+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:56:23.261+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:56:23.834+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:56:23.833+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ac600ffa8e <unknown>
#1 0x55ac5fbbcb0b <unknown>
#2 0x55ac5fbf34ea <unknown>
#3 0x55ac5fbeeaef <unknown>
#4 0x55ac5fc42b18 <unknown>
#5 0x55ac5fc309b3 <unknown>
#6 0x55ac5fbfac59 <unknown>
#7 0x55ac5fbfba08 <unknown>
#8 0x55ac600cc40a <unknown>
#9 0x55ac600cf85e <unknown>
#10 0x55ac600cf308 <unknown>
#11 0x55ac600cfce5 <unknown>
#12 0x55ac600b5b7b <unknown>
#13 0x55ac600d0050 <unknown>
#14 0x55ac6009eae9 <unknown>
#15 0x55ac600eedf5 <unknown>
#16 0x55ac600eefdb <unknown>
#17 0x55ac600fec05 <unknown>
#18 0x7f27b3c8a134 <unknown>
[2025-05-07T07:56:23.835+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:56:23.851+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.342 seconds
[2025-05-07T07:56:54.353+0000] {processor.py:186} INFO - Started process (PID=27241) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:56:54.354+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:56:54.356+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:56:54.355+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:06.566+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:57:00.993+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:00.992+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:57:01.574+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:01.572+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55de9ff75a8e <unknown>
#1 0x55de9fa32b0b <unknown>
#2 0x55de9fa694ea <unknown>
#3 0x55de9fa64aef <unknown>
#4 0x55de9fab8b18 <unknown>
#5 0x55de9faa69b3 <unknown>
#6 0x55de9fa70c59 <unknown>
#7 0x55de9fa71a08 <unknown>
#8 0x55de9ff4240a <unknown>
#9 0x55de9ff4585e <unknown>
#10 0x55de9ff45308 <unknown>
#11 0x55de9ff45ce5 <unknown>
#12 0x55de9ff2bb7b <unknown>
#13 0x55de9ff46050 <unknown>
#14 0x55de9ff14ae9 <unknown>
#15 0x55de9ff64df5 <unknown>
#16 0x55de9ff64fdb <unknown>
#17 0x55de9ff74c05 <unknown>
#18 0x7fbf19d70134 <unknown>
[2025-05-07T07:57:01.574+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:01.589+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.251 seconds
[2025-05-07T07:57:31.657+0000] {processor.py:186} INFO - Started process (PID=27383) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:31.658+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:57:31.658+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:31.658+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:32.020+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T07:57:32.364+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:32.364+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:57:32.938+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:32.937+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c30203aa8e <unknown>
#1 0x55c301af7b0b <unknown>
#2 0x55c301b2e4ea <unknown>
#3 0x55c301b29aef <unknown>
#4 0x55c301b7db18 <unknown>
#5 0x55c301b6b9b3 <unknown>
#6 0x55c301b35c59 <unknown>
#7 0x55c301b36a08 <unknown>
#8 0x55c30200740a <unknown>
#9 0x55c30200a85e <unknown>
#10 0x55c30200a308 <unknown>
#11 0x55c30200ace5 <unknown>
#12 0x55c301ff0b7b <unknown>
#13 0x55c30200b050 <unknown>
#14 0x55c301fd9ae9 <unknown>
#15 0x55c302029df5 <unknown>
#16 0x55c302029fdb <unknown>
#17 0x55c302039c05 <unknown>
#18 0x7fac47604134 <unknown>
[2025-05-07T07:57:32.938+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:32.952+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.581 seconds
[2025-05-07T07:57:39.660+0000] {processor.py:186} INFO - Started process (PID=27519) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:39.661+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:57:39.662+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:39.662+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:39.748+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:39.748+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:57:40.314+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:57:40.313+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d7cb8a0a8e <unknown>
#1 0x55d7cb35db0b <unknown>
#2 0x55d7cb3944ea <unknown>
#3 0x55d7cb38faef <unknown>
#4 0x55d7cb3e3b18 <unknown>
#5 0x55d7cb3d19b3 <unknown>
#6 0x55d7cb39bc59 <unknown>
#7 0x55d7cb39ca08 <unknown>
#8 0x55d7cb86d40a <unknown>
#9 0x55d7cb87085e <unknown>
#10 0x55d7cb870308 <unknown>
#11 0x55d7cb870ce5 <unknown>
#12 0x55d7cb856b7b <unknown>
#13 0x55d7cb871050 <unknown>
#14 0x55d7cb83fae9 <unknown>
#15 0x55d7cb88fdf5 <unknown>
#16 0x55d7cb88ffdb <unknown>
#17 0x55d7cb89fc05 <unknown>
#18 0x7f238d007134 <unknown>
[2025-05-07T07:57:40.314+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:57:40.328+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.673 seconds
[2025-05-07T07:58:10.389+0000] {processor.py:186} INFO - Started process (PID=27553) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:58:10.390+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:58:10.391+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:58:10.391+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:58:10.482+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:58:10.482+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:58:11.067+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:58:11.066+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55644f555a8e <unknown>
#1 0x55644f012b0b <unknown>
#2 0x55644f0494ea <unknown>
#3 0x55644f044aef <unknown>
#4 0x55644f098b18 <unknown>
#5 0x55644f0869b3 <unknown>
#6 0x55644f050c59 <unknown>
#7 0x55644f051a08 <unknown>
#8 0x55644f52240a <unknown>
#9 0x55644f52585e <unknown>
#10 0x55644f525308 <unknown>
#11 0x55644f525ce5 <unknown>
#12 0x55644f50bb7b <unknown>
#13 0x55644f526050 <unknown>
#14 0x55644f4f4ae9 <unknown>
#15 0x55644f544df5 <unknown>
#16 0x55644f544fdb <unknown>
#17 0x55644f554c05 <unknown>
#18 0x7f1e1f89f134 <unknown>
[2025-05-07T07:58:11.068+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:58:11.083+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.700 seconds
[2025-05-07T07:58:41.296+0000] {processor.py:186} INFO - Started process (PID=27587) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:58:41.296+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:58:41.297+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:58:41.297+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:58:41.397+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:58:41.397+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T07:58:41.978+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:58:41.977+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55cc4392ba8e <unknown>
#1 0x55cc433e8b0b <unknown>
#2 0x55cc4341f4ea <unknown>
#3 0x55cc4341aaef <unknown>
#4 0x55cc4346eb18 <unknown>
#5 0x55cc4345c9b3 <unknown>
#6 0x55cc43426c59 <unknown>
#7 0x55cc43427a08 <unknown>
#8 0x55cc438f840a <unknown>
#9 0x55cc438fb85e <unknown>
#10 0x55cc438fb308 <unknown>
#11 0x55cc438fbce5 <unknown>
#12 0x55cc438e1b7b <unknown>
#13 0x55cc438fc050 <unknown>
#14 0x55cc438caae9 <unknown>
#15 0x55cc4391adf5 <unknown>
#16 0x55cc4391afdb <unknown>
#17 0x55cc4392ac05 <unknown>
#18 0x7fd53d3a4134 <unknown>
[2025-05-07T07:58:41.979+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:58:41.996+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.705 seconds
[2025-05-07T07:59:12.720+0000] {processor.py:186} INFO - Started process (PID=27622) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:59:12.721+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:59:12.722+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:12.721+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:59:12.801+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:59:12.867+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:12.867+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:instagram_videos_scraper_dag
[2025-05-07T07:59:12.873+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:12.873+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:instagram_videos_scraper_dag
[2025-05-07T07:59:12.877+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:12.877+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:instagram_videos_scraper_dag
[2025-05-07T07:59:12.878+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:12.878+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T07:59:12.887+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:12.887+0000] {dag.py:3234} INFO - Creating ORM DAG for instagram_videos_scraper_dag
[2025-05-07T07:59:12.888+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:12.888+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T07:59:12.898+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.183 seconds
[2025-05-07T07:59:43.493+0000] {processor.py:186} INFO - Started process (PID=27638) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T07:59:43.515+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T07:59:43.517+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:43.516+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T07:59:43.592+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T07:59:43.605+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:43.605+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T07:59:43.614+0000] {logging_mixin.py:190} INFO - [2025-05-07T07:59:43.614+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T07:59:43.638+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.150 seconds
[2025-05-07T08:00:14.646+0000] {processor.py:186} INFO - Started process (PID=27654) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:00:14.647+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:00:14.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:00:14.647+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:00:14.735+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:00:14.751+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:00:14.750+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:00:14.759+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:00:14.759+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:00:14.774+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2025-05-07T08:00:47.031+0000] {processor.py:186} INFO - Started process (PID=27667) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:00:47.032+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:00:47.033+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:00:47.033+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:00:41.326+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:00:41.340+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:00:41.340+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:00:41.350+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:00:41.350+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:00:41.362+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2025-05-07T08:01:12.095+0000] {processor.py:186} INFO - Started process (PID=27679) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:01:12.096+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:01:12.097+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:01:12.097+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:01:06.401+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:01:06.415+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:01:06.415+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:01:06.424+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:01:06.424+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:01:06.436+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.127 seconds
[2025-05-07T08:01:41.981+0000] {processor.py:186} INFO - Started process (PID=27695) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:01:41.982+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:01:41.983+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:01:41.983+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:01:42.072+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:01:42.086+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:01:42.086+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:01:42.096+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:01:42.096+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:01:42.108+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.134 seconds
[2025-05-07T08:02:12.819+0000] {processor.py:186} INFO - Started process (PID=27722) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:02:12.821+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:02:12.822+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:02:12.821+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:02:12.900+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:02:12.914+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:02:12.914+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:02:12.922+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:02:12.922+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:02:12.934+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.120 seconds
[2025-05-07T08:02:43.015+0000] {processor.py:186} INFO - Started process (PID=27739) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:02:43.016+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:02:43.016+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:02:43.016+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:02:43.096+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:02:43.109+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:02:43.109+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:02:43.117+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:02:43.117+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:02:43.129+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2025-05-07T08:03:13.799+0000] {processor.py:186} INFO - Started process (PID=27754) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:03:13.804+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:03:13.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:03:13.804+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:03:13.885+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:03:13.899+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:03:13.898+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:03:13.907+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:03:13.907+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:03:13.921+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.127 seconds
[2025-05-07T08:03:47.310+0000] {processor.py:186} INFO - Started process (PID=27770) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:03:47.311+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:03:47.312+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:03:47.312+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:03:47.391+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:03:47.404+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:03:47.404+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:03:41.631+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:03:41.630+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:03:41.648+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.128 seconds
[2025-05-07T08:04:11.747+0000] {processor.py:186} INFO - Started process (PID=27786) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:04:11.748+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:04:11.749+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:04:11.749+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:04:11.824+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:04:11.838+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:04:11.838+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:04:11.847+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:04:11.847+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:04:11.858+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.116 seconds
[2025-05-07T08:04:41.937+0000] {processor.py:186} INFO - Started process (PID=27802) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:04:41.939+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:04:41.939+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:04:41.939+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:04:42.021+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:04:42.034+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:04:42.034+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:04:42.043+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:04:42.043+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:04:42.054+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.122 seconds
[2025-05-07T08:05:12.856+0000] {processor.py:186} INFO - Started process (PID=27818) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:05:12.858+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:05:12.859+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:05:12.859+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:05:12.973+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:05:12.990+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:05:12.990+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2025-05-07T08:05:12.999+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:05:12.999+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T08:05:13.011+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.162 seconds
[2025-05-07T08:05:47.332+0000] {processor.py:186} INFO - Started process (PID=27834) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:05:47.339+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:05:47.341+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:05:47.341+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:05:47.515+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:05:47.515+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:05:42.299+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:05:42.298+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c4584cca8e <unknown>
#1 0x55c457f89b0b <unknown>
#2 0x55c457fc04ea <unknown>
#3 0x55c457fbbaef <unknown>
#4 0x55c45800fb18 <unknown>
#5 0x55c457ffd9b3 <unknown>
#6 0x55c457fc7c59 <unknown>
#7 0x55c457fc8a08 <unknown>
#8 0x55c45849940a <unknown>
#9 0x55c45849c85e <unknown>
#10 0x55c45849c308 <unknown>
#11 0x55c45849cce5 <unknown>
#12 0x55c458482b7b <unknown>
#13 0x55c45849d050 <unknown>
#14 0x55c45846bae9 <unknown>
#15 0x55c4584bbdf5 <unknown>
#16 0x55c4584bbfdb <unknown>
#17 0x55c4584cbc05 <unknown>
#18 0x7f570b1fe134 <unknown>
[2025-05-07T08:05:42.300+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:05:42.315+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.778 seconds
[2025-05-07T08:06:13.045+0000] {processor.py:186} INFO - Started process (PID=27868) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:06:13.047+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:06:13.048+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:06:13.047+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:06:13.140+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:06:13.140+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:06:13.726+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:06:13.725+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b8d969aa8e <unknown>
#1 0x55b8d9157b0b <unknown>
#2 0x55b8d918e4ea <unknown>
#3 0x55b8d9189aef <unknown>
#4 0x55b8d91ddb18 <unknown>
#5 0x55b8d91cb9b3 <unknown>
#6 0x55b8d9195c59 <unknown>
#7 0x55b8d9196a08 <unknown>
#8 0x55b8d966740a <unknown>
#9 0x55b8d966a85e <unknown>
#10 0x55b8d966a308 <unknown>
#11 0x55b8d966ace5 <unknown>
#12 0x55b8d9650b7b <unknown>
#13 0x55b8d966b050 <unknown>
#14 0x55b8d9639ae9 <unknown>
#15 0x55b8d9689df5 <unknown>
#16 0x55b8d9689fdb <unknown>
#17 0x55b8d9699c05 <unknown>
#18 0x7f2fb321a134 <unknown>
[2025-05-07T08:06:13.727+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:06:13.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.705 seconds
[2025-05-07T08:06:47.555+0000] {processor.py:186} INFO - Started process (PID=27908) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:06:47.556+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:06:47.557+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:06:47.556+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:06:47.646+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:06:47.646+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:06:42.452+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:06:42.451+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e0316c4a8e <unknown>
#1 0x55e031181b0b <unknown>
#2 0x55e0311b84ea <unknown>
#3 0x55e0311b3aef <unknown>
#4 0x55e031207b18 <unknown>
#5 0x55e0311f59b3 <unknown>
#6 0x55e0311bfc59 <unknown>
#7 0x55e0311c0a08 <unknown>
#8 0x55e03169140a <unknown>
#9 0x55e03169485e <unknown>
#10 0x55e031694308 <unknown>
#11 0x55e031694ce5 <unknown>
#12 0x55e03167ab7b <unknown>
#13 0x55e031695050 <unknown>
#14 0x55e031663ae9 <unknown>
#15 0x55e0316b3df5 <unknown>
#16 0x55e0316b3fdb <unknown>
#17 0x55e0316c3c05 <unknown>
#18 0x7fae76821134 <unknown>
[2025-05-07T08:06:42.453+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:06:42.465+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.700 seconds
[2025-05-07T08:07:12.519+0000] {processor.py:186} INFO - Started process (PID=27936) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:07:12.520+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:07:12.521+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:07:12.521+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:07:12.633+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:07:12.632+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:07:07.430+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:07:07.428+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561d10830a8e <unknown>
#1 0x561d102edb0b <unknown>
#2 0x561d103244ea <unknown>
#3 0x561d1031faef <unknown>
#4 0x561d10373b18 <unknown>
#5 0x561d103619b3 <unknown>
#6 0x561d1032bc59 <unknown>
#7 0x561d1032ca08 <unknown>
#8 0x561d107fd40a <unknown>
#9 0x561d1080085e <unknown>
#10 0x561d10800308 <unknown>
#11 0x561d10800ce5 <unknown>
#12 0x561d107e6b7b <unknown>
#13 0x561d10801050 <unknown>
#14 0x561d107cfae9 <unknown>
#15 0x561d1081fdf5 <unknown>
#16 0x561d1081ffdb <unknown>
#17 0x561d1082fc05 <unknown>
#18 0x7f170be0b134 <unknown>
[2025-05-07T08:07:07.430+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:07:07.449+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.722 seconds
[2025-05-07T08:07:37.535+0000] {processor.py:186} INFO - Started process (PID=27968) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:07:37.547+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:07:37.548+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:07:37.548+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:07:37.637+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:07:37.637+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:07:32.437+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:07:32.436+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55773c027a8e <unknown>
#1 0x55773bae4b0b <unknown>
#2 0x55773bb1b4ea <unknown>
#3 0x55773bb16aef <unknown>
#4 0x55773bb6ab18 <unknown>
#5 0x55773bb589b3 <unknown>
#6 0x55773bb22c59 <unknown>
#7 0x55773bb23a08 <unknown>
#8 0x55773bff440a <unknown>
#9 0x55773bff785e <unknown>
#10 0x55773bff7308 <unknown>
#11 0x55773bff7ce5 <unknown>
#12 0x55773bfddb7b <unknown>
#13 0x55773bff8050 <unknown>
#14 0x55773bfc6ae9 <unknown>
#15 0x55773c016df5 <unknown>
#16 0x55773c016fdb <unknown>
#17 0x55773c026c05 <unknown>
#18 0x7f5711c60134 <unknown>
[2025-05-07T08:07:32.438+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:07:32.452+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.706 seconds
[2025-05-07T08:08:07.791+0000] {processor.py:186} INFO - Started process (PID=27999) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:08:07.795+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:08:07.797+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:08:07.796+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:08:02.103+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:08:02.103+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:08:02.675+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:08:02.674+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556bc4e14a8e <unknown>
#1 0x556bc48d1b0b <unknown>
#2 0x556bc49084ea <unknown>
#3 0x556bc4903aef <unknown>
#4 0x556bc4957b18 <unknown>
#5 0x556bc49459b3 <unknown>
#6 0x556bc490fc59 <unknown>
#7 0x556bc4910a08 <unknown>
#8 0x556bc4de140a <unknown>
#9 0x556bc4de485e <unknown>
#10 0x556bc4de4308 <unknown>
#11 0x556bc4de4ce5 <unknown>
#12 0x556bc4dcab7b <unknown>
#13 0x556bc4de5050 <unknown>
#14 0x556bc4db3ae9 <unknown>
#15 0x556bc4e03df5 <unknown>
#16 0x556bc4e03fdb <unknown>
#17 0x556bc4e13c05 <unknown>
#18 0x7f0d974ae134 <unknown>
[2025-05-07T08:08:02.676+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:08:02.691+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.691 seconds
[2025-05-07T08:08:33.283+0000] {processor.py:186} INFO - Started process (PID=28042) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:08:33.284+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:08:33.285+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:08:33.285+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:08:33.390+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:08:33.390+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:08:34.001+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:08:34.000+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5593832eba8e <unknown>
#1 0x559382da8b0b <unknown>
#2 0x559382ddf4ea <unknown>
#3 0x559382ddaaef <unknown>
#4 0x559382e2eb18 <unknown>
#5 0x559382e1c9b3 <unknown>
#6 0x559382de6c59 <unknown>
#7 0x559382de7a08 <unknown>
#8 0x5593832b840a <unknown>
#9 0x5593832bb85e <unknown>
#10 0x5593832bb308 <unknown>
#11 0x5593832bbce5 <unknown>
#12 0x5593832a1b7b <unknown>
#13 0x5593832bc050 <unknown>
#14 0x55938328aae9 <unknown>
#15 0x5593832dadf5 <unknown>
#16 0x5593832dafdb <unknown>
#17 0x5593832eac05 <unknown>
#18 0x7f43641bc134 <unknown>
[2025-05-07T08:08:34.001+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:08:34.020+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.743 seconds
[2025-05-07T08:09:07.701+0000] {processor.py:186} INFO - Started process (PID=28073) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:09:07.701+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:09:07.703+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:09:07.703+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:09:07.810+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:09:07.810+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:09:02.585+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:09:02.584+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56475b6bca8e <unknown>
#1 0x56475b179b0b <unknown>
#2 0x56475b1b04ea <unknown>
#3 0x56475b1abaef <unknown>
#4 0x56475b1ffb18 <unknown>
#5 0x56475b1ed9b3 <unknown>
#6 0x56475b1b7c59 <unknown>
#7 0x56475b1b8a08 <unknown>
#8 0x56475b68940a <unknown>
#9 0x56475b68c85e <unknown>
#10 0x56475b68c308 <unknown>
#11 0x56475b68cce5 <unknown>
#12 0x56475b672b7b <unknown>
#13 0x56475b68d050 <unknown>
#14 0x56475b65bae9 <unknown>
#15 0x56475b6abdf5 <unknown>
#16 0x56475b6abfdb <unknown>
#17 0x56475b6bbc05 <unknown>
#18 0x7fb60ef34134 <unknown>
[2025-05-07T08:09:02.586+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:09:02.600+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.695 seconds
[2025-05-07T08:09:33.337+0000] {processor.py:186} INFO - Started process (PID=28108) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:09:33.338+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:09:33.339+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:09:33.338+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:09:33.428+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:09:33.428+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:09:33.999+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:09:33.998+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556c70be6a8e <unknown>
#1 0x556c706a3b0b <unknown>
#2 0x556c706da4ea <unknown>
#3 0x556c706d5aef <unknown>
#4 0x556c70729b18 <unknown>
#5 0x556c707179b3 <unknown>
#6 0x556c706e1c59 <unknown>
#7 0x556c706e2a08 <unknown>
#8 0x556c70bb340a <unknown>
#9 0x556c70bb685e <unknown>
#10 0x556c70bb6308 <unknown>
#11 0x556c70bb6ce5 <unknown>
#12 0x556c70b9cb7b <unknown>
#13 0x556c70bb7050 <unknown>
#14 0x556c70b85ae9 <unknown>
#15 0x556c70bd5df5 <unknown>
#16 0x556c70bd5fdb <unknown>
#17 0x556c70be5c05 <unknown>
#18 0x7f5e2bcd7134 <unknown>
[2025-05-07T08:09:33.999+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:09:34.013+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.682 seconds
[2025-05-07T08:10:07.774+0000] {processor.py:186} INFO - Started process (PID=28141) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:10:07.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:10:07.776+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:10:07.776+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:10:07.868+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:10:07.868+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:10:02.657+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:10:02.655+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561fca971a8e <unknown>
#1 0x561fca42eb0b <unknown>
#2 0x561fca4654ea <unknown>
#3 0x561fca460aef <unknown>
#4 0x561fca4b4b18 <unknown>
#5 0x561fca4a29b3 <unknown>
#6 0x561fca46cc59 <unknown>
#7 0x561fca46da08 <unknown>
#8 0x561fca93e40a <unknown>
#9 0x561fca94185e <unknown>
#10 0x561fca941308 <unknown>
#11 0x561fca941ce5 <unknown>
#12 0x561fca927b7b <unknown>
#13 0x561fca942050 <unknown>
#14 0x561fca910ae9 <unknown>
#15 0x561fca960df5 <unknown>
#16 0x561fca960fdb <unknown>
#17 0x561fca970c05 <unknown>
#18 0x7f3f26ffe134 <unknown>
[2025-05-07T08:10:02.657+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:10:02.672+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.693 seconds
[2025-05-07T08:10:38.015+0000] {processor.py:186} INFO - Started process (PID=28175) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:10:38.031+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:10:38.032+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:10:38.032+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:10:32.342+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:10:32.341+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:10:32.918+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:10:32.917+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556d586b2a8e <unknown>
#1 0x556d5816fb0b <unknown>
#2 0x556d581a64ea <unknown>
#3 0x556d581a1aef <unknown>
#4 0x556d581f5b18 <unknown>
#5 0x556d581e39b3 <unknown>
#6 0x556d581adc59 <unknown>
#7 0x556d581aea08 <unknown>
#8 0x556d5867f40a <unknown>
#9 0x556d5868285e <unknown>
#10 0x556d58682308 <unknown>
#11 0x556d58682ce5 <unknown>
#12 0x556d58668b7b <unknown>
#13 0x556d58683050 <unknown>
#14 0x556d58651ae9 <unknown>
#15 0x556d586a1df5 <unknown>
#16 0x556d586a1fdb <unknown>
#17 0x556d586b1c05 <unknown>
#18 0x7f41969b1134 <unknown>
[2025-05-07T08:10:32.919+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:10:32.945+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.724 seconds
[2025-05-07T08:11:03.008+0000] {processor.py:186} INFO - Started process (PID=28212) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:11:03.010+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:11:03.011+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:11:03.011+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:11:03.110+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:11:03.110+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:11:03.675+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:11:03.674+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55bb31456a8e <unknown>
#1 0x55bb30f13b0b <unknown>
#2 0x55bb30f4a4ea <unknown>
#3 0x55bb30f45aef <unknown>
#4 0x55bb30f99b18 <unknown>
#5 0x55bb30f879b3 <unknown>
#6 0x55bb30f51c59 <unknown>
#7 0x55bb30f52a08 <unknown>
#8 0x55bb3142340a <unknown>
#9 0x55bb3142685e <unknown>
#10 0x55bb31426308 <unknown>
#11 0x55bb31426ce5 <unknown>
#12 0x55bb3140cb7b <unknown>
#13 0x55bb31427050 <unknown>
#14 0x55bb313f5ae9 <unknown>
#15 0x55bb31445df5 <unknown>
#16 0x55bb31445fdb <unknown>
#17 0x55bb31455c05 <unknown>
#18 0x7fd11f8fc134 <unknown>
[2025-05-07T08:11:03.676+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:11:03.692+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.690 seconds
[2025-05-07T08:11:32.372+0000] {processor.py:186} INFO - Started process (PID=28243) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:11:32.373+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:11:32.374+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:11:32.374+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:11:32.471+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:11:32.471+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:11:33.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:11:33.050+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5655083dea8e <unknown>
#1 0x565507e9bb0b <unknown>
#2 0x565507ed24ea <unknown>
#3 0x565507ecdaef <unknown>
#4 0x565507f21b18 <unknown>
#5 0x565507f0f9b3 <unknown>
#6 0x565507ed9c59 <unknown>
#7 0x565507edaa08 <unknown>
#8 0x5655083ab40a <unknown>
#9 0x5655083ae85e <unknown>
#10 0x5655083ae308 <unknown>
#11 0x5655083aece5 <unknown>
#12 0x565508394b7b <unknown>
#13 0x5655083af050 <unknown>
#14 0x56550837dae9 <unknown>
#15 0x5655083cddf5 <unknown>
#16 0x5655083cdfdb <unknown>
#17 0x5655083ddc05 <unknown>
#18 0x7f7d220f0134 <unknown>
[2025-05-07T08:11:33.052+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:11:33.070+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.703 seconds
[2025-05-07T08:12:08.141+0000] {processor.py:186} INFO - Started process (PID=28277) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:12:08.142+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:12:08.143+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:12:08.142+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:12:02.446+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:12:02.446+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:12:03.012+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:12:03.011+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559ccca0da8e <unknown>
#1 0x559ccc4cab0b <unknown>
#2 0x559ccc5014ea <unknown>
#3 0x559ccc4fcaef <unknown>
#4 0x559ccc550b18 <unknown>
#5 0x559ccc53e9b3 <unknown>
#6 0x559ccc508c59 <unknown>
#7 0x559ccc509a08 <unknown>
#8 0x559ccc9da40a <unknown>
#9 0x559ccc9dd85e <unknown>
#10 0x559ccc9dd308 <unknown>
#11 0x559ccc9ddce5 <unknown>
#12 0x559ccc9c3b7b <unknown>
#13 0x559ccc9de050 <unknown>
#14 0x559ccc9acae9 <unknown>
#15 0x559ccc9fcdf5 <unknown>
#16 0x559ccc9fcfdb <unknown>
#17 0x559ccca0cc05 <unknown>
#18 0x7f5bf3b31134 <unknown>
[2025-05-07T08:12:03.013+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:12:03.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.682 seconds
[2025-05-07T08:12:38.054+0000] {processor.py:186} INFO - Started process (PID=28311) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:12:38.054+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:12:38.055+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:12:38.055+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:12:38.154+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:12:38.154+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:12:32.937+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:12:32.936+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5642637daa8e <unknown>
#1 0x564263297b0b <unknown>
#2 0x5642632ce4ea <unknown>
#3 0x5642632c9aef <unknown>
#4 0x56426331db18 <unknown>
#5 0x56426330b9b3 <unknown>
#6 0x5642632d5c59 <unknown>
#7 0x5642632d6a08 <unknown>
#8 0x5642637a740a <unknown>
#9 0x5642637aa85e <unknown>
#10 0x5642637aa308 <unknown>
#11 0x5642637aace5 <unknown>
#12 0x564263790b7b <unknown>
#13 0x5642637ab050 <unknown>
#14 0x564263779ae9 <unknown>
#15 0x5642637c9df5 <unknown>
#16 0x5642637c9fdb <unknown>
#17 0x5642637d9c05 <unknown>
#18 0x7f30799d3134 <unknown>
[2025-05-07T08:12:32.938+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:12:32.952+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.696 seconds
[2025-05-07T08:13:08.325+0000] {processor.py:186} INFO - Started process (PID=28345) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:08.326+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:13:08.326+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:08.326+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:02.713+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:02.713+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:13:03.292+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:03.289+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55aa4f70ca8e <unknown>
#1 0x55aa4f1c9b0b <unknown>
#2 0x55aa4f2004ea <unknown>
#3 0x55aa4f1fbaef <unknown>
#4 0x55aa4f24fb18 <unknown>
#5 0x55aa4f23d9b3 <unknown>
#6 0x55aa4f207c59 <unknown>
#7 0x55aa4f208a08 <unknown>
#8 0x55aa4f6d940a <unknown>
#9 0x55aa4f6dc85e <unknown>
#10 0x55aa4f6dc308 <unknown>
#11 0x55aa4f6dcce5 <unknown>
#12 0x55aa4f6c2b7b <unknown>
#13 0x55aa4f6dd050 <unknown>
#14 0x55aa4f6abae9 <unknown>
#15 0x55aa4f6fbdf5 <unknown>
#16 0x55aa4f6fbfdb <unknown>
#17 0x55aa4f70bc05 <unknown>
#18 0x7f4435a3a134 <unknown>
[2025-05-07T08:13:03.292+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:03.306+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.779 seconds
[2025-05-07T08:13:38.238+0000] {processor.py:186} INFO - Started process (PID=28385) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:38.239+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:13:38.240+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:38.240+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:38.339+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:38.338+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:13:33.107+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:33.106+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561d308d3a8e <unknown>
#1 0x561d30390b0b <unknown>
#2 0x561d303c74ea <unknown>
#3 0x561d303c2aef <unknown>
#4 0x561d30416b18 <unknown>
#5 0x561d304049b3 <unknown>
#6 0x561d303cec59 <unknown>
#7 0x561d303cfa08 <unknown>
#8 0x561d308a040a <unknown>
#9 0x561d308a385e <unknown>
#10 0x561d308a3308 <unknown>
#11 0x561d308a3ce5 <unknown>
#12 0x561d30889b7b <unknown>
#13 0x561d308a4050 <unknown>
#14 0x561d30872ae9 <unknown>
#15 0x561d308c2df5 <unknown>
#16 0x561d308c2fdb <unknown>
#17 0x561d308d2c05 <unknown>
#18 0x7f9dd0360134 <unknown>
[2025-05-07T08:13:33.108+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:33.123+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.687 seconds
[2025-05-07T08:14:03.317+0000] {processor.py:186} INFO - Started process (PID=28409) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:14:03.317+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:14:03.318+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:14:03.318+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:57.631+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:57.631+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:13:58.197+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:13:58.196+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c8876c7a8e <unknown>
#1 0x55c887184b0b <unknown>
#2 0x55c8871bb4ea <unknown>
#3 0x55c8871b6aef <unknown>
#4 0x55c88720ab18 <unknown>
#5 0x55c8871f89b3 <unknown>
#6 0x55c8871c2c59 <unknown>
#7 0x55c8871c3a08 <unknown>
#8 0x55c88769440a <unknown>
#9 0x55c88769785e <unknown>
#10 0x55c887697308 <unknown>
#11 0x55c887697ce5 <unknown>
#12 0x55c88767db7b <unknown>
#13 0x55c887698050 <unknown>
#14 0x55c887666ae9 <unknown>
#15 0x55c8876b6df5 <unknown>
#16 0x55c8876b6fdb <unknown>
#17 0x55c8876c6c05 <unknown>
#18 0x7fbf203e0134 <unknown>
[2025-05-07T08:13:58.198+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:13:58.214+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.690 seconds
[2025-05-07T08:14:45.480+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:14:45.482+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:14:45.484+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:14:45.483+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:15:06.781+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:15:06.781+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:15:07.703+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:15:07.699+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ad7be6ea8e <unknown>
#1 0x55ad7b92bb0b <unknown>
#2 0x55ad7b9624ea <unknown>
#3 0x55ad7b95daef <unknown>
#4 0x55ad7b9b1b18 <unknown>
#5 0x55ad7b99f9b3 <unknown>
#6 0x55ad7b969c59 <unknown>
#7 0x55ad7b96aa08 <unknown>
#8 0x55ad7be3b40a <unknown>
#9 0x55ad7be3e85e <unknown>
#10 0x55ad7be3e308 <unknown>
#11 0x55ad7be3ece5 <unknown>
#12 0x55ad7be24b7b <unknown>
#13 0x55ad7be3f050 <unknown>
#14 0x55ad7be0dae9 <unknown>
#15 0x55ad7be5ddf5 <unknown>
#16 0x55ad7be5dfdb <unknown>
#17 0x55ad7be6dc05 <unknown>
#18 0x7f22280ec134 <unknown>
[2025-05-07T08:15:07.704+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:15:07.724+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 24.267 seconds
[2025-05-07T08:15:38.400+0000] {processor.py:186} INFO - Started process (PID=173) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:15:38.400+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:15:38.402+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:15:38.402+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:15:32.850+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:15:32.850+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:15:33.682+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:15:33.681+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5579f7cbea8e <unknown>
#1 0x5579f777bb0b <unknown>
#2 0x5579f77b24ea <unknown>
#3 0x5579f77adaef <unknown>
#4 0x5579f7801b18 <unknown>
#5 0x5579f77ef9b3 <unknown>
#6 0x5579f77b9c59 <unknown>
#7 0x5579f77baa08 <unknown>
#8 0x5579f7c8b40a <unknown>
#9 0x5579f7c8e85e <unknown>
#10 0x5579f7c8e308 <unknown>
#11 0x5579f7c8ece5 <unknown>
#12 0x5579f7c74b7b <unknown>
#13 0x5579f7c8f050 <unknown>
#14 0x5579f7c5dae9 <unknown>
#15 0x5579f7caddf5 <unknown>
#16 0x5579f7cadfdb <unknown>
#17 0x5579f7cbdc05 <unknown>
#18 0x7f24f7b48134 <unknown>
[2025-05-07T08:15:33.682+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:15:33.697+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.102 seconds
[2025-05-07T08:16:03.866+0000] {processor.py:186} INFO - Started process (PID=213) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:03.870+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:16:03.872+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:03.872+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:04.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:04.093+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:16:04.919+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:04.918+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55fe43c7ea8e <unknown>
#1 0x55fe4373bb0b <unknown>
#2 0x55fe437724ea <unknown>
#3 0x55fe4376daef <unknown>
#4 0x55fe437c1b18 <unknown>
#5 0x55fe437af9b3 <unknown>
#6 0x55fe43779c59 <unknown>
#7 0x55fe4377aa08 <unknown>
#8 0x55fe43c4b40a <unknown>
#9 0x55fe43c4e85e <unknown>
#10 0x55fe43c4e308 <unknown>
#11 0x55fe43c4ece5 <unknown>
#12 0x55fe43c34b7b <unknown>
#13 0x55fe43c4f050 <unknown>
#14 0x55fe43c1dae9 <unknown>
#15 0x55fe43c6ddf5 <unknown>
#16 0x55fe43c6dfdb <unknown>
#17 0x55fe43c7dc05 <unknown>
#18 0x7f5fa836d134 <unknown>
[2025-05-07T08:16:04.920+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:04.933+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 1.074 seconds
[2025-05-07T08:16:16.741+0000] {processor.py:186} INFO - Started process (PID=252) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:16.741+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:16:16.743+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:16.743+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:24.196+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:16:24.528+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:24.609+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:24.608+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:16:24.618+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:24.618+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:16:24.620+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:24.620+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:16:24.621+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:24.621+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:16:24.642+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.904 seconds
[2025-05-07T08:16:58.478+0000] {processor.py:186} INFO - Started process (PID=389) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:58.479+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:16:58.481+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:58.480+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:58.973+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:16:59.309+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:16:59.327+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:59.327+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:16:59.337+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:59.337+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:16:59.339+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:59.339+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:16:59.340+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:16:59.340+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:16:59.366+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.185 seconds
[2025-05-07T08:17:29.394+0000] {processor.py:186} INFO - Started process (PID=509) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:17:29.395+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:17:29.398+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:17:29.398+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:17:37.121+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:17:37.477+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:17:37.504+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:17:37.504+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:17:37.514+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:17:37.514+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:17:37.517+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:17:37.517+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:17:37.518+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:17:37.518+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:17:37.530+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.647 seconds
[2025-05-07T08:18:13.742+0000] {processor.py:186} INFO - Started process (PID=642) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:18:13.743+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:18:13.745+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:13.744+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:18:14.603+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:18:14.928+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:18:14.946+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:14.945+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:18:14.953+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:14.953+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:18:14.955+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:14.955+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:18:14.956+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:14.956+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:18:14.966+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.525 seconds
[2025-05-07T08:18:48.775+0000] {processor.py:186} INFO - Started process (PID=770) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:18:48.785+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:18:48.787+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:48.787+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:18:49.309+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:18:49.627+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:18:49.645+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:49.645+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:18:49.654+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:49.653+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:18:49.656+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:49.655+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:18:49.656+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:18:49.656+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:18:49.666+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.184 seconds
[2025-05-07T08:19:23.855+0000] {processor.py:186} INFO - Started process (PID=892) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:19:23.867+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:19:23.869+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:19:23.869+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:19:24.925+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:19:25.114+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:19:25.133+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:19:25.133+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:19:25.141+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:19:25.141+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:19:25.143+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:19:25.143+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:19:25.144+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:19:25.144+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:19:25.153+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.596 seconds
[2025-05-07T08:19:55.412+0000] {processor.py:186} INFO - Started process (PID=1021) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:19:55.414+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:19:55.416+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:19:55.415+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:20:02.473+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:20:02.763+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:20:02.784+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:02.784+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:20:02.792+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:02.792+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:20:02.795+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:02.795+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:20:02.796+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:02.796+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:20:02.807+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.904 seconds
[2025-05-07T08:20:39.083+0000] {processor.py:186} INFO - Started process (PID=1153) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:20:39.084+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:20:39.086+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:39.086+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:20:40.355+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:20:40.615+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:20:40.640+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:40.639+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:20:40.652+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:40.652+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:20:40.654+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:40.654+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:20:40.656+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:20:40.655+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:20:40.668+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.891 seconds
[2025-05-07T08:21:11.388+0000] {processor.py:186} INFO - Started process (PID=1287) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:21:11.390+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:21:11.391+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:11.391+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:21:18.216+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:21:18.563+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:21:18.588+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:18.587+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:21:18.598+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:18.597+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:21:23.894+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:23.894+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:21:23.895+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:23.895+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:21:23.907+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.732 seconds
[2025-05-07T08:21:59.188+0000] {processor.py:186} INFO - Started process (PID=1407) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:21:59.189+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:21:59.192+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:59.191+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:21:59.595+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:21:59.897+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:21:59.916+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:59.916+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:21:59.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:59.924+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:21:59.926+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:59.926+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:21:59.927+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:21:59.927+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:21:59.938+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.053 seconds
[2025-05-07T08:22:30.844+0000] {processor.py:186} INFO - Started process (PID=1531) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:22:30.846+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:22:30.847+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:22:30.847+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:22:37.460+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:22:37.774+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:22:37.795+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:22:37.794+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:22:37.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:22:37.804+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:22:37.807+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:22:37.807+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:22:37.808+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:22:37.808+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:22:37.818+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.483 seconds
[2025-05-07T08:23:09.313+0000] {processor.py:186} INFO - Started process (PID=1651) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:23:09.314+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:23:09.316+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:09.316+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:23:10.124+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:23:10.432+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:23:10.453+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:10.453+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:23:10.462+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:10.461+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:23:10.464+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:10.464+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:23:10.465+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:10.464+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:23:10.475+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.463 seconds
[2025-05-07T08:23:41.091+0000] {processor.py:186} INFO - Started process (PID=1770) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:23:41.092+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:23:41.094+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:41.093+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:23:47.648+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:23:48.015+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:23:48.039+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:48.038+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:23:48.049+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:48.049+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:23:48.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:48.051+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:23:48.052+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:23:48.052+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:23:48.064+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.483 seconds
[2025-05-07T08:24:24.226+0000] {processor.py:186} INFO - Started process (PID=1898) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:24:24.227+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:24:24.228+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:24:24.228+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:24:24.950+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:24:25.263+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:24:25.287+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:24:25.286+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:24:25.298+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:24:25.297+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:24:25.302+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:24:25.302+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:24:25.303+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:24:25.303+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:24:25.315+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.396 seconds
[2025-05-07T08:24:59.536+0000] {processor.py:186} INFO - Started process (PID=2017) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:24:59.538+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:24:59.540+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:24:59.539+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:25:00.480+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:25:00.672+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:25:00.695+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:00.695+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:25:00.705+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:00.704+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:25:00.707+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:00.707+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:25:00.708+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:00.707+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:25:00.718+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.484 seconds
[2025-05-07T08:25:28.820+0000] {processor.py:186} INFO - Started process (PID=2141) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:25:28.826+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:25:28.828+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:28.828+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:25:35.069+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:25:35.378+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:25:35.409+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:35.409+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:25:35.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:35.417+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:25:35.420+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:35.419+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:25:35.420+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:25:35.420+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:25:35.431+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.116 seconds
[2025-05-07T08:26:05.943+0000] {processor.py:186} INFO - Started process (PID=2261) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:26:05.946+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:26:05.948+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:05.948+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:26:12.105+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:26:12.418+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:26:12.440+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:12.440+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:26:12.449+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:12.449+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:26:12.451+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:12.451+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:26:12.452+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:12.452+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:26:12.464+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.032 seconds
[2025-05-07T08:26:43.065+0000] {processor.py:186} INFO - Started process (PID=2391) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:26:43.066+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:26:43.067+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:43.067+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:26:49.121+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:26:49.347+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:26:49.375+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:49.375+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:26:49.388+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:49.388+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:26:49.391+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:49.391+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:26:49.392+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:26:49.392+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:26:49.404+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.334 seconds
[2025-05-07T08:27:24.633+0000] {processor.py:186} INFO - Started process (PID=2524) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:27:24.634+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:27:24.637+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:27:24.636+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:27:25.168+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:27:25.471+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:27:25.496+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:27:25.495+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:27:25.506+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:27:25.505+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:27:25.508+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:27:25.508+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:27:25.509+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:27:25.509+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:27:25.521+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.192 seconds
[2025-05-07T08:27:56.242+0000] {processor.py:186} INFO - Started process (PID=2654) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:27:56.246+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:27:56.248+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:27:56.248+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:28:02.987+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:28:03.308+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:28:03.331+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:03.330+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:28:03.340+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:03.339+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:28:03.342+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:03.342+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:28:03.343+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:03.343+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:28:03.354+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.614 seconds
[2025-05-07T08:28:33.595+0000] {processor.py:186} INFO - Started process (PID=2784) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:28:33.599+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:28:33.600+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:33.600+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:28:39.721+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:28:40.043+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:28:40.068+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:40.067+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:28:40.078+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:40.078+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:28:40.081+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:40.081+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:28:40.082+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:28:40.082+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:28:40.094+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.500 seconds
[2025-05-07T08:29:10.484+0000] {processor.py:186} INFO - Started process (PID=2905) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:29:10.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:29:10.504+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:10.504+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:29:17.212+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:29:17.437+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:29:17.463+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:17.463+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:29:17.473+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:17.473+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:29:17.475+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:17.475+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:29:17.476+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:17.476+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:29:17.503+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.520 seconds
[2025-05-07T08:29:48.490+0000] {processor.py:186} INFO - Started process (PID=3030) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:29:48.498+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:29:48.500+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:48.500+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:29:54.662+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:29:55.010+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:29:55.032+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:55.032+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:29:55.043+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:55.043+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:29:55.045+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:55.045+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:29:55.046+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:29:55.046+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:29:55.058+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.574 seconds
[2025-05-07T08:30:25.970+0000] {processor.py:186} INFO - Started process (PID=3155) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:30:25.974+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:30:25.976+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:30:25.976+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:30:31.954+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:30:32.240+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:30:32.265+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:30:32.264+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:30:32.274+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:30:32.273+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:30:32.276+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:30:32.276+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:30:32.277+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:30:32.277+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:30:32.289+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.825 seconds
[2025-05-07T08:31:02.955+0000] {processor.py:186} INFO - Started process (PID=3278) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:31:02.958+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:31:02.960+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:02.960+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:31:09.519+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:31:15.185+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:31:09.409+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:09.409+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:31:09.420+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:09.420+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:31:09.423+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:09.423+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:31:09.424+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:09.424+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:31:09.438+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.489 seconds
[2025-05-07T08:31:39.624+0000] {processor.py:186} INFO - Started process (PID=3405) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:31:39.625+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:31:39.627+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:39.627+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:31:45.451+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:31:45.643+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:31:45.670+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:45.669+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:31:45.682+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:45.682+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:31:45.685+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:45.685+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:31:45.686+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:31:45.686+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:31:45.717+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.097 seconds
[2025-05-07T08:32:20.151+0000] {processor.py:186} INFO - Started process (PID=3531) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:32:20.152+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:32:20.154+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:20.153+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:32:20.491+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:32:20.789+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:32:20.811+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:20.810+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:32:20.820+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:20.820+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:32:20.822+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:20.822+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:32:20.823+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:20.823+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:32:20.834+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.984 seconds
[2025-05-07T08:32:38.139+0000] {processor.py:186} INFO - Started process (PID=3659) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:32:38.139+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:32:38.141+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:38.141+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:32:44.245+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:32:44.586+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:32:44.621+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:44.620+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:32:44.640+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:44.640+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:32:44.646+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:44.646+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:32:44.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:32:44.648+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:32:44.676+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.046 seconds
[2025-05-07T08:33:15.153+0000] {processor.py:186} INFO - Started process (PID=3780) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:33:15.153+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:33:15.155+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:15.155+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:33:15.513+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:33:15.853+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:33:15.876+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:15.875+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:33:15.887+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:15.886+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:33:15.889+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:15.889+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:33:15.890+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:15.890+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:33:15.900+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.056 seconds
[2025-05-07T08:33:46.819+0000] {processor.py:186} INFO - Started process (PID=3900) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:33:46.823+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:33:46.824+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:46.824+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:33:53.617+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:33:53.838+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:33:53.865+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:53.865+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:33:53.875+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:53.875+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:33:53.878+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:53.877+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:33:53.878+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:33:53.878+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:33:53.890+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.568 seconds
[2025-05-07T08:34:24.916+0000] {processor.py:186} INFO - Started process (PID=4034) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:34:24.920+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:34:24.922+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:34:24.922+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:34:31.517+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:34:31.832+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:34:31.853+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:34:31.853+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:34:31.862+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:34:31.862+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:34:31.864+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:34:31.864+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:34:31.865+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:34:31.865+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:34:31.874+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.969 seconds
[2025-05-07T08:35:05.380+0000] {processor.py:186} INFO - Started process (PID=4153) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:35:05.381+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:35:05.382+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:05.382+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:35:05.910+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:35:06.204+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:35:06.223+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:06.223+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:35:06.233+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:06.233+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:35:06.235+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:06.235+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:35:06.236+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:06.235+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:35:06.246+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.172 seconds
[2025-05-07T08:35:37.027+0000] {processor.py:186} INFO - Started process (PID=4278) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:35:37.029+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:35:37.031+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:37.031+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:35:43.296+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:35:43.410+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:35:43.432+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:43.432+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:35:43.440+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:43.440+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:35:43.443+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:43.443+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:35:43.444+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:35:43.443+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:35:43.453+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.926 seconds
[2025-05-07T08:36:13.554+0000] {processor.py:186} INFO - Started process (PID=4399) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:36:13.555+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:36:13.557+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:13.556+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:36:20.203+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:36:20.515+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:36:20.537+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:20.537+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:36:20.547+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:20.547+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:36:20.549+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:20.549+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:36:20.550+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:20.550+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:36:20.561+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.009 seconds
[2025-05-07T08:36:51.439+0000] {processor.py:186} INFO - Started process (PID=4523) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:36:51.448+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:36:51.449+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:51.449+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:36:57.744+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:36:58.097+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:36:58.120+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:58.120+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:36:58.130+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:58.129+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:36:58.133+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:58.132+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:36:58.134+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:36:58.134+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:36:58.160+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.227 seconds
[2025-05-07T08:37:28.669+0000] {processor.py:186} INFO - Started process (PID=4652) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:37:28.670+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:37:28.672+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:37:28.672+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:37:40.645+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:37:35.216+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:37:35.239+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:37:35.239+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:37:35.248+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:37:35.248+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:37:35.250+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:37:35.250+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:37:35.251+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:37:35.251+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:37:35.261+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.600 seconds
[2025-05-07T08:38:05.791+0000] {processor.py:186} INFO - Started process (PID=4771) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:38:05.792+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:38:05.794+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:05.793+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:38:06.761+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:38:06.956+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:38:06.981+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:06.980+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:38:06.990+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:06.990+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:38:06.992+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:06.992+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:38:06.993+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:06.993+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:38:07.004+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.518 seconds
[2025-05-07T08:38:37.490+0000] {processor.py:186} INFO - Started process (PID=4898) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:38:37.493+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:38:37.495+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:37.495+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:38:43.596+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:38:43.908+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:38:43.931+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:43.931+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:38:43.941+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:43.941+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:38:43.943+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:43.943+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:38:43.944+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:38:43.944+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:38:43.956+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.964 seconds
[2025-05-07T08:39:14.694+0000] {processor.py:186} INFO - Started process (PID=5018) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:39:14.695+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:39:14.697+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:14.697+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:39:20.261+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:39:20.562+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:39:20.585+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:20.584+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:39:20.596+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:20.596+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:39:20.598+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:20.598+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:39:20.599+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:20.599+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:39:20.610+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.927 seconds
[2025-05-07T08:39:51.426+0000] {processor.py:186} INFO - Started process (PID=5154) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:39:51.427+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:39:51.429+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:51.429+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:39:58.190+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:39:58.484+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:39:58.507+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:58.506+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:39:58.516+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:58.516+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:39:58.519+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:58.519+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:39:58.519+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:39:58.519+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:39:58.530+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.612 seconds
[2025-05-07T08:40:28.714+0000] {processor.py:186} INFO - Started process (PID=5279) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:40:28.715+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:40:28.717+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:40:28.717+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:40:35.524+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:40:40.976+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:40:40.998+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:40:40.997+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:40:41.008+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:40:41.007+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:40:41.010+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:40:41.010+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:40:41.011+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:40:41.011+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:40:41.022+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.515 seconds
[2025-05-07T08:41:11.125+0000] {processor.py:186} INFO - Started process (PID=5414) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:41:11.126+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:41:11.127+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:11.127+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:41:12.623+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:41:12.966+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:41:12.990+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:12.989+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:41:13.000+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:13.000+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:41:13.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:13.003+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:41:13.004+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:13.004+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:41:13.016+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.204 seconds
[2025-05-07T08:41:46.268+0000] {processor.py:186} INFO - Started process (PID=5539) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:41:46.269+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:41:46.271+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:46.271+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:41:47.029+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:41:47.377+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:41:47.401+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:47.401+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:41:47.411+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:47.411+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:41:47.414+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:47.414+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:41:47.415+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:41:47.415+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:41:47.426+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.472 seconds
[2025-05-07T08:42:18.223+0000] {processor.py:186} INFO - Started process (PID=5666) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:42:18.227+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:42:18.229+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:42:18.228+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:42:24.841+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:42:25.178+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:42:25.200+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:42:25.200+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:42:25.210+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:42:25.210+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:42:25.213+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:42:25.212+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:42:25.213+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:42:25.213+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:42:25.224+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.556 seconds
[2025-05-07T08:43:01.213+0000] {processor.py:186} INFO - Started process (PID=5791) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:43:01.214+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:43:01.215+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:01.215+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:43:01.859+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:43:02.089+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:43:02.110+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:02.110+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:43:02.120+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:02.120+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:43:02.123+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:02.123+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:43:02.124+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:02.124+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:43:02.134+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.230 seconds
[2025-05-07T08:43:32.753+0000] {processor.py:186} INFO - Started process (PID=5920) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:43:32.754+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:43:32.756+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:32.756+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:43:39.341+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:43:39.652+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:43:39.674+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:39.674+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:43:39.684+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:39.684+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:43:39.686+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:39.686+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:43:39.687+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:43:39.687+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:43:39.698+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.458 seconds
[2025-05-07T08:44:10.485+0000] {processor.py:186} INFO - Started process (PID=6039) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:44:10.489+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:44:10.491+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:10.490+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:44:16.663+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:44:16.992+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:44:17.016+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:17.015+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:44:17.025+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:17.025+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:44:17.028+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:17.028+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:44:17.029+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:17.029+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:44:17.041+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.566 seconds
[2025-05-07T08:44:51.469+0000] {processor.py:186} INFO - Started process (PID=6161) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:44:51.470+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:44:51.472+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:51.471+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:44:52.338+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:44:52.705+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:44:52.729+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:52.728+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:44:52.739+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:52.739+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:44:52.742+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:52.742+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:44:52.743+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:52.743+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:44:52.754+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.601 seconds
[2025-05-07T08:44:56.305+0000] {processor.py:186} INFO - Started process (PID=6283) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:44:56.306+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:44:56.307+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:44:56.307+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:02.278+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:45:02.617+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:02.639+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:02.639+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:45:02.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:02.648+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:45:02.652+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:02.652+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:45:02.653+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:02.653+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:45:02.664+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.869 seconds
[2025-05-07T08:45:33.225+0000] {processor.py:186} INFO - Started process (PID=6407) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:33.227+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:45:33.229+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:33.228+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:39.519+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:45:39.842+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:39.864+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:39.864+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T08:45:39.875+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:39.875+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T08:45:39.877+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:39.877+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T08:45:39.878+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:39.878+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T08:45:39.889+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.168 seconds
[2025-05-07T08:45:40.801+0000] {processor.py:186} INFO - Started process (PID=6519) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:40.802+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:45:40.803+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:40.803+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:46.633+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:45:47.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:45:47.002+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 5, in fb_videos_scraper
    from utils.account_videos import AccountVideo
  File "/opt/airflow/dags/utils/account_videos.py", line 5, in <module>
    from .facebook_base import BaseFacebookScraper
  File "/opt/airflow/dags/utils/facebook_base.py", line 11, in <module>
    from ...logs import Logs
ImportError: attempted relative import beyond top-level package
[2025-05-07T08:45:47.003+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:45:47.021+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.227 seconds
[2025-05-07T08:46:21.630+0000] {processor.py:186} INFO - Started process (PID=6648) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:46:21.632+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:46:21.634+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:46:21.634+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:46:22.740+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:46:23.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:46:23.003+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:46:23.905+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:46:23.902+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556e89a88a8e <unknown>
#1 0x556e89545b0b <unknown>
#2 0x556e8957c4ea <unknown>
#3 0x556e89577aef <unknown>
#4 0x556e895cbb18 <unknown>
#5 0x556e895b99b3 <unknown>
#6 0x556e89583c59 <unknown>
#7 0x556e89584a08 <unknown>
#8 0x556e89a5540a <unknown>
#9 0x556e89a5885e <unknown>
#10 0x556e89a58308 <unknown>
#11 0x556e89a58ce5 <unknown>
#12 0x556e89a3eb7b <unknown>
#13 0x556e89a59050 <unknown>
#14 0x556e89a27ae9 <unknown>
#15 0x556e89a77df5 <unknown>
#16 0x556e89a77fdb <unknown>
#17 0x556e89a87c05 <unknown>
#18 0x7f390ee82134 <unknown>
[2025-05-07T08:46:23.906+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:46:23.924+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.607 seconds
[2025-05-07T08:46:56.883+0000] {processor.py:186} INFO - Started process (PID=6799) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:46:56.884+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:46:56.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:46:56.885+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:46:57.755+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:46:58.135+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:46:58.135+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:46:59.018+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:46:59.017+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f6200bea8e <unknown>
#1 0x55f61fb7bb0b <unknown>
#2 0x55f61fbb24ea <unknown>
#3 0x55f61fbadaef <unknown>
#4 0x55f61fc01b18 <unknown>
#5 0x55f61fbef9b3 <unknown>
#6 0x55f61fbb9c59 <unknown>
#7 0x55f61fbbaa08 <unknown>
#8 0x55f62008b40a <unknown>
#9 0x55f62008e85e <unknown>
#10 0x55f62008e308 <unknown>
#11 0x55f62008ece5 <unknown>
#12 0x55f620074b7b <unknown>
#13 0x55f62008f050 <unknown>
#14 0x55f62005dae9 <unknown>
#15 0x55f6200addf5 <unknown>
#16 0x55f6200adfdb <unknown>
#17 0x55f6200bdc05 <unknown>
#18 0x7fcf32f1c134 <unknown>
[2025-05-07T08:46:59.019+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:46:59.034+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.459 seconds
[2025-05-07T08:47:29.766+0000] {processor.py:186} INFO - Started process (PID=6957) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:47:29.770+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:47:29.771+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:47:29.771+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:47:41.823+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:47:36.348+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:47:36.348+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:47:37.221+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:47:37.219+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5600cb388a8e <unknown>
#1 0x5600cae45b0b <unknown>
#2 0x5600cae7c4ea <unknown>
#3 0x5600cae77aef <unknown>
#4 0x5600caecbb18 <unknown>
#5 0x5600caeb99b3 <unknown>
#6 0x5600cae83c59 <unknown>
#7 0x5600cae84a08 <unknown>
#8 0x5600cb35540a <unknown>
#9 0x5600cb35885e <unknown>
#10 0x5600cb358308 <unknown>
#11 0x5600cb358ce5 <unknown>
#12 0x5600cb33eb7b <unknown>
#13 0x5600cb359050 <unknown>
#14 0x5600cb327ae9 <unknown>
#15 0x5600cb377df5 <unknown>
#16 0x5600cb377fdb <unknown>
#17 0x5600cb387c05 <unknown>
#18 0x7fc35860a134 <unknown>
[2025-05-07T08:47:37.222+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:47:37.236+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.479 seconds
[2025-05-07T08:48:11.774+0000] {processor.py:186} INFO - Started process (PID=7109) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:48:11.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:48:11.776+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:48:11.776+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:48:12.807+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:48:13.146+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:48:13.146+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:48:14.037+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:48:14.036+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559359e98a8e <unknown>
#1 0x559359955b0b <unknown>
#2 0x55935998c4ea <unknown>
#3 0x559359987aef <unknown>
#4 0x5593599dbb18 <unknown>
#5 0x5593599c99b3 <unknown>
#6 0x559359993c59 <unknown>
#7 0x559359994a08 <unknown>
#8 0x559359e6540a <unknown>
#9 0x559359e6885e <unknown>
#10 0x559359e68308 <unknown>
#11 0x559359e68ce5 <unknown>
#12 0x559359e4eb7b <unknown>
#13 0x559359e69050 <unknown>
#14 0x559359e37ae9 <unknown>
#15 0x559359e87df5 <unknown>
#16 0x559359e87fdb <unknown>
#17 0x559359e97c05 <unknown>
#18 0x7f58e7e25134 <unknown>
[2025-05-07T08:48:14.037+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:48:14.053+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.599 seconds
[2025-05-07T08:48:44.458+0000] {processor.py:186} INFO - Started process (PID=7258) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:48:44.459+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:48:44.460+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:48:44.459+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:48:51.108+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:48:51.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:48:51.418+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:48:51.699+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:48:51.698+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x558b0b5f8a8e <unknown>
#1 0x558b0b0b5b0b <unknown>
#2 0x558b0b0ec4ea <unknown>
#3 0x558b0b0e7aef <unknown>
#4 0x558b0b13bb18 <unknown>
#5 0x558b0b1299b3 <unknown>
#6 0x558b0b0f3c59 <unknown>
#7 0x558b0b0f4a08 <unknown>
#8 0x558b0b5c540a <unknown>
#9 0x558b0b5c885e <unknown>
#10 0x558b0b5c8308 <unknown>
#11 0x558b0b5c8ce5 <unknown>
#12 0x558b0b5aeb7b <unknown>
#13 0x558b0b5c9050 <unknown>
#14 0x558b0b597ae9 <unknown>
#15 0x558b0b5e7df5 <unknown>
#16 0x558b0b5e7fdb <unknown>
#17 0x558b0b5f7c05 <unknown>
#18 0x7f6dd952e134 <unknown>
[2025-05-07T08:48:51.699+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:48:51.713+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.262 seconds
[2025-05-07T08:49:22.482+0000] {processor.py:186} INFO - Started process (PID=7402) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:49:22.485+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:49:22.486+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:49:22.486+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:49:29.127+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:49:29.438+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:49:29.437+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:49:30.337+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:49:30.336+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55896ca44a8e <unknown>
#1 0x55896c501b0b <unknown>
#2 0x55896c5384ea <unknown>
#3 0x55896c533aef <unknown>
#4 0x55896c587b18 <unknown>
#5 0x55896c5759b3 <unknown>
#6 0x55896c53fc59 <unknown>
#7 0x55896c540a08 <unknown>
#8 0x55896ca1140a <unknown>
#9 0x55896ca1485e <unknown>
#10 0x55896ca14308 <unknown>
#11 0x55896ca14ce5 <unknown>
#12 0x55896c9fab7b <unknown>
#13 0x55896ca15050 <unknown>
#14 0x55896c9e3ae9 <unknown>
#15 0x55896ca33df5 <unknown>
#16 0x55896ca33fdb <unknown>
#17 0x55896ca43c05 <unknown>
#18 0x7f76ce8a9134 <unknown>
[2025-05-07T08:49:30.337+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:49:30.351+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.380 seconds
[2025-05-07T08:50:00.807+0000] {processor.py:186} INFO - Started process (PID=7551) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:50:00.811+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:50:00.812+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:50:00.812+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:50:07.348+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:50:07.554+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:50:07.554+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:50:08.436+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:50:08.435+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a387b3fa8e <unknown>
#1 0x55a3875fcb0b <unknown>
#2 0x55a3876334ea <unknown>
#3 0x55a38762eaef <unknown>
#4 0x55a387682b18 <unknown>
#5 0x55a3876709b3 <unknown>
#6 0x55a38763ac59 <unknown>
#7 0x55a38763ba08 <unknown>
#8 0x55a387b0c40a <unknown>
#9 0x55a387b0f85e <unknown>
#10 0x55a387b0f308 <unknown>
#11 0x55a387b0fce5 <unknown>
#12 0x55a387af5b7b <unknown>
#13 0x55a387b10050 <unknown>
#14 0x55a387adeae9 <unknown>
#15 0x55a387b2edf5 <unknown>
#16 0x55a387b2efdb <unknown>
#17 0x55a387b3ec05 <unknown>
#18 0x7f26667c5134 <unknown>
[2025-05-07T08:50:08.437+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:50:08.458+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.650 seconds
[2025-05-07T08:50:42.095+0000] {processor.py:186} INFO - Started process (PID=7696) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:50:42.096+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:50:42.097+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:50:42.096+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:50:42.494+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:50:42.830+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:50:42.830+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:50:43.775+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:50:43.774+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559feea45a8e <unknown>
#1 0x559fee502b0b <unknown>
#2 0x559fee5394ea <unknown>
#3 0x559fee534aef <unknown>
#4 0x559fee588b18 <unknown>
#5 0x559fee5769b3 <unknown>
#6 0x559fee540c59 <unknown>
#7 0x559fee541a08 <unknown>
#8 0x559feea1240a <unknown>
#9 0x559feea1585e <unknown>
#10 0x559feea15308 <unknown>
#11 0x559feea15ce5 <unknown>
#12 0x559fee9fbb7b <unknown>
#13 0x559feea16050 <unknown>
#14 0x559fee9e4ae9 <unknown>
#15 0x559feea34df5 <unknown>
#16 0x559feea34fdb <unknown>
#17 0x559feea44c05 <unknown>
#18 0x7f41b62fa134 <unknown>
[2025-05-07T08:50:43.775+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:50:43.793+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.021 seconds
[2025-05-07T08:51:14.767+0000] {processor.py:186} INFO - Started process (PID=7848) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:51:14.771+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:51:14.773+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:51:14.772+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:51:21.060+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:51:21.265+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:51:21.264+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:51:21.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:51:21.647+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x558850646a8e <unknown>
#1 0x558850103b0b <unknown>
#2 0x55885013a4ea <unknown>
#3 0x558850135aef <unknown>
#4 0x558850189b18 <unknown>
#5 0x5588501779b3 <unknown>
#6 0x558850141c59 <unknown>
#7 0x558850142a08 <unknown>
#8 0x55885061340a <unknown>
#9 0x55885061685e <unknown>
#10 0x558850616308 <unknown>
#11 0x558850616ce5 <unknown>
#12 0x5588505fcb7b <unknown>
#13 0x558850617050 <unknown>
#14 0x5588505e5ae9 <unknown>
#15 0x558850635df5 <unknown>
#16 0x558850635fdb <unknown>
#17 0x558850645c05 <unknown>
#18 0x7fe798c26134 <unknown>
[2025-05-07T08:51:21.649+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:51:21.661+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.897 seconds
[2025-05-07T08:51:52.737+0000] {processor.py:186} INFO - Started process (PID=8001) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:51:52.740+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:51:52.741+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:51:52.741+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:51:58.911+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:51:59.256+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:51:59.256+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:52:00.150+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:52:00.149+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a43d93ca8e <unknown>
#1 0x55a43d3f9b0b <unknown>
#2 0x55a43d4304ea <unknown>
#3 0x55a43d42baef <unknown>
#4 0x55a43d47fb18 <unknown>
#5 0x55a43d46d9b3 <unknown>
#6 0x55a43d437c59 <unknown>
#7 0x55a43d438a08 <unknown>
#8 0x55a43d90940a <unknown>
#9 0x55a43d90c85e <unknown>
#10 0x55a43d90c308 <unknown>
#11 0x55a43d90cce5 <unknown>
#12 0x55a43d8f2b7b <unknown>
#13 0x55a43d90d050 <unknown>
#14 0x55a43d8dbae9 <unknown>
#15 0x55a43d92bdf5 <unknown>
#16 0x55a43d92bfdb <unknown>
#17 0x55a43d93bc05 <unknown>
#18 0x7ff693a07134 <unknown>
[2025-05-07T08:52:00.150+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:52:00.165+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.946 seconds
[2025-05-07T08:52:32.257+0000] {processor.py:186} INFO - Started process (PID=8145) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:52:32.257+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:52:32.258+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:52:32.258+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:52:32.676+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:52:32.992+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:52:32.991+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:52:33.821+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:52:33.820+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55bb200d5a8e <unknown>
#1 0x55bb1fb92b0b <unknown>
#2 0x55bb1fbc94ea <unknown>
#3 0x55bb1fbc4aef <unknown>
#4 0x55bb1fc18b18 <unknown>
#5 0x55bb1fc069b3 <unknown>
#6 0x55bb1fbd0c59 <unknown>
#7 0x55bb1fbd1a08 <unknown>
#8 0x55bb200a240a <unknown>
#9 0x55bb200a585e <unknown>
#10 0x55bb200a5308 <unknown>
#11 0x55bb200a5ce5 <unknown>
#12 0x55bb2008bb7b <unknown>
#13 0x55bb200a6050 <unknown>
#14 0x55bb20074ae9 <unknown>
#15 0x55bb200c4df5 <unknown>
#16 0x55bb200c4fdb <unknown>
#17 0x55bb200d4c05 <unknown>
#18 0x7feb360f9134 <unknown>
[2025-05-07T08:52:33.822+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:52:33.965+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.026 seconds
[2025-05-07T08:53:07.443+0000] {processor.py:186} INFO - Started process (PID=8293) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:53:07.444+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:53:07.445+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:53:07.445+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:53:08.218+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:53:08.536+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:53:08.536+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:53:09.435+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:53:09.434+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x560f7e677a8e <unknown>
#1 0x560f7e134b0b <unknown>
#2 0x560f7e16b4ea <unknown>
#3 0x560f7e166aef <unknown>
#4 0x560f7e1bab18 <unknown>
#5 0x560f7e1a89b3 <unknown>
#6 0x560f7e172c59 <unknown>
#7 0x560f7e173a08 <unknown>
#8 0x560f7e64440a <unknown>
#9 0x560f7e64785e <unknown>
#10 0x560f7e647308 <unknown>
#11 0x560f7e647ce5 <unknown>
#12 0x560f7e62db7b <unknown>
#13 0x560f7e648050 <unknown>
#14 0x560f7e616ae9 <unknown>
#15 0x560f7e666df5 <unknown>
#16 0x560f7e666fdb <unknown>
#17 0x560f7e676c05 <unknown>
#18 0x7f28a4dd6134 <unknown>
[2025-05-07T08:53:09.435+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:53:09.452+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.330 seconds
[2025-05-07T08:53:40.246+0000] {processor.py:186} INFO - Started process (PID=8444) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:53:40.248+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:53:40.249+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:53:40.248+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:53:46.907+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:53:46.775+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:53:46.774+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:53:47.693+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:53:47.692+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559d0c64fa8e <unknown>
#1 0x559d0c10cb0b <unknown>
#2 0x559d0c1434ea <unknown>
#3 0x559d0c13eaef <unknown>
#4 0x559d0c192b18 <unknown>
#5 0x559d0c1809b3 <unknown>
#6 0x559d0c14ac59 <unknown>
#7 0x559d0c14ba08 <unknown>
#8 0x559d0c61c40a <unknown>
#9 0x559d0c61f85e <unknown>
#10 0x559d0c61f308 <unknown>
#11 0x559d0c61fce5 <unknown>
#12 0x559d0c605b7b <unknown>
#13 0x559d0c620050 <unknown>
#14 0x559d0c5eeae9 <unknown>
#15 0x559d0c63edf5 <unknown>
#16 0x559d0c63efdb <unknown>
#17 0x559d0c64ec05 <unknown>
#18 0x7ff1fedba134 <unknown>
[2025-05-07T08:53:47.694+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:53:47.708+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.465 seconds
[2025-05-07T08:54:18.116+0000] {processor.py:186} INFO - Started process (PID=8598) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:54:18.118+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:54:18.118+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:54:18.118+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:54:29.386+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:54:29.720+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:54:29.720+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:54:30.659+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:54:30.658+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5617766c4a8e <unknown>
#1 0x561776181b0b <unknown>
#2 0x5617761b84ea <unknown>
#3 0x5617761b3aef <unknown>
#4 0x561776207b18 <unknown>
#5 0x5617761f59b3 <unknown>
#6 0x5617761bfc59 <unknown>
#7 0x5617761c0a08 <unknown>
#8 0x56177669140a <unknown>
#9 0x56177669485e <unknown>
#10 0x561776694308 <unknown>
#11 0x561776694ce5 <unknown>
#12 0x56177667ab7b <unknown>
#13 0x561776695050 <unknown>
#14 0x561776663ae9 <unknown>
#15 0x5617766b3df5 <unknown>
#16 0x5617766b3fdb <unknown>
#17 0x5617766c3c05 <unknown>
#18 0x7fbdd8505134 <unknown>
[2025-05-07T08:54:30.660+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:54:30.675+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 13.554 seconds
[2025-05-07T08:55:01.181+0000] {processor.py:186} INFO - Started process (PID=8749) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:55:01.192+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:55:01.192+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:55:01.192+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:55:08.017+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:55:08.360+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:55:08.360+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:55:09.292+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:55:09.291+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a9a70fda8e <unknown>
#1 0x55a9a6bbab0b <unknown>
#2 0x55a9a6bf14ea <unknown>
#3 0x55a9a6becaef <unknown>
#4 0x55a9a6c40b18 <unknown>
#5 0x55a9a6c2e9b3 <unknown>
#6 0x55a9a6bf8c59 <unknown>
#7 0x55a9a6bf9a08 <unknown>
#8 0x55a9a70ca40a <unknown>
#9 0x55a9a70cd85e <unknown>
#10 0x55a9a70cd308 <unknown>
#11 0x55a9a70cdce5 <unknown>
#12 0x55a9a70b3b7b <unknown>
#13 0x55a9a70ce050 <unknown>
#14 0x55a9a709cae9 <unknown>
#15 0x55a9a70ecdf5 <unknown>
#16 0x55a9a70ecfdb <unknown>
#17 0x55a9a70fcc05 <unknown>
#18 0x7fda6d4f2134 <unknown>
[2025-05-07T08:55:09.293+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:55:09.321+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.152 seconds
[2025-05-07T08:55:42.643+0000] {processor.py:186} INFO - Started process (PID=8900) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:55:42.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:55:42.645+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:55:42.645+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:55:43.550+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:55:43.857+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:55:43.856+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:55:44.758+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:55:44.757+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d5f161ca8e <unknown>
#1 0x55d5f10d9b0b <unknown>
#2 0x55d5f11104ea <unknown>
#3 0x55d5f110baef <unknown>
#4 0x55d5f115fb18 <unknown>
#5 0x55d5f114d9b3 <unknown>
#6 0x55d5f1117c59 <unknown>
#7 0x55d5f1118a08 <unknown>
#8 0x55d5f15e940a <unknown>
#9 0x55d5f15ec85e <unknown>
#10 0x55d5f15ec308 <unknown>
#11 0x55d5f15ecce5 <unknown>
#12 0x55d5f15d2b7b <unknown>
#13 0x55d5f15ed050 <unknown>
#14 0x55d5f15bbae9 <unknown>
#15 0x55d5f160bdf5 <unknown>
#16 0x55d5f160bfdb <unknown>
#17 0x55d5f161bc05 <unknown>
#18 0x7f288527a134 <unknown>
[2025-05-07T08:55:44.758+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:55:44.783+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.464 seconds
[2025-05-07T08:56:15.208+0000] {processor.py:186} INFO - Started process (PID=9051) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:56:15.212+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:56:15.213+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:56:15.212+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:56:21.930+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:56:27.594+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:56:27.594+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:56:22.716+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:56:22.714+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e1c1977a8e <unknown>
#1 0x55e1c1434b0b <unknown>
#2 0x55e1c146b4ea <unknown>
#3 0x55e1c1466aef <unknown>
#4 0x55e1c14bab18 <unknown>
#5 0x55e1c14a89b3 <unknown>
#6 0x55e1c1472c59 <unknown>
#7 0x55e1c1473a08 <unknown>
#8 0x55e1c194440a <unknown>
#9 0x55e1c194785e <unknown>
#10 0x55e1c1947308 <unknown>
#11 0x55e1c1947ce5 <unknown>
#12 0x55e1c192db7b <unknown>
#13 0x55e1c1948050 <unknown>
#14 0x55e1c1916ae9 <unknown>
#15 0x55e1c1966df5 <unknown>
#16 0x55e1c1966fdb <unknown>
#17 0x55e1c1976c05 <unknown>
#18 0x7fc6c5c63134 <unknown>
[2025-05-07T08:56:22.717+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:56:22.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.519 seconds
[2025-05-07T08:56:52.037+0000] {processor.py:186} INFO - Started process (PID=9198) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:56:52.038+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:56:52.039+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:56:52.039+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:56:58.692+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:56:59.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:56:59.092+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:57:00.074+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:57:00.073+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a3e9acda8e <unknown>
#1 0x55a3e958ab0b <unknown>
#2 0x55a3e95c14ea <unknown>
#3 0x55a3e95bcaef <unknown>
#4 0x55a3e9610b18 <unknown>
#5 0x55a3e95fe9b3 <unknown>
#6 0x55a3e95c8c59 <unknown>
#7 0x55a3e95c9a08 <unknown>
#8 0x55a3e9a9a40a <unknown>
#9 0x55a3e9a9d85e <unknown>
#10 0x55a3e9a9d308 <unknown>
#11 0x55a3e9a9dce5 <unknown>
#12 0x55a3e9a83b7b <unknown>
#13 0x55a3e9a9e050 <unknown>
#14 0x55a3e9a6cae9 <unknown>
#15 0x55a3e9abcdf5 <unknown>
#16 0x55a3e9abcfdb <unknown>
#17 0x55a3e9accc05 <unknown>
#18 0x7fbdb9b45134 <unknown>
[2025-05-07T08:57:00.075+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:57:00.089+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.556 seconds
[2025-05-07T08:57:30.435+0000] {processor.py:186} INFO - Started process (PID=9349) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:57:30.436+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:57:30.437+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:57:30.437+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:57:36.567+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:57:36.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:57:36.924+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:57:37.310+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:57:37.309+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 23, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a5c6c5fa8e <unknown>
#1 0x55a5c671cb0b <unknown>
#2 0x55a5c67534ea <unknown>
#3 0x55a5c674eaef <unknown>
#4 0x55a5c67a2b18 <unknown>
#5 0x55a5c67909b3 <unknown>
#6 0x55a5c675ac59 <unknown>
#7 0x55a5c675ba08 <unknown>
#8 0x55a5c6c2c40a <unknown>
#9 0x55a5c6c2f85e <unknown>
#10 0x55a5c6c2f308 <unknown>
#11 0x55a5c6c2fce5 <unknown>
#12 0x55a5c6c15b7b <unknown>
#13 0x55a5c6c30050 <unknown>
#14 0x55a5c6bfeae9 <unknown>
#15 0x55a5c6c4edf5 <unknown>
#16 0x55a5c6c4efdb <unknown>
#17 0x55a5c6c5ec05 <unknown>
#18 0x7f15bb176134 <unknown>
[2025-05-07T08:57:37.311+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:57:37.327+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.890 seconds
[2025-05-07T08:58:12.711+0000] {processor.py:186} INFO - Started process (PID=9519) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:58:12.713+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:58:12.713+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:58:12.713+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:58:13.204+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:58:13.563+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:58:13.562+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:58:34.400+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:58:34.398+0000] {timeout.py:68} ERROR - Process timed out, PID: 9519
[2025-05-07T08:59:14.332+0000] {processor.py:186} INFO - Started process (PID=9800) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:59:14.333+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:59:14.334+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:59:14.334+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:59:21.325+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:59:21.724+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:59:21.724+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:59:22.402+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:59:22.400+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557d4d724a8e <unknown>
#1 0x557d4d1e1b0b <unknown>
#2 0x557d4d2184ea <unknown>
#3 0x557d4d213aef <unknown>
#4 0x557d4d267b18 <unknown>
#5 0x557d4d2559b3 <unknown>
#6 0x557d4d21fc59 <unknown>
#7 0x557d4d220a08 <unknown>
#8 0x557d4d6f140a <unknown>
#9 0x557d4d6f485e <unknown>
#10 0x557d4d6f4308 <unknown>
#11 0x557d4d6f4ce5 <unknown>
#12 0x557d4d6dab7b <unknown>
#13 0x557d4d6f5050 <unknown>
#14 0x557d4d6c3ae9 <unknown>
#15 0x557d4d713df5 <unknown>
#16 0x557d4d713fdb <unknown>
#17 0x557d4d723c05 <unknown>
#18 0x7f8d30fbb134 <unknown>
[2025-05-07T08:59:22.403+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:59:22.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.600 seconds
[2025-05-07T08:59:52.456+0000] {processor.py:186} INFO - Started process (PID=9951) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T08:59:52.457+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T08:59:52.458+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:59:52.458+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T08:59:58.045+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T08:59:58.348+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:59:58.348+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T08:59:58.965+0000] {logging_mixin.py:190} INFO - [2025-05-07T08:59:58.964+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55b1d761fa8e <unknown>
#1 0x55b1d70dcb0b <unknown>
#2 0x55b1d71134ea <unknown>
#3 0x55b1d710eaef <unknown>
#4 0x55b1d7162b18 <unknown>
#5 0x55b1d71509b3 <unknown>
#6 0x55b1d711ac59 <unknown>
#7 0x55b1d711ba08 <unknown>
#8 0x55b1d75ec40a <unknown>
#9 0x55b1d75ef85e <unknown>
#10 0x55b1d75ef308 <unknown>
#11 0x55b1d75efce5 <unknown>
#12 0x55b1d75d5b7b <unknown>
#13 0x55b1d75f0050 <unknown>
#14 0x55b1d75beae9 <unknown>
#15 0x55b1d760edf5 <unknown>
#16 0x55b1d760efdb <unknown>
#17 0x55b1d761ec05 <unknown>
#18 0x7fcaa3bbc134 <unknown>
[2025-05-07T08:59:58.966+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T08:59:58.980+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.524 seconds
[2025-05-07T09:00:33.027+0000] {processor.py:186} INFO - Started process (PID=10108) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:00:33.028+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:00:33.029+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:00:33.029+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:00:33.505+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:00:33.841+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:00:33.840+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:00:34.463+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:00:34.462+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f67dfa9a8e <unknown>
#1 0x55f67da66b0b <unknown>
#2 0x55f67da9d4ea <unknown>
#3 0x55f67da98aef <unknown>
#4 0x55f67daecb18 <unknown>
#5 0x55f67dada9b3 <unknown>
#6 0x55f67daa4c59 <unknown>
#7 0x55f67daa5a08 <unknown>
#8 0x55f67df7640a <unknown>
#9 0x55f67df7985e <unknown>
#10 0x55f67df79308 <unknown>
#11 0x55f67df79ce5 <unknown>
#12 0x55f67df5fb7b <unknown>
#13 0x55f67df7a050 <unknown>
#14 0x55f67df48ae9 <unknown>
#15 0x55f67df98df5 <unknown>
#16 0x55f67df98fdb <unknown>
#17 0x55f67dfa8c05 <unknown>
#18 0x7f860fc0d134 <unknown>
[2025-05-07T09:00:34.464+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:00:34.478+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.768 seconds
[2025-05-07T09:01:04.646+0000] {processor.py:186} INFO - Started process (PID=10277) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:01:04.647+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:01:04.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:01:04.648+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:01:11.651+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:01:11.923+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:01:11.923+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:01:12.541+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:01:12.541+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55bd5898ea8e <unknown>
#1 0x55bd5844bb0b <unknown>
#2 0x55bd584824ea <unknown>
#3 0x55bd5847daef <unknown>
#4 0x55bd584d1b18 <unknown>
#5 0x55bd584bf9b3 <unknown>
#6 0x55bd58489c59 <unknown>
#7 0x55bd5848aa08 <unknown>
#8 0x55bd5895b40a <unknown>
#9 0x55bd5895e85e <unknown>
#10 0x55bd5895e308 <unknown>
#11 0x55bd5895ece5 <unknown>
#12 0x55bd58944b7b <unknown>
#13 0x55bd5895f050 <unknown>
#14 0x55bd5892dae9 <unknown>
#15 0x55bd5897ddf5 <unknown>
#16 0x55bd5897dfdb <unknown>
#17 0x55bd5898dc05 <unknown>
#18 0x7fbdff5fa134 <unknown>
[2025-05-07T09:01:12.542+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:01:12.559+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.417 seconds
[2025-05-07T09:01:48.010+0000] {processor.py:186} INFO - Started process (PID=10439) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:01:48.026+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:01:48.027+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:01:48.027+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:01:48.761+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:01:49.079+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:01:49.078+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:01:49.684+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:01:49.683+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55fb28402a8e <unknown>
#1 0x55fb27ebfb0b <unknown>
#2 0x55fb27ef64ea <unknown>
#3 0x55fb27ef1aef <unknown>
#4 0x55fb27f45b18 <unknown>
#5 0x55fb27f339b3 <unknown>
#6 0x55fb27efdc59 <unknown>
#7 0x55fb27efea08 <unknown>
#8 0x55fb283cf40a <unknown>
#9 0x55fb283d285e <unknown>
#10 0x55fb283d2308 <unknown>
#11 0x55fb283d2ce5 <unknown>
#12 0x55fb283b8b7b <unknown>
#13 0x55fb283d3050 <unknown>
#14 0x55fb283a1ae9 <unknown>
#15 0x55fb283f1df5 <unknown>
#16 0x55fb283f1fdb <unknown>
#17 0x55fb28401c05 <unknown>
#18 0x7fa8183e9134 <unknown>
[2025-05-07T09:01:49.684+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:01:49.700+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.013 seconds
[2025-05-07T09:02:23.172+0000] {processor.py:186} INFO - Started process (PID=10588) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:02:23.185+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:02:23.186+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:02:23.185+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:02:23.454+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:02:23.751+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:02:23.751+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:02:24.334+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:02:24.333+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ab6866ba8e <unknown>
#1 0x55ab68128b0b <unknown>
#2 0x55ab6815f4ea <unknown>
#3 0x55ab6815aaef <unknown>
#4 0x55ab681aeb18 <unknown>
#5 0x55ab6819c9b3 <unknown>
#6 0x55ab68166c59 <unknown>
#7 0x55ab68167a08 <unknown>
#8 0x55ab6863840a <unknown>
#9 0x55ab6863b85e <unknown>
#10 0x55ab6863b308 <unknown>
#11 0x55ab6863bce5 <unknown>
#12 0x55ab68621b7b <unknown>
#13 0x55ab6863c050 <unknown>
#14 0x55ab6860aae9 <unknown>
#15 0x55ab6865adf5 <unknown>
#16 0x55ab6865afdb <unknown>
#17 0x55ab6866ac05 <unknown>
#18 0x7f1c559ca134 <unknown>
[2025-05-07T09:02:24.334+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:02:24.360+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.512 seconds
[2025-05-07T09:02:55.049+0000] {processor.py:186} INFO - Started process (PID=10741) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:02:55.053+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:02:55.053+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:02:55.053+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:03:01.677+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:03:01.951+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:03:01.951+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:03:02.525+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:03:02.525+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5592e304da8e <unknown>
#1 0x5592e2b0ab0b <unknown>
#2 0x5592e2b414ea <unknown>
#3 0x5592e2b3caef <unknown>
#4 0x5592e2b90b18 <unknown>
#5 0x5592e2b7e9b3 <unknown>
#6 0x5592e2b48c59 <unknown>
#7 0x5592e2b49a08 <unknown>
#8 0x5592e301a40a <unknown>
#9 0x5592e301d85e <unknown>
#10 0x5592e301d308 <unknown>
#11 0x5592e301dce5 <unknown>
#12 0x5592e3003b7b <unknown>
#13 0x5592e301e050 <unknown>
#14 0x5592e2fecae9 <unknown>
#15 0x5592e303cdf5 <unknown>
#16 0x5592e303cfdb <unknown>
#17 0x5592e304cc05 <unknown>
#18 0x7ff488bc1134 <unknown>
[2025-05-07T09:03:02.526+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:03:02.542+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.997 seconds
[2025-05-07T09:03:32.681+0000] {processor.py:186} INFO - Started process (PID=10905) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:03:32.683+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:03:32.684+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:03:32.683+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:03:38.867+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:03:39.166+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:03:39.166+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:03:39.785+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:03:39.784+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d206d1ba8e <unknown>
#1 0x55d2067d8b0b <unknown>
#2 0x55d20680f4ea <unknown>
#3 0x55d20680aaef <unknown>
#4 0x55d20685eb18 <unknown>
#5 0x55d20684c9b3 <unknown>
#6 0x55d206816c59 <unknown>
#7 0x55d206817a08 <unknown>
#8 0x55d206ce840a <unknown>
#9 0x55d206ceb85e <unknown>
#10 0x55d206ceb308 <unknown>
#11 0x55d206cebce5 <unknown>
#12 0x55d206cd1b7b <unknown>
#13 0x55d206cec050 <unknown>
#14 0x55d206cbaae9 <unknown>
#15 0x55d206d0adf5 <unknown>
#16 0x55d206d0afdb <unknown>
#17 0x55d206d1ac05 <unknown>
#18 0x7f1fb0779134 <unknown>
[2025-05-07T09:03:39.786+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:03:39.800+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.623 seconds
[2025-05-07T09:04:13.209+0000] {processor.py:186} INFO - Started process (PID=11050) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:04:13.211+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:04:13.212+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:04:13.212+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:04:14.499+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:04:14.815+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:04:14.815+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:04:15.383+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:04:15.382+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562be00dba8e <unknown>
#1 0x562bdfb98b0b <unknown>
#2 0x562bdfbcf4ea <unknown>
#3 0x562bdfbcaaef <unknown>
#4 0x562bdfc1eb18 <unknown>
#5 0x562bdfc0c9b3 <unknown>
#6 0x562bdfbd6c59 <unknown>
#7 0x562bdfbd7a08 <unknown>
#8 0x562be00a840a <unknown>
#9 0x562be00ab85e <unknown>
#10 0x562be00ab308 <unknown>
#11 0x562be00abce5 <unknown>
#12 0x562be0091b7b <unknown>
#13 0x562be00ac050 <unknown>
#14 0x562be007aae9 <unknown>
#15 0x562be00cadf5 <unknown>
#16 0x562be00cafdb <unknown>
#17 0x562be00dac05 <unknown>
#18 0x7f09e3e5f134 <unknown>
[2025-05-07T09:04:15.384+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:04:15.400+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.517 seconds
[2025-05-07T09:04:48.374+0000] {processor.py:186} INFO - Started process (PID=11198) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:04:48.376+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:04:48.377+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:04:48.377+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:04:49.799+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:04:50.083+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:04:50.083+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:04:50.706+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:04:50.703+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5630a9e98a8e <unknown>
#1 0x5630a9955b0b <unknown>
#2 0x5630a998c4ea <unknown>
#3 0x5630a9987aef <unknown>
#4 0x5630a99dbb18 <unknown>
#5 0x5630a99c99b3 <unknown>
#6 0x5630a9993c59 <unknown>
#7 0x5630a9994a08 <unknown>
#8 0x5630a9e6540a <unknown>
#9 0x5630a9e6885e <unknown>
#10 0x5630a9e68308 <unknown>
#11 0x5630a9e68ce5 <unknown>
#12 0x5630a9e4eb7b <unknown>
#13 0x5630a9e69050 <unknown>
#14 0x5630a9e37ae9 <unknown>
#15 0x5630a9e87df5 <unknown>
#16 0x5630a9e87fdb <unknown>
#17 0x5630a9e97c05 <unknown>
#18 0x7f63579b1134 <unknown>
[2025-05-07T09:04:50.706+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:04:50.722+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.673 seconds
[2025-05-07T09:05:21.018+0000] {processor.py:186} INFO - Started process (PID=11350) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:05:21.019+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:05:21.020+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:05:21.019+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:05:27.566+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:05:27.930+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:05:27.929+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:05:28.050+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:05:28.049+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559c6b510a8e <unknown>
#1 0x559c6afcdb0b <unknown>
#2 0x559c6b0044ea <unknown>
#3 0x559c6afffaef <unknown>
#4 0x559c6b053b18 <unknown>
#5 0x559c6b0419b3 <unknown>
#6 0x559c6b00bc59 <unknown>
#7 0x559c6b00ca08 <unknown>
#8 0x559c6b4dd40a <unknown>
#9 0x559c6b4e085e <unknown>
#10 0x559c6b4e0308 <unknown>
#11 0x559c6b4e0ce5 <unknown>
#12 0x559c6b4c6b7b <unknown>
#13 0x559c6b4e1050 <unknown>
#14 0x559c6b4afae9 <unknown>
#15 0x559c6b4ffdf5 <unknown>
#16 0x559c6b4fffdb <unknown>
#17 0x559c6b50fc05 <unknown>
#18 0x7f8b494f7134 <unknown>
[2025-05-07T09:05:28.050+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:05:28.066+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.059 seconds
[2025-05-07T09:05:58.499+0000] {processor.py:186} INFO - Started process (PID=11505) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:05:58.500+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:05:58.501+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:05:58.501+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:06:05.323+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:06:05.496+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:06:05.495+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:06:06.063+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:06:06.062+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55aacea7aa8e <unknown>
#1 0x55aace537b0b <unknown>
#2 0x55aace56e4ea <unknown>
#3 0x55aace569aef <unknown>
#4 0x55aace5bdb18 <unknown>
#5 0x55aace5ab9b3 <unknown>
#6 0x55aace575c59 <unknown>
#7 0x55aace576a08 <unknown>
#8 0x55aacea4740a <unknown>
#9 0x55aacea4a85e <unknown>
#10 0x55aacea4a308 <unknown>
#11 0x55aacea4ace5 <unknown>
#12 0x55aacea30b7b <unknown>
#13 0x55aacea4b050 <unknown>
#14 0x55aacea19ae9 <unknown>
#15 0x55aacea69df5 <unknown>
#16 0x55aacea69fdb <unknown>
#17 0x55aacea79c05 <unknown>
#18 0x7f7ad55f8134 <unknown>
[2025-05-07T09:06:06.064+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:06:06.079+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.084 seconds
[2025-05-07T09:06:38.443+0000] {processor.py:186} INFO - Started process (PID=11657) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:06:38.447+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:06:38.448+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:06:38.448+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:06:39.192+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:06:39.467+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:06:39.467+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:06:40.032+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:06:40.032+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556adc5cba8e <unknown>
#1 0x556adc088b0b <unknown>
#2 0x556adc0bf4ea <unknown>
#3 0x556adc0baaef <unknown>
#4 0x556adc10eb18 <unknown>
#5 0x556adc0fc9b3 <unknown>
#6 0x556adc0c6c59 <unknown>
#7 0x556adc0c7a08 <unknown>
#8 0x556adc59840a <unknown>
#9 0x556adc59b85e <unknown>
#10 0x556adc59b308 <unknown>
#11 0x556adc59bce5 <unknown>
#12 0x556adc581b7b <unknown>
#13 0x556adc59c050 <unknown>
#14 0x556adc56aae9 <unknown>
#15 0x556adc5badf5 <unknown>
#16 0x556adc5bafdb <unknown>
#17 0x556adc5cac05 <unknown>
#18 0x7fabec1a1134 <unknown>
[2025-05-07T09:06:40.033+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:06:40.048+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.932 seconds
[2025-05-07T09:07:13.548+0000] {processor.py:186} INFO - Started process (PID=11802) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:07:13.549+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:07:13.550+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:07:13.550+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:07:14.384+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:07:14.676+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:07:14.676+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:07:15.284+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:07:15.283+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5609a0d89a8e <unknown>
#1 0x5609a0846b0b <unknown>
#2 0x5609a087d4ea <unknown>
#3 0x5609a0878aef <unknown>
#4 0x5609a08ccb18 <unknown>
#5 0x5609a08ba9b3 <unknown>
#6 0x5609a0884c59 <unknown>
#7 0x5609a0885a08 <unknown>
#8 0x5609a0d5640a <unknown>
#9 0x5609a0d5985e <unknown>
#10 0x5609a0d59308 <unknown>
#11 0x5609a0d59ce5 <unknown>
#12 0x5609a0d3fb7b <unknown>
#13 0x5609a0d5a050 <unknown>
#14 0x5609a0d28ae9 <unknown>
#15 0x5609a0d78df5 <unknown>
#16 0x5609a0d78fdb <unknown>
#17 0x5609a0d88c05 <unknown>
#18 0x7fcb4f19f134 <unknown>
[2025-05-07T09:07:15.284+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:07:15.301+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.076 seconds
[2025-05-07T09:07:45.721+0000] {processor.py:186} INFO - Started process (PID=11955) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:07:45.725+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:07:45.726+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:07:45.726+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:07:52.851+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:07:53.112+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:07:53.112+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:07:53.678+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:07:53.677+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x564504708a8e <unknown>
#1 0x5645041c5b0b <unknown>
#2 0x5645041fc4ea <unknown>
#3 0x5645041f7aef <unknown>
#4 0x56450424bb18 <unknown>
#5 0x5645042399b3 <unknown>
#6 0x564504203c59 <unknown>
#7 0x564504204a08 <unknown>
#8 0x5645046d540a <unknown>
#9 0x5645046d885e <unknown>
#10 0x5645046d8308 <unknown>
#11 0x5645046d8ce5 <unknown>
#12 0x5645046beb7b <unknown>
#13 0x5645046d9050 <unknown>
#14 0x5645046a7ae9 <unknown>
#15 0x5645046f7df5 <unknown>
#16 0x5645046f7fdb <unknown>
#17 0x564504707c05 <unknown>
#18 0x7f06fb5a8134 <unknown>
[2025-05-07T09:07:53.678+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:07:53.691+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.976 seconds
[2025-05-07T09:08:23.859+0000] {processor.py:186} INFO - Started process (PID=12105) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:08:23.860+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:08:23.862+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:08:23.861+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:08:24.312+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:08:24.596+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:08:24.596+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:08:25.215+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:08:25.215+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557da774ca8e <unknown>
#1 0x557da7209b0b <unknown>
#2 0x557da72404ea <unknown>
#3 0x557da723baef <unknown>
#4 0x557da728fb18 <unknown>
#5 0x557da727d9b3 <unknown>
#6 0x557da7247c59 <unknown>
#7 0x557da7248a08 <unknown>
#8 0x557da771940a <unknown>
#9 0x557da771c85e <unknown>
#10 0x557da771c308 <unknown>
#11 0x557da771cce5 <unknown>
#12 0x557da7702b7b <unknown>
#13 0x557da771d050 <unknown>
#14 0x557da76ebae9 <unknown>
#15 0x557da773bdf5 <unknown>
#16 0x557da773bfdb <unknown>
#17 0x557da774bc05 <unknown>
#18 0x7fb01bb3e134 <unknown>
[2025-05-07T09:08:25.216+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:08:25.233+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.692 seconds
[2025-05-07T09:08:58.731+0000] {processor.py:186} INFO - Started process (PID=12255) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:08:58.733+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:08:58.734+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:08:58.734+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:08:59.786+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:09:00.065+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:09:00.065+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:09:00.683+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:09:00.682+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ed764f7a8e <unknown>
#1 0x55ed75fb4b0b <unknown>
#2 0x55ed75feb4ea <unknown>
#3 0x55ed75fe6aef <unknown>
#4 0x55ed7603ab18 <unknown>
#5 0x55ed760289b3 <unknown>
#6 0x55ed75ff2c59 <unknown>
#7 0x55ed75ff3a08 <unknown>
#8 0x55ed764c440a <unknown>
#9 0x55ed764c785e <unknown>
#10 0x55ed764c7308 <unknown>
#11 0x55ed764c7ce5 <unknown>
#12 0x55ed764adb7b <unknown>
#13 0x55ed764c8050 <unknown>
#14 0x55ed76496ae9 <unknown>
#15 0x55ed764e6df5 <unknown>
#16 0x55ed764e6fdb <unknown>
#17 0x55ed764f6c05 <unknown>
#18 0x7fd5768ab134 <unknown>
[2025-05-07T09:09:00.683+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:09:00.701+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.294 seconds
[2025-05-07T09:09:31.328+0000] {processor.py:186} INFO - Started process (PID=12418) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:09:31.331+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:09:31.332+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:09:31.332+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:09:37.991+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:09:38.298+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:09:38.298+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:09:38.408+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:09:38.407+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e44ebb1a8e <unknown>
#1 0x55e44e66eb0b <unknown>
#2 0x55e44e6a54ea <unknown>
#3 0x55e44e6a0aef <unknown>
#4 0x55e44e6f4b18 <unknown>
#5 0x55e44e6e29b3 <unknown>
#6 0x55e44e6acc59 <unknown>
#7 0x55e44e6ada08 <unknown>
#8 0x55e44eb7e40a <unknown>
#9 0x55e44eb8185e <unknown>
#10 0x55e44eb81308 <unknown>
#11 0x55e44eb81ce5 <unknown>
#12 0x55e44eb67b7b <unknown>
#13 0x55e44eb82050 <unknown>
#14 0x55e44eb50ae9 <unknown>
#15 0x55e44eba0df5 <unknown>
#16 0x55e44eba0fdb <unknown>
#17 0x55e44ebb0c05 <unknown>
#18 0x7f7314209134 <unknown>
[2025-05-07T09:09:38.408+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:09:38.424+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.099 seconds
[2025-05-07T09:10:13.985+0000] {processor.py:186} INFO - Started process (PID=12565) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:10:13.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:10:13.987+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:10:13.987+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:10:16.609+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:10:16.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:10:16.886+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:10:17.503+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:10:17.502+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x560b25cfaa8e <unknown>
#1 0x560b257b7b0b <unknown>
#2 0x560b257ee4ea <unknown>
#3 0x560b257e9aef <unknown>
#4 0x560b2583db18 <unknown>
#5 0x560b2582b9b3 <unknown>
#6 0x560b257f5c59 <unknown>
#7 0x560b257f6a08 <unknown>
#8 0x560b25cc740a <unknown>
#9 0x560b25cca85e <unknown>
#10 0x560b25cca308 <unknown>
#11 0x560b25ccace5 <unknown>
#12 0x560b25cb0b7b <unknown>
#13 0x560b25ccb050 <unknown>
#14 0x560b25c99ae9 <unknown>
#15 0x560b25ce9df5 <unknown>
#16 0x560b25ce9fdb <unknown>
#17 0x560b25cf9c05 <unknown>
#18 0x7f20cb993134 <unknown>
[2025-05-07T09:10:17.504+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:10:17.519+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.861 seconds
[2025-05-07T09:10:47.961+0000] {processor.py:186} INFO - Started process (PID=12716) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:10:47.962+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:10:47.963+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:10:47.963+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:10:54.235+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:10:54.544+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:10:54.544+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:10:55.163+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:10:55.162+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5579d0aaea8e <unknown>
#1 0x5579d056bb0b <unknown>
#2 0x5579d05a24ea <unknown>
#3 0x5579d059daef <unknown>
#4 0x5579d05f1b18 <unknown>
#5 0x5579d05df9b3 <unknown>
#6 0x5579d05a9c59 <unknown>
#7 0x5579d05aaa08 <unknown>
#8 0x5579d0a7b40a <unknown>
#9 0x5579d0a7e85e <unknown>
#10 0x5579d0a7e308 <unknown>
#11 0x5579d0a7ece5 <unknown>
#12 0x5579d0a64b7b <unknown>
#13 0x5579d0a7f050 <unknown>
#14 0x5579d0a4dae9 <unknown>
#15 0x5579d0a9ddf5 <unknown>
#16 0x5579d0a9dfdb <unknown>
#17 0x5579d0aadc05 <unknown>
#18 0x7f876c84c134 <unknown>
[2025-05-07T09:10:55.163+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:10:55.177+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.220 seconds
[2025-05-07T09:11:25.701+0000] {processor.py:186} INFO - Started process (PID=12875) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:11:25.703+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:11:25.704+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:11:25.703+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:11:31.880+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:11:32.209+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:11:32.208+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:11:32.862+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:11:32.860+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5574858c2a8e <unknown>
#1 0x55748537fb0b <unknown>
#2 0x5574853b64ea <unknown>
#3 0x5574853b1aef <unknown>
#4 0x557485405b18 <unknown>
#5 0x5574853f39b3 <unknown>
#6 0x5574853bdc59 <unknown>
#7 0x5574853bea08 <unknown>
#8 0x55748588f40a <unknown>
#9 0x55748589285e <unknown>
#10 0x557485892308 <unknown>
#11 0x557485892ce5 <unknown>
#12 0x557485878b7b <unknown>
#13 0x557485893050 <unknown>
#14 0x557485861ae9 <unknown>
#15 0x5574858b1df5 <unknown>
#16 0x5574858b1fdb <unknown>
#17 0x5574858c1c05 <unknown>
#18 0x7f12df768134 <unknown>
[2025-05-07T09:11:32.862+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:11:32.877+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.686 seconds
[2025-05-07T09:12:03.960+0000] {processor.py:186} INFO - Started process (PID=13017) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:12:03.961+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:12:03.962+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:12:03.962+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:12:04.947+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:12:05.119+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:12:05.119+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:12:05.701+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:12:05.700+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563804e32a8e <unknown>
#1 0x5638048efb0b <unknown>
#2 0x5638049264ea <unknown>
#3 0x563804921aef <unknown>
#4 0x563804975b18 <unknown>
#5 0x5638049639b3 <unknown>
#6 0x56380492dc59 <unknown>
#7 0x56380492ea08 <unknown>
#8 0x563804dff40a <unknown>
#9 0x563804e0285e <unknown>
#10 0x563804e02308 <unknown>
#11 0x563804e02ce5 <unknown>
#12 0x563804de8b7b <unknown>
#13 0x563804e03050 <unknown>
#14 0x563804dd1ae9 <unknown>
#15 0x563804e21df5 <unknown>
#16 0x563804e21fdb <unknown>
#17 0x563804e31c05 <unknown>
#18 0x7f80ad578134 <unknown>
[2025-05-07T09:12:05.701+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:12:05.714+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.084 seconds
[2025-05-07T09:12:36.119+0000] {processor.py:186} INFO - Started process (PID=13174) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:12:36.121+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:12:36.122+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:12:36.122+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:12:43.407+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:12:43.621+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:12:43.621+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:12:43.792+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:12:43.792+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556565bb2a8e <unknown>
#1 0x55656566fb0b <unknown>
#2 0x5565656a64ea <unknown>
#3 0x5565656a1aef <unknown>
#4 0x5565656f5b18 <unknown>
#5 0x5565656e39b3 <unknown>
#6 0x5565656adc59 <unknown>
#7 0x5565656aea08 <unknown>
#8 0x556565b7f40a <unknown>
#9 0x556565b8285e <unknown>
#10 0x556565b82308 <unknown>
#11 0x556565b82ce5 <unknown>
#12 0x556565b68b7b <unknown>
#13 0x556565b83050 <unknown>
#14 0x556565b51ae9 <unknown>
#15 0x556565ba1df5 <unknown>
#16 0x556565ba1fdb <unknown>
#17 0x556565bb1c05 <unknown>
#18 0x7f94d4473134 <unknown>
[2025-05-07T09:12:43.793+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:12:43.810+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.700 seconds
[2025-05-07T09:13:14.077+0000] {processor.py:186} INFO - Started process (PID=13332) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:13:14.077+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:13:14.079+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:13:14.078+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:13:15.267+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:13:15.462+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:13:15.462+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:13:16.079+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:13:16.078+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563957b0ba8e <unknown>
#1 0x5639575c8b0b <unknown>
#2 0x5639575ff4ea <unknown>
#3 0x5639575faaef <unknown>
#4 0x56395764eb18 <unknown>
#5 0x56395763c9b3 <unknown>
#6 0x563957606c59 <unknown>
#7 0x563957607a08 <unknown>
#8 0x563957ad840a <unknown>
#9 0x563957adb85e <unknown>
#10 0x563957adb308 <unknown>
#11 0x563957adbce5 <unknown>
#12 0x563957ac1b7b <unknown>
#13 0x563957adc050 <unknown>
#14 0x563957aaaae9 <unknown>
#15 0x563957afadf5 <unknown>
#16 0x563957afafdb <unknown>
#17 0x563957b0ac05 <unknown>
#18 0x7fd0a59eb134 <unknown>
[2025-05-07T09:13:16.080+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:13:16.094+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.349 seconds
[2025-05-07T09:13:49.242+0000] {processor.py:186} INFO - Started process (PID=13476) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:13:49.243+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:13:49.244+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:13:49.244+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:13:50.037+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:13:50.349+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:13:50.349+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:13:50.970+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:13:50.969+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556e0f199a8e <unknown>
#1 0x556e0ec56b0b <unknown>
#2 0x556e0ec8d4ea <unknown>
#3 0x556e0ec88aef <unknown>
#4 0x556e0ecdcb18 <unknown>
#5 0x556e0ecca9b3 <unknown>
#6 0x556e0ec94c59 <unknown>
#7 0x556e0ec95a08 <unknown>
#8 0x556e0f16640a <unknown>
#9 0x556e0f16985e <unknown>
#10 0x556e0f169308 <unknown>
#11 0x556e0f169ce5 <unknown>
#12 0x556e0f14fb7b <unknown>
#13 0x556e0f16a050 <unknown>
#14 0x556e0f138ae9 <unknown>
#15 0x556e0f188df5 <unknown>
#16 0x556e0f188fdb <unknown>
#17 0x556e0f198c05 <unknown>
#18 0x7f23ca0dd134 <unknown>
[2025-05-07T09:13:50.971+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:13:50.987+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.072 seconds
[2025-05-07T09:14:21.726+0000] {processor.py:186} INFO - Started process (PID=13622) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:14:21.730+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:14:21.731+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:14:21.731+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:14:28.598+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:14:34.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:14:34.157+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:14:28.974+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:14:28.973+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x555acf3d1a8e <unknown>
#1 0x555acee8eb0b <unknown>
#2 0x555aceec54ea <unknown>
#3 0x555aceec0aef <unknown>
#4 0x555acef14b18 <unknown>
#5 0x555acef029b3 <unknown>
#6 0x555aceeccc59 <unknown>
#7 0x555aceecda08 <unknown>
#8 0x555acf39e40a <unknown>
#9 0x555acf3a185e <unknown>
#10 0x555acf3a1308 <unknown>
#11 0x555acf3a1ce5 <unknown>
#12 0x555acf387b7b <unknown>
#13 0x555acf3a2050 <unknown>
#14 0x555acf370ae9 <unknown>
#15 0x555acf3c0df5 <unknown>
#16 0x555acf3c0fdb <unknown>
#17 0x555acf3d0c05 <unknown>
#18 0x7f8d5d8b4134 <unknown>
[2025-05-07T09:14:28.975+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:14:28.990+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.264 seconds
[2025-05-07T09:14:58.639+0000] {processor.py:186} INFO - Started process (PID=13776) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:14:58.640+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:14:58.641+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:14:58.641+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:15:05.327+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:15:05.958+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:15:05.958+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:15:06.588+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:15:06.585+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559be247ca8e <unknown>
#1 0x559be1f39b0b <unknown>
#2 0x559be1f704ea <unknown>
#3 0x559be1f6baef <unknown>
#4 0x559be1fbfb18 <unknown>
#5 0x559be1fad9b3 <unknown>
#6 0x559be1f77c59 <unknown>
#7 0x559be1f78a08 <unknown>
#8 0x559be244940a <unknown>
#9 0x559be244c85e <unknown>
#10 0x559be244c308 <unknown>
#11 0x559be244cce5 <unknown>
#12 0x559be2432b7b <unknown>
#13 0x559be244d050 <unknown>
#14 0x559be241bae9 <unknown>
#15 0x559be246bdf5 <unknown>
#16 0x559be246bfdb <unknown>
#17 0x559be247bc05 <unknown>
#18 0x7f2d18798134 <unknown>
[2025-05-07T09:15:06.589+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:15:06.609+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.473 seconds
[2025-05-07T09:15:37.364+0000] {processor.py:186} INFO - Started process (PID=13955) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:15:37.365+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:15:37.366+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:15:37.366+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:15:43.724+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:15:44.881+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:15:44.880+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:15:45.447+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:15:45.446+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f0b1665a8e <unknown>
#1 0x55f0b1122b0b <unknown>
#2 0x55f0b11594ea <unknown>
#3 0x55f0b1154aef <unknown>
#4 0x55f0b11a8b18 <unknown>
#5 0x55f0b11969b3 <unknown>
#6 0x55f0b1160c59 <unknown>
#7 0x55f0b1161a08 <unknown>
#8 0x55f0b163240a <unknown>
#9 0x55f0b163585e <unknown>
#10 0x55f0b1635308 <unknown>
#11 0x55f0b1635ce5 <unknown>
#12 0x55f0b161bb7b <unknown>
#13 0x55f0b1636050 <unknown>
#14 0x55f0b1604ae9 <unknown>
#15 0x55f0b1654df5 <unknown>
#16 0x55f0b1654fdb <unknown>
#17 0x55f0b1664c05 <unknown>
#18 0x7fd8060ee134 <unknown>
[2025-05-07T09:15:45.448+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:15:45.463+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.113 seconds
[2025-05-07T09:16:15.992+0000] {processor.py:186} INFO - Started process (PID=14124) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:16:15.994+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:16:15.995+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:16:15.994+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:16:22.060+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:16:22.328+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:16:22.328+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:16:22.896+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:16:22.895+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55af27a0ba8e <unknown>
#1 0x55af274c8b0b <unknown>
#2 0x55af274ff4ea <unknown>
#3 0x55af274faaef <unknown>
#4 0x55af2754eb18 <unknown>
#5 0x55af2753c9b3 <unknown>
#6 0x55af27506c59 <unknown>
#7 0x55af27507a08 <unknown>
#8 0x55af279d840a <unknown>
#9 0x55af279db85e <unknown>
#10 0x55af279db308 <unknown>
#11 0x55af279dbce5 <unknown>
#12 0x55af279c1b7b <unknown>
#13 0x55af279dc050 <unknown>
#14 0x55af279aaae9 <unknown>
#15 0x55af279fadf5 <unknown>
#16 0x55af279fafdb <unknown>
#17 0x55af27a0ac05 <unknown>
#18 0x7f7a2aead134 <unknown>
[2025-05-07T09:16:22.897+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:16:22.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.419 seconds
[2025-05-07T09:16:53.330+0000] {processor.py:186} INFO - Started process (PID=14275) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:16:53.334+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:16:53.335+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:16:53.335+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:16:58.938+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:16:59.263+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:16:59.262+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:16:59.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:16:59.885+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5564cac96a8e <unknown>
#1 0x5564ca753b0b <unknown>
#2 0x5564ca78a4ea <unknown>
#3 0x5564ca785aef <unknown>
#4 0x5564ca7d9b18 <unknown>
#5 0x5564ca7c79b3 <unknown>
#6 0x5564ca791c59 <unknown>
#7 0x5564ca792a08 <unknown>
#8 0x5564cac6340a <unknown>
#9 0x5564cac6685e <unknown>
#10 0x5564cac66308 <unknown>
#11 0x5564cac66ce5 <unknown>
#12 0x5564cac4cb7b <unknown>
#13 0x5564cac67050 <unknown>
#14 0x5564cac35ae9 <unknown>
#15 0x5564cac85df5 <unknown>
#16 0x5564cac85fdb <unknown>
#17 0x5564cac95c05 <unknown>
#18 0x7f7320648134 <unknown>
[2025-05-07T09:16:59.886+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:16:59.901+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.579 seconds
[2025-05-07T09:17:30.041+0000] {processor.py:186} INFO - Started process (PID=14431) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:17:30.042+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:17:30.043+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:17:30.043+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:17:37.277+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:17:37.417+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:17:37.417+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:17:37.985+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:17:37.984+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55dc27167a8e <unknown>
#1 0x55dc26c24b0b <unknown>
#2 0x55dc26c5b4ea <unknown>
#3 0x55dc26c56aef <unknown>
#4 0x55dc26caab18 <unknown>
#5 0x55dc26c989b3 <unknown>
#6 0x55dc26c62c59 <unknown>
#7 0x55dc26c63a08 <unknown>
#8 0x55dc2713440a <unknown>
#9 0x55dc2713785e <unknown>
#10 0x55dc27137308 <unknown>
#11 0x55dc27137ce5 <unknown>
#12 0x55dc2711db7b <unknown>
#13 0x55dc27138050 <unknown>
#14 0x55dc27106ae9 <unknown>
#15 0x55dc27156df5 <unknown>
#16 0x55dc27156fdb <unknown>
#17 0x55dc27166c05 <unknown>
#18 0x7f97ac14a134 <unknown>
[2025-05-07T09:17:37.986+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:17:38.002+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.470 seconds
[2025-05-07T09:18:09.745+0000] {processor.py:186} INFO - Started process (PID=14580) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:18:09.747+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:18:09.748+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:18:09.748+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:18:10.540+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:18:10.632+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:18:10.632+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:18:11.252+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:18:11.251+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c3bd8f7a8e <unknown>
#1 0x55c3bd3b4b0b <unknown>
#2 0x55c3bd3eb4ea <unknown>
#3 0x55c3bd3e6aef <unknown>
#4 0x55c3bd43ab18 <unknown>
#5 0x55c3bd4289b3 <unknown>
#6 0x55c3bd3f2c59 <unknown>
#7 0x55c3bd3f3a08 <unknown>
#8 0x55c3bd8c440a <unknown>
#9 0x55c3bd8c785e <unknown>
#10 0x55c3bd8c7308 <unknown>
#11 0x55c3bd8c7ce5 <unknown>
#12 0x55c3bd8adb7b <unknown>
#13 0x55c3bd8c8050 <unknown>
#14 0x55c3bd896ae9 <unknown>
#15 0x55c3bd8e6df5 <unknown>
#16 0x55c3bd8e6fdb <unknown>
#17 0x55c3bd8f6c05 <unknown>
#18 0x7f377de7b134 <unknown>
[2025-05-07T09:18:11.252+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:18:11.267+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.849 seconds
[2025-05-07T09:18:42.175+0000] {processor.py:186} INFO - Started process (PID=14744) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:18:42.176+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:18:42.177+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:18:42.177+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:18:48.399+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:18:48.718+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:18:48.718+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:18:54.649+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:18:54.648+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55be477caa8e <unknown>
#1 0x55be47287b0b <unknown>
#2 0x55be472be4ea <unknown>
#3 0x55be472b9aef <unknown>
#4 0x55be4730db18 <unknown>
#5 0x55be472fb9b3 <unknown>
#6 0x55be472c5c59 <unknown>
#7 0x55be472c6a08 <unknown>
#8 0x55be4779740a <unknown>
#9 0x55be4779a85e <unknown>
#10 0x55be4779a308 <unknown>
#11 0x55be4779ace5 <unknown>
#12 0x55be47780b7b <unknown>
#13 0x55be4779b050 <unknown>
#14 0x55be47769ae9 <unknown>
#15 0x55be477b9df5 <unknown>
#16 0x55be477b9fdb <unknown>
#17 0x55be477c9c05 <unknown>
#18 0x7f74228ad134 <unknown>
[2025-05-07T09:18:54.649+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:18:54.666+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.672 seconds
[2025-05-07T09:19:25.141+0000] {processor.py:186} INFO - Started process (PID=14905) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:19:25.153+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:19:25.155+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:19:25.155+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:19:31.786+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:19:32.115+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:19:32.115+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:19:32.734+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:19:32.733+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x564571897a8e <unknown>
#1 0x564571354b0b <unknown>
#2 0x56457138b4ea <unknown>
#3 0x564571386aef <unknown>
#4 0x5645713dab18 <unknown>
#5 0x5645713c89b3 <unknown>
#6 0x564571392c59 <unknown>
#7 0x564571393a08 <unknown>
#8 0x56457186440a <unknown>
#9 0x56457186785e <unknown>
#10 0x564571867308 <unknown>
#11 0x564571867ce5 <unknown>
#12 0x56457184db7b <unknown>
#13 0x564571868050 <unknown>
#14 0x564571836ae9 <unknown>
#15 0x564571886df5 <unknown>
#16 0x564571886fdb <unknown>
#17 0x564571896c05 <unknown>
#18 0x7f2f604da134 <unknown>
[2025-05-07T09:19:32.735+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:19:32.752+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.116 seconds
[2025-05-07T09:20:03.378+0000] {processor.py:186} INFO - Started process (PID=15064) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:20:03.379+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:20:03.381+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:20:03.381+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:20:10.139+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:20:10.566+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:20:10.565+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:20:11.185+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:20:11.184+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56298814ca8e <unknown>
#1 0x562987c09b0b <unknown>
#2 0x562987c404ea <unknown>
#3 0x562987c3baef <unknown>
#4 0x562987c8fb18 <unknown>
#5 0x562987c7d9b3 <unknown>
#6 0x562987c47c59 <unknown>
#7 0x562987c48a08 <unknown>
#8 0x56298811940a <unknown>
#9 0x56298811c85e <unknown>
#10 0x56298811c308 <unknown>
#11 0x56298811cce5 <unknown>
#12 0x562988102b7b <unknown>
#13 0x56298811d050 <unknown>
#14 0x5629880ebae9 <unknown>
#15 0x56298813bdf5 <unknown>
#16 0x56298813bfdb <unknown>
#17 0x56298814bc05 <unknown>
#18 0x7fa025992134 <unknown>
[2025-05-07T09:20:11.186+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:20:11.203+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.840 seconds
[2025-05-07T09:20:44.921+0000] {processor.py:186} INFO - Started process (PID=15211) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:20:44.922+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:20:44.923+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:20:44.923+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:20:45.698+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:20:46.013+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:20:46.012+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:20:46.633+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:20:46.630+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562c9d8eba8e <unknown>
#1 0x562c9d3a8b0b <unknown>
#2 0x562c9d3df4ea <unknown>
#3 0x562c9d3daaef <unknown>
#4 0x562c9d42eb18 <unknown>
#5 0x562c9d41c9b3 <unknown>
#6 0x562c9d3e6c59 <unknown>
#7 0x562c9d3e7a08 <unknown>
#8 0x562c9d8b840a <unknown>
#9 0x562c9d8bb85e <unknown>
#10 0x562c9d8bb308 <unknown>
#11 0x562c9d8bbce5 <unknown>
#12 0x562c9d8a1b7b <unknown>
#13 0x562c9d8bc050 <unknown>
#14 0x562c9d88aae9 <unknown>
#15 0x562c9d8dadf5 <unknown>
#16 0x562c9d8dafdb <unknown>
#17 0x562c9d8eac05 <unknown>
#18 0x7fb5f2f63134 <unknown>
[2025-05-07T09:20:46.634+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:20:46.649+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.061 seconds
[2025-05-07T09:21:19.996+0000] {processor.py:186} INFO - Started process (PID=15355) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:21:20.000+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:21:20.001+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:21:20.001+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:21:20.639+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:21:20.942+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:21:20.942+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:21:21.531+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:21:21.529+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 57, in <module>
    scrape_task = fb_videos_scraper(id="official.parkhangseo")
  File "/opt/airflow/dags/tasks/fb_videos_scraper.py", line 7, in fb_videos_scraper
    videos_scraper = AccountVideo(id)
  File "/opt/airflow/dags/utils/account_videos.py", line 19, in __init__
    super().__init__(user_id, base_url=f"https://www.facebook.com/{user_id}/videos")
  File "/opt/airflow/dags/utils/facebook_base.py", line 24, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f7da80da8e <unknown>
#1 0x55f7da2cab0b <unknown>
#2 0x55f7da3014ea <unknown>
#3 0x55f7da2fcaef <unknown>
#4 0x55f7da350b18 <unknown>
#5 0x55f7da33e9b3 <unknown>
#6 0x55f7da308c59 <unknown>
#7 0x55f7da309a08 <unknown>
#8 0x55f7da7da40a <unknown>
#9 0x55f7da7dd85e <unknown>
#10 0x55f7da7dd308 <unknown>
#11 0x55f7da7ddce5 <unknown>
#12 0x55f7da7c3b7b <unknown>
#13 0x55f7da7de050 <unknown>
#14 0x55f7da7acae9 <unknown>
#15 0x55f7da7fcdf5 <unknown>
#16 0x55f7da7fcfdb <unknown>
#17 0x55f7da80cc05 <unknown>
#18 0x7f115dd16134 <unknown>
[2025-05-07T09:21:21.532+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:21:21.546+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.892 seconds
[2025-05-07T09:22:14.460+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:22:14.461+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:22:14.466+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:22:14.465+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:22:24.285+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:22:41.462+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:22:41.461+0000] {timeout.py:68} ERROR - Process timed out, PID: 55
[2025-05-07T09:22:47.770+0000] {processor.py:186} INFO - Started process (PID=249) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:22:47.770+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:22:47.772+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:22:47.772+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:22:54.230+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:22:54.563+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:22:59.958+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:22:59.958+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:22:59.965+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:22:59.965+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:22:59.967+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:22:59.967+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:22:59.968+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:22:59.968+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:22:59.987+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.398 seconds
[2025-05-07T09:23:30.139+0000] {processor.py:186} INFO - Started process (PID=376) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:23:30.150+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:23:30.267+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:23:30.267+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:23:37.171+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:23:37.367+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:23:37.387+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:23:37.386+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:23:37.395+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:23:37.395+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:23:37.398+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:23:37.398+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:23:37.399+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:23:37.398+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:23:37.409+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.776 seconds
[2025-05-07T09:24:07.666+0000] {processor.py:186} INFO - Started process (PID=518) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:24:07.667+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:24:07.669+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:07.668+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:24:13.938+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:24:14.257+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:24:14.279+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:14.279+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:24:14.288+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:14.288+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:24:14.290+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:14.290+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:24:14.291+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:14.290+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:24:14.318+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.156 seconds
[2025-05-07T09:24:45.235+0000] {processor.py:186} INFO - Started process (PID=642) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:24:45.246+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:24:45.248+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:45.248+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:24:45.568+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:24:45.869+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:24:45.890+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:45.890+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:24:45.898+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:45.898+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:24:45.900+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:45.900+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:24:45.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:24:45.901+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:24:45.921+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.020 seconds
[2025-05-07T09:25:16.781+0000] {processor.py:186} INFO - Started process (PID=773) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:25:16.782+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:25:16.783+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:25:16.783+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:25:23.380+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:25:23.711+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:25:23.736+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:25:23.736+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:25:23.745+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:25:23.745+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:25:23.748+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:25:23.748+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:25:23.749+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:25:23.749+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:25:23.759+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.484 seconds
[2025-05-07T09:28:41.071+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:28:41.073+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:28:41.075+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:28:41.075+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:29:08.577+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:29:08.576+0000] {timeout.py:68} ERROR - Process timed out, PID: 55
[2025-05-07T09:29:39.104+0000] {processor.py:186} INFO - Started process (PID=167) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:29:39.115+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:29:39.117+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:29:39.117+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:29:45.989+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:29:46.305+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:29:46.372+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:29:46.372+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:29:46.379+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:29:46.379+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:29:46.381+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:29:46.381+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:29:46.382+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:29:46.381+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:29:46.406+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.311 seconds
[2025-05-07T09:30:20.809+0000] {processor.py:186} INFO - Started process (PID=315) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:30:20.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:30:20.818+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:20.818+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:30:21.444+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:30:21.879+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:30:21.901+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:21.901+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:30:21.909+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:21.909+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:30:21.911+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:21.911+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:30:21.912+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:21.912+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:30:21.923+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.454 seconds
[2025-05-07T09:30:55.865+0000] {processor.py:186} INFO - Started process (PID=440) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:30:55.866+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:30:55.868+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:55.868+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:30:56.328+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:30:56.626+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:30:56.644+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:56.644+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:30:56.653+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:56.653+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:30:56.655+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:56.655+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:30:56.656+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:30:56.656+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:30:56.682+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.146 seconds
[2025-05-07T09:31:27.414+0000] {processor.py:186} INFO - Started process (PID=556) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:31:27.429+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:31:27.431+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:31:27.431+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:31:33.626+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:31:33.952+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:31:33.973+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:31:33.972+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:31:33.981+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:31:33.981+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:31:33.983+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:31:33.983+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:31:33.984+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:31:33.984+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:31:33.995+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.084 seconds
[2025-05-07T09:32:05.929+0000] {processor.py:186} INFO - Started process (PID=681) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:32:05.930+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:32:05.934+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:05.934+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:32:07.388+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:32:07.743+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:32:07.767+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:07.766+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:32:07.777+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:07.776+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:32:07.779+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:07.779+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:32:07.780+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:07.780+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:32:07.790+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.206 seconds
[2025-05-07T09:32:37.926+0000] {processor.py:186} INFO - Started process (PID=805) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:32:37.929+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:32:37.930+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:37.930+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:32:45.337+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:32:51.060+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:32:51.085+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:51.084+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:32:51.093+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:51.093+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:32:51.097+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:51.097+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:32:51.098+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:32:51.098+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:32:51.109+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.355 seconds
[2025-05-07T09:33:21.981+0000] {processor.py:186} INFO - Started process (PID=947) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:33:21.982+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:33:21.984+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:33:21.983+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:33:29.813+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:33:30.154+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:33:30.176+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:33:30.176+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:33:30.184+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:33:30.184+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:33:30.186+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:33:30.186+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:33:30.187+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:33:30.187+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:33:30.196+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.715 seconds
[2025-05-07T09:34:00.464+0000] {processor.py:186} INFO - Started process (PID=1070) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:34:00.466+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:34:00.468+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:00.467+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:34:07.195+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:34:07.378+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:34:07.397+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:07.397+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:34:07.406+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:07.406+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:34:07.408+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:07.408+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:34:07.409+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:07.409+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:34:07.419+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.464 seconds
[2025-05-07T09:34:37.776+0000] {processor.py:186} INFO - Started process (PID=1197) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:34:37.777+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:34:37.779+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:37.778+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:34:43.991+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:34:44.324+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:34:44.345+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:44.345+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:34:44.355+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:44.355+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:34:44.357+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:44.357+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:34:44.358+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:34:44.358+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:34:44.369+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.094 seconds
[2025-05-07T09:35:14.904+0000] {processor.py:186} INFO - Started process (PID=1327) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:35:14.905+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:35:14.906+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:14.906+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:35:20.904+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:35:21.114+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:35:21.138+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:21.138+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:35:21.148+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:21.148+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:35:21.151+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:21.150+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:35:21.152+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:21.152+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:35:21.176+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.280 seconds
[2025-05-07T09:35:51.792+0000] {processor.py:186} INFO - Started process (PID=1452) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:35:51.793+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:35:51.795+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:51.795+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:35:57.906+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:35:58.235+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:35:58.261+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:58.261+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:35:58.272+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:58.272+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:35:58.275+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:58.275+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:35:58.276+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:35:58.276+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:35:58.299+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.006 seconds
[2025-05-07T09:36:29.255+0000] {processor.py:186} INFO - Started process (PID=1582) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:36:29.273+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:36:29.275+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:36:29.275+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:36:35.577+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:36:35.886+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:36:35.906+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:36:35.906+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:36:35.916+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:36:35.916+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:36:35.919+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:36:35.918+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:36:35.919+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:36:35.919+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:36:35.930+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.178 seconds
[2025-05-07T09:37:06.383+0000] {processor.py:186} INFO - Started process (PID=1703) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:37:06.394+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:37:06.396+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:06.396+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:37:06.826+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:37:07.128+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:37:07.150+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:07.149+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:37:07.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:07.158+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:37:07.161+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:07.160+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:37:07.161+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:07.161+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:37:07.189+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.137 seconds
[2025-05-07T09:37:37.220+0000] {processor.py:186} INFO - Started process (PID=1820) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:37:37.224+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:37:37.225+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:37.225+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:37:43.238+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:37:43.571+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:37:43.597+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:43.596+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:37:43.607+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:43.607+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:37:43.610+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:43.609+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:37:43.610+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:37:43.610+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:37:43.621+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.911 seconds
[2025-05-07T09:38:14.539+0000] {processor.py:186} INFO - Started process (PID=1946) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:38:14.542+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:38:14.544+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:14.544+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:38:20.972+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:38:26.679+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:38:26.700+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:26.700+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:38:26.709+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:26.709+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:38:26.712+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:26.711+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:38:26.712+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:26.712+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:38:20.896+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.353 seconds
[2025-05-07T09:38:51.113+0000] {processor.py:186} INFO - Started process (PID=2072) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:38:51.114+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:38:51.117+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:51.116+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:38:57.052+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:38:57.383+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:38:57.405+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:57.404+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:38:57.415+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:57.415+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:38:57.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:57.417+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:38:57.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:38:57.418+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:38:57.430+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.324 seconds
[2025-05-07T09:39:28.198+0000] {processor.py:186} INFO - Started process (PID=2197) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:39:28.200+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:39:28.202+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:39:28.202+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:39:34.438+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:39:34.750+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:39:34.771+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:39:34.771+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:39:34.780+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:39:34.780+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:39:34.782+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:39:34.782+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:39:34.783+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:39:34.783+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:39:34.793+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.109 seconds
[2025-05-07T09:40:05.465+0000] {processor.py:186} INFO - Started process (PID=2318) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:40:05.466+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:40:05.468+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:05.468+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:40:11.106+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:40:11.466+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:40:11.561+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:11.561+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:40:11.569+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:11.569+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:40:11.572+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:11.572+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:40:11.573+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:11.573+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:40:11.585+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.127 seconds
[2025-05-07T09:40:46.864+0000] {processor.py:186} INFO - Started process (PID=2449) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:40:46.865+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:40:46.867+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:46.866+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:40:47.440+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:40:47.777+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:40:47.803+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:47.803+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:40:47.815+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:47.815+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:40:47.818+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:47.818+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:40:47.820+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:40:47.820+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:40:47.833+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.305 seconds
[2025-05-07T09:41:18.428+0000] {processor.py:186} INFO - Started process (PID=2576) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:41:18.431+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:41:18.433+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:41:18.433+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:41:24.837+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:41:24.994+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:41:25.019+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:41:25.018+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:41:25.029+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:41:25.029+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:41:25.032+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:41:25.032+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:41:25.033+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:41:25.033+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:41:25.045+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.129 seconds
[2025-05-07T09:41:55.779+0000] {processor.py:186} INFO - Started process (PID=2693) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:41:55.780+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:41:55.782+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:41:55.781+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:42:01.373+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:42:01.643+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:42:01.663+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:01.663+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:42:01.672+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:01.672+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:42:01.674+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:01.674+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:42:01.674+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:01.674+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:42:01.685+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 6.911 seconds
[2025-05-07T09:42:32.056+0000] {processor.py:186} INFO - Started process (PID=2824) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:42:32.057+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:42:32.059+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:32.059+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:42:33.526+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:42:33.880+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:42:33.907+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:33.906+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:42:33.920+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:33.919+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:42:33.923+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:33.923+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:42:33.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:42:33.924+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:42:33.938+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.222 seconds
[2025-05-07T09:43:04.590+0000] {processor.py:186} INFO - Started process (PID=2972) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:43:04.591+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:43:04.593+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:04.592+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:43:11.289+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:43:16.935+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:43:16.957+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:16.957+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:43:16.968+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:16.967+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:43:16.970+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:16.970+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:43:16.971+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:16.971+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:43:16.982+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.567 seconds
[2025-05-07T09:43:47.048+0000] {processor.py:186} INFO - Started process (PID=3095) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:43:47.049+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:43:47.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:47.051+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:43:48.014+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:43:48.331+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:43:48.369+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:48.368+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:43:48.379+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:48.379+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:43:48.382+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:48.382+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:43:48.383+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:43:48.383+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:43:48.394+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.686 seconds
[2025-05-07T09:44:22.123+0000] {processor.py:186} INFO - Started process (PID=3225) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:44:22.125+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:44:22.129+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:44:22.129+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:44:23.285+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:44:23.581+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:44:23.606+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:44:23.606+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:44:23.616+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:44:23.616+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:44:23.619+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:44:23.619+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:44:23.619+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:44:23.619+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:44:23.630+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.847 seconds
[2025-05-07T09:44:54.195+0000] {processor.py:186} INFO - Started process (PID=3357) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:44:54.196+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:44:54.198+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:44:54.198+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:45:00.791+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:45:01.125+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:45:01.146+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:01.146+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:45:01.157+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:01.157+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:45:01.159+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:01.159+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:45:01.160+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:01.160+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:45:01.172+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.485 seconds
[2025-05-07T09:45:32.385+0000] {processor.py:186} INFO - Started process (PID=3480) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:45:32.386+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:45:32.388+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:32.388+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:45:33.452+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:45:33.623+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:45:33.649+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:33.649+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:45:33.658+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:33.658+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:45:33.661+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:33.661+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:45:33.662+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:45:33.662+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:45:33.673+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.629 seconds
[2025-05-07T09:46:07.433+0000] {processor.py:186} INFO - Started process (PID=3612) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:46:07.433+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:46:07.435+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:07.435+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:46:07.931+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:46:08.266+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:46:08.291+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:08.290+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:46:08.301+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:08.301+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:46:08.303+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:08.303+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:46:08.304+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:08.304+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:46:08.315+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.230 seconds
[2025-05-07T09:46:38.979+0000] {processor.py:186} INFO - Started process (PID=3737) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:46:38.983+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:46:38.992+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:38.991+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:46:45.332+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:46:45.658+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:46:45.682+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:45.682+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:46:45.691+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:45.691+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:46:45.694+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:45.694+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:46:45.695+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:46:45.695+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:46:45.708+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.241 seconds
[2025-05-07T09:47:17.378+0000] {processor.py:186} INFO - Started process (PID=3863) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:47:17.379+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:47:17.381+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:17.381+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:47:17.994+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:47:18.347+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:47:18.368+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:18.368+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:47:18.380+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:18.379+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:47:18.382+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:18.382+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:47:18.383+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:18.383+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:47:18.394+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.357 seconds
[2025-05-07T09:47:49.048+0000] {processor.py:186} INFO - Started process (PID=3988) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:47:49.049+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:47:49.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:49.051+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:47:57.575+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:47:58.682+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:47:58.826+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:58.826+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:47:58.837+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:58.837+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:47:58.840+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:58.839+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:47:58.840+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:47:58.840+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:47:58.853+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 10.822 seconds
[2025-05-07T09:48:32.500+0000] {processor.py:186} INFO - Started process (PID=4143) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:48:32.501+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:48:32.503+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:48:32.503+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:48:34.203+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:48:34.530+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:48:34.553+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:48:34.552+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:48:34.562+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:48:34.562+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:48:34.564+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:48:34.564+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:48:34.565+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:48:34.565+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:48:34.575+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.417 seconds
[2025-05-07T09:49:04.827+0000] {processor.py:186} INFO - Started process (PID=4266) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:49:04.839+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:49:04.841+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:49:04.841+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:49:11.225+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:49:11.535+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:49:11.556+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:49:11.555+0000] {dag.py:3211} INFO - Sync 3 DAGs
[2025-05-07T09:49:11.566+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:49:11.565+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T09:49:11.568+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:49:11.568+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T09:49:11.569+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:49:11.569+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T09:49:11.596+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.275 seconds
[2025-05-07T09:49:41.798+0000] {processor.py:186} INFO - Started process (PID=4394) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:49:41.814+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:49:41.816+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:49:41.816+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:49:48.049+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:50:08.825+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:50:08.825+0000] {timeout.py:68} ERROR - Process timed out, PID: 4394
[2025-05-07T09:50:26.842+0000] {processor.py:186} INFO - Started process (PID=4580) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:50:26.844+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:50:26.846+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:50:26.845+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:50:32.955+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:50:34.105+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:50:34.105+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:50:35.414+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:50:35.412+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f97decca8e <unknown>
#1 0x55f97d989b0b <unknown>
#2 0x55f97d9c04ea <unknown>
#3 0x55f97d9bbaef <unknown>
#4 0x55f97da0fb18 <unknown>
#5 0x55f97d9fd9b3 <unknown>
#6 0x55f97d9c7c59 <unknown>
#7 0x55f97d9c8a08 <unknown>
#8 0x55f97de9940a <unknown>
#9 0x55f97de9c85e <unknown>
#10 0x55f97de9c308 <unknown>
#11 0x55f97de9cce5 <unknown>
#12 0x55f97de82b7b <unknown>
#13 0x55f97de9d050 <unknown>
#14 0x55f97de6bae9 <unknown>
#15 0x55f97debbdf5 <unknown>
#16 0x55f97debbfdb <unknown>
#17 0x55f97decbc05 <unknown>
#18 0x7fe26b568134 <unknown>
[2025-05-07T09:50:35.415+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:50:35.432+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.601 seconds
[2025-05-07T09:50:47.909+0000] {processor.py:186} INFO - Started process (PID=4744) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:50:47.910+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:50:47.911+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:50:47.910+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:50:54.766+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:50:55.095+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:50:55.095+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:50:56.505+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:50:56.505+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5586f2ac8a8e <unknown>
#1 0x5586f2585b0b <unknown>
#2 0x5586f25bc4ea <unknown>
#3 0x5586f25b7aef <unknown>
#4 0x5586f260bb18 <unknown>
#5 0x5586f25f99b3 <unknown>
#6 0x5586f25c3c59 <unknown>
#7 0x5586f25c4a08 <unknown>
#8 0x5586f2a9540a <unknown>
#9 0x5586f2a9885e <unknown>
#10 0x5586f2a98308 <unknown>
#11 0x5586f2a98ce5 <unknown>
#12 0x5586f2a7eb7b <unknown>
#13 0x5586f2a99050 <unknown>
#14 0x5586f2a67ae9 <unknown>
#15 0x5586f2ab7df5 <unknown>
#16 0x5586f2ab7fdb <unknown>
#17 0x5586f2ac7c05 <unknown>
#18 0x7f6c905de134 <unknown>
[2025-05-07T09:50:56.506+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:50:56.520+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.119 seconds
[2025-05-07T09:51:27.143+0000] {processor.py:186} INFO - Started process (PID=4889) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:51:27.161+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:51:27.162+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:51:27.162+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:51:33.242+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:51:33.576+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:51:33.576+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:51:35.152+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:51:35.151+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55612657ca8e <unknown>
#1 0x556126039b0b <unknown>
#2 0x5561260704ea <unknown>
#3 0x55612606baef <unknown>
#4 0x5561260bfb18 <unknown>
#5 0x5561260ad9b3 <unknown>
#6 0x556126077c59 <unknown>
#7 0x556126078a08 <unknown>
#8 0x55612654940a <unknown>
#9 0x55612654c85e <unknown>
#10 0x55612654c308 <unknown>
#11 0x55612654cce5 <unknown>
#12 0x556126532b7b <unknown>
#13 0x55612654d050 <unknown>
#14 0x55612651bae9 <unknown>
#15 0x55612656bdf5 <unknown>
#16 0x55612656bfdb <unknown>
#17 0x55612657bc05 <unknown>
#18 0x7ffbbe432134 <unknown>
[2025-05-07T09:51:35.153+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:51:35.167+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.014 seconds
[2025-05-07T09:52:05.770+0000] {processor.py:186} INFO - Started process (PID=5040) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:52:05.771+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:52:05.772+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:52:05.772+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:52:17.867+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:52:12.393+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:52:12.393+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:52:13.955+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:52:13.954+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56326387ba8e <unknown>
#1 0x563263338b0b <unknown>
#2 0x56326336f4ea <unknown>
#3 0x56326336aaef <unknown>
#4 0x5632633beb18 <unknown>
#5 0x5632633ac9b3 <unknown>
#6 0x563263376c59 <unknown>
#7 0x563263377a08 <unknown>
#8 0x56326384840a <unknown>
#9 0x56326384b85e <unknown>
#10 0x56326384b308 <unknown>
#11 0x56326384bce5 <unknown>
#12 0x563263831b7b <unknown>
#13 0x56326384c050 <unknown>
#14 0x56326381aae9 <unknown>
#15 0x56326386adf5 <unknown>
#16 0x56326386afdb <unknown>
#17 0x56326387ac05 <unknown>
#18 0x7ff5d8529134 <unknown>
[2025-05-07T09:52:13.955+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:52:13.969+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.200 seconds
[2025-05-07T09:52:44.756+0000] {processor.py:186} INFO - Started process (PID=5197) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:52:44.760+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:52:44.762+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:52:44.762+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:52:51.655+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:52:51.994+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:52:51.994+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:52:52.906+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:52:52.905+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559ad7c85a8e <unknown>
#1 0x559ad7742b0b <unknown>
#2 0x559ad77794ea <unknown>
#3 0x559ad7774aef <unknown>
#4 0x559ad77c8b18 <unknown>
#5 0x559ad77b69b3 <unknown>
#6 0x559ad7780c59 <unknown>
#7 0x559ad7781a08 <unknown>
#8 0x559ad7c5240a <unknown>
#9 0x559ad7c5585e <unknown>
#10 0x559ad7c55308 <unknown>
#11 0x559ad7c55ce5 <unknown>
#12 0x559ad7c3bb7b <unknown>
#13 0x559ad7c56050 <unknown>
#14 0x559ad7c24ae9 <unknown>
#15 0x559ad7c74df5 <unknown>
#16 0x559ad7c74fdb <unknown>
#17 0x559ad7c84c05 <unknown>
#18 0x7fa1f5dd8134 <unknown>
[2025-05-07T09:52:52.907+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:52:52.922+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.173 seconds
[2025-05-07T09:53:23.416+0000] {processor.py:186} INFO - Started process (PID=5352) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:53:23.417+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:53:23.418+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:53:23.418+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:53:30.819+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:53:31.141+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:53:31.141+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:53:32.575+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:53:32.574+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5643ea364a8e <unknown>
#1 0x5643e9e21b0b <unknown>
#2 0x5643e9e584ea <unknown>
#3 0x5643e9e53aef <unknown>
#4 0x5643e9ea7b18 <unknown>
#5 0x5643e9e959b3 <unknown>
#6 0x5643e9e5fc59 <unknown>
#7 0x5643e9e60a08 <unknown>
#8 0x5643ea33140a <unknown>
#9 0x5643ea33485e <unknown>
#10 0x5643ea334308 <unknown>
#11 0x5643ea334ce5 <unknown>
#12 0x5643ea31ab7b <unknown>
#13 0x5643ea335050 <unknown>
#14 0x5643ea303ae9 <unknown>
#15 0x5643ea353df5 <unknown>
#16 0x5643ea353fdb <unknown>
#17 0x5643ea363c05 <unknown>
#18 0x7f92fbab1134 <unknown>
[2025-05-07T09:53:32.576+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:53:32.590+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.668 seconds
[2025-05-07T09:54:03.520+0000] {processor.py:186} INFO - Started process (PID=5505) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:54:03.531+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:54:03.532+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:54:03.532+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:54:10.105+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:54:10.412+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:54:10.412+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:54:11.864+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:54:11.863+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f8bf36da8e <unknown>
#1 0x55f8bee2ab0b <unknown>
#2 0x55f8bee614ea <unknown>
#3 0x55f8bee5caef <unknown>
#4 0x55f8beeb0b18 <unknown>
#5 0x55f8bee9e9b3 <unknown>
#6 0x55f8bee68c59 <unknown>
#7 0x55f8bee69a08 <unknown>
#8 0x55f8bf33a40a <unknown>
#9 0x55f8bf33d85e <unknown>
#10 0x55f8bf33d308 <unknown>
#11 0x55f8bf33dce5 <unknown>
#12 0x55f8bf323b7b <unknown>
#13 0x55f8bf33e050 <unknown>
#14 0x55f8bf30cae9 <unknown>
#15 0x55f8bf35cdf5 <unknown>
#16 0x55f8bf35cfdb <unknown>
#17 0x55f8bf36cc05 <unknown>
#18 0x7f81b1cfb134 <unknown>
[2025-05-07T09:54:11.864+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:54:11.879+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.872 seconds
[2025-05-07T09:54:41.924+0000] {processor.py:186} INFO - Started process (PID=5662) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:54:41.926+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:54:41.927+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:54:41.926+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:54:47.776+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:54:48.102+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:54:48.102+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:54:49.616+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:54:49.615+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55ee21f1da8e <unknown>
#1 0x55ee219dab0b <unknown>
#2 0x55ee21a114ea <unknown>
#3 0x55ee21a0caef <unknown>
#4 0x55ee21a60b18 <unknown>
#5 0x55ee21a4e9b3 <unknown>
#6 0x55ee21a18c59 <unknown>
#7 0x55ee21a19a08 <unknown>
#8 0x55ee21eea40a <unknown>
#9 0x55ee21eed85e <unknown>
#10 0x55ee21eed308 <unknown>
#11 0x55ee21eedce5 <unknown>
#12 0x55ee21ed3b7b <unknown>
#13 0x55ee21eee050 <unknown>
#14 0x55ee21ebcae9 <unknown>
#15 0x55ee21f0cdf5 <unknown>
#16 0x55ee21f0cfdb <unknown>
#17 0x55ee21f1cc05 <unknown>
#18 0x7fe70c725134 <unknown>
[2025-05-07T09:54:49.617+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:54:49.631+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.719 seconds
[2025-05-07T09:55:20.228+0000] {processor.py:186} INFO - Started process (PID=5814) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:55:20.230+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:55:20.231+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:55:20.231+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:55:26.851+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:55:27.166+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:55:27.166+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:55:28.212+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:55:28.210+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d5b5c84a8e <unknown>
#1 0x55d5b5741b0b <unknown>
#2 0x55d5b57784ea <unknown>
#3 0x55d5b5773aef <unknown>
#4 0x55d5b57c7b18 <unknown>
#5 0x55d5b57b59b3 <unknown>
#6 0x55d5b577fc59 <unknown>
#7 0x55d5b5780a08 <unknown>
#8 0x55d5b5c5140a <unknown>
#9 0x55d5b5c5485e <unknown>
#10 0x55d5b5c54308 <unknown>
#11 0x55d5b5c54ce5 <unknown>
#12 0x55d5b5c3ab7b <unknown>
#13 0x55d5b5c55050 <unknown>
#14 0x55d5b5c23ae9 <unknown>
#15 0x55d5b5c73df5 <unknown>
#16 0x55d5b5c73fdb <unknown>
#17 0x55d5b5c83c05 <unknown>
#18 0x7ff80914a134 <unknown>
[2025-05-07T09:55:28.213+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:55:28.227+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.008 seconds
[2025-05-07T09:56:03.246+0000] {processor.py:186} INFO - Started process (PID=5969) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:56:03.251+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:56:03.252+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:56:03.251+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:56:03.558+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:56:03.914+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:56:03.914+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:56:05.415+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:56:05.414+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56133d180a8e <unknown>
#1 0x56133cc3db0b <unknown>
#2 0x56133cc744ea <unknown>
#3 0x56133cc6faef <unknown>
#4 0x56133ccc3b18 <unknown>
#5 0x56133ccb19b3 <unknown>
#6 0x56133cc7bc59 <unknown>
#7 0x56133cc7ca08 <unknown>
#8 0x56133d14d40a <unknown>
#9 0x56133d15085e <unknown>
#10 0x56133d150308 <unknown>
#11 0x56133d150ce5 <unknown>
#12 0x56133d136b7b <unknown>
#13 0x56133d151050 <unknown>
#14 0x56133d11fae9 <unknown>
#15 0x56133d16fdf5 <unknown>
#16 0x56133d16ffdb <unknown>
#17 0x56133d17fc05 <unknown>
#18 0x7f76a1b3a134 <unknown>
[2025-05-07T09:56:05.416+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:56:05.431+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.535 seconds
[2025-05-07T09:56:36.418+0000] {processor.py:186} INFO - Started process (PID=6133) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:56:36.435+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:56:36.436+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:56:36.436+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:56:48.512+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:56:42.991+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:56:42.990+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:56:44.402+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:56:44.401+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55e22e673a8e <unknown>
#1 0x55e22e130b0b <unknown>
#2 0x55e22e1674ea <unknown>
#3 0x55e22e162aef <unknown>
#4 0x55e22e1b6b18 <unknown>
#5 0x55e22e1a49b3 <unknown>
#6 0x55e22e16ec59 <unknown>
#7 0x55e22e16fa08 <unknown>
#8 0x55e22e64040a <unknown>
#9 0x55e22e64385e <unknown>
#10 0x55e22e643308 <unknown>
#11 0x55e22e643ce5 <unknown>
#12 0x55e22e629b7b <unknown>
#13 0x55e22e644050 <unknown>
#14 0x55e22e612ae9 <unknown>
#15 0x55e22e662df5 <unknown>
#16 0x55e22e662fdb <unknown>
#17 0x55e22e672c05 <unknown>
#18 0x7f3eb5295134 <unknown>
[2025-05-07T09:56:44.402+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:56:44.415+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.999 seconds
[2025-05-07T09:57:14.481+0000] {processor.py:186} INFO - Started process (PID=6279) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:57:14.485+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:57:14.486+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:57:14.486+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:57:21.161+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:57:21.471+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:57:21.471+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:57:28.347+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:57:28.346+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5566caac1a8e <unknown>
#1 0x5566ca57eb0b <unknown>
#2 0x5566ca5b54ea <unknown>
#3 0x5566ca5b0aef <unknown>
#4 0x5566ca604b18 <unknown>
#5 0x5566ca5f29b3 <unknown>
#6 0x5566ca5bcc59 <unknown>
#7 0x5566ca5bda08 <unknown>
#8 0x5566caa8e40a <unknown>
#9 0x5566caa9185e <unknown>
#10 0x5566caa91308 <unknown>
#11 0x5566caa91ce5 <unknown>
#12 0x5566caa77b7b <unknown>
#13 0x5566caa92050 <unknown>
#14 0x5566caa60ae9 <unknown>
#15 0x5566caab0df5 <unknown>
#16 0x5566caab0fdb <unknown>
#17 0x5566caac0c05 <unknown>
#18 0x7f4eb00fa134 <unknown>
[2025-05-07T09:57:28.348+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:57:28.363+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.042 seconds
[2025-05-07T09:58:03.580+0000] {processor.py:186} INFO - Started process (PID=6438) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:58:03.581+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:58:03.582+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:58:03.582+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:58:04.972+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:58:05.221+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:58:05.220+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:58:06.672+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:58:06.671+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x563ec25b0a8e <unknown>
#1 0x563ec206db0b <unknown>
#2 0x563ec20a44ea <unknown>
#3 0x563ec209faef <unknown>
#4 0x563ec20f3b18 <unknown>
#5 0x563ec20e19b3 <unknown>
#6 0x563ec20abc59 <unknown>
#7 0x563ec20aca08 <unknown>
#8 0x563ec257d40a <unknown>
#9 0x563ec258085e <unknown>
#10 0x563ec2580308 <unknown>
#11 0x563ec2580ce5 <unknown>
#12 0x563ec2566b7b <unknown>
#13 0x563ec2581050 <unknown>
#14 0x563ec254fae9 <unknown>
#15 0x563ec259fdf5 <unknown>
#16 0x563ec259ffdb <unknown>
#17 0x563ec25afc05 <unknown>
#18 0x7fe2b3d6f134 <unknown>
[2025-05-07T09:58:06.672+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:58:06.693+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.466 seconds
[2025-05-07T09:58:37.235+0000] {processor.py:186} INFO - Started process (PID=6591) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:58:37.237+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:58:37.238+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:58:37.237+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:58:43.501+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:58:43.853+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:58:43.853+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:58:45.204+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:58:45.203+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5579a1b0aa8e <unknown>
#1 0x5579a15c7b0b <unknown>
#2 0x5579a15fe4ea <unknown>
#3 0x5579a15f9aef <unknown>
#4 0x5579a164db18 <unknown>
#5 0x5579a163b9b3 <unknown>
#6 0x5579a1605c59 <unknown>
#7 0x5579a1606a08 <unknown>
#8 0x5579a1ad740a <unknown>
#9 0x5579a1ada85e <unknown>
#10 0x5579a1ada308 <unknown>
#11 0x5579a1adace5 <unknown>
#12 0x5579a1ac0b7b <unknown>
#13 0x5579a1adb050 <unknown>
#14 0x5579a1aa9ae9 <unknown>
#15 0x5579a1af9df5 <unknown>
#16 0x5579a1af9fdb <unknown>
#17 0x5579a1b09c05 <unknown>
#18 0x7f3c59d88134 <unknown>
[2025-05-07T09:58:45.205+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:58:45.219+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.997 seconds
[2025-05-07T09:59:16.095+0000] {processor.py:186} INFO - Started process (PID=6745) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:59:16.098+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:59:16.100+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:59:16.099+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T09:59:22.241+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T09:59:22.545+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:59:22.545+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T09:59:23.559+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:59:23.558+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a146bbfa8e <unknown>
#1 0x55a14667cb0b <unknown>
#2 0x55a1466b34ea <unknown>
#3 0x55a1466aeaef <unknown>
#4 0x55a146702b18 <unknown>
#5 0x55a1466f09b3 <unknown>
#6 0x55a1466bac59 <unknown>
#7 0x55a1466bba08 <unknown>
#8 0x55a146b8c40a <unknown>
#9 0x55a146b8f85e <unknown>
#10 0x55a146b8f308 <unknown>
#11 0x55a146b8fce5 <unknown>
#12 0x55a146b75b7b <unknown>
#13 0x55a146b90050 <unknown>
#14 0x55a146b5eae9 <unknown>
#15 0x55a146baedf5 <unknown>
#16 0x55a146baefdb <unknown>
#17 0x55a146bbec05 <unknown>
#18 0x7fd4d22db134 <unknown>
[2025-05-07T09:59:23.560+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T09:59:23.574+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.483 seconds
[2025-05-07T09:59:54.015+0000] {processor.py:186} INFO - Started process (PID=6890) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T09:59:54.034+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T09:59:54.035+0000] {logging_mixin.py:190} INFO - [2025-05-07T09:59:54.035+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:00:00.960+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:00:01.287+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:00:01.287+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:00:02.782+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:00:02.764+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56519815aa8e <unknown>
#1 0x565197c17b0b <unknown>
#2 0x565197c4e4ea <unknown>
#3 0x565197c49aef <unknown>
#4 0x565197c9db18 <unknown>
#5 0x565197c8b9b3 <unknown>
#6 0x565197c55c59 <unknown>
#7 0x565197c56a08 <unknown>
#8 0x56519812740a <unknown>
#9 0x56519812a85e <unknown>
#10 0x56519812a308 <unknown>
#11 0x56519812ace5 <unknown>
#12 0x565198110b7b <unknown>
#13 0x56519812b050 <unknown>
#14 0x5651980f9ae9 <unknown>
#15 0x565198149df5 <unknown>
#16 0x565198149fdb <unknown>
#17 0x565198159c05 <unknown>
#18 0x7fa9f6921134 <unknown>
[2025-05-07T10:00:02.783+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:00:02.798+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.287 seconds
[2025-05-07T10:00:33.070+0000] {processor.py:186} INFO - Started process (PID=7041) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:00:33.071+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:00:33.072+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:00:33.072+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:00:39.309+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:00:39.648+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:00:39.647+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:00:41.268+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:00:41.267+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559d80d27a8e <unknown>
#1 0x559d807e4b0b <unknown>
#2 0x559d8081b4ea <unknown>
#3 0x559d80816aef <unknown>
#4 0x559d8086ab18 <unknown>
#5 0x559d808589b3 <unknown>
#6 0x559d80822c59 <unknown>
#7 0x559d80823a08 <unknown>
#8 0x559d80cf440a <unknown>
#9 0x559d80cf785e <unknown>
#10 0x559d80cf7308 <unknown>
#11 0x559d80cf7ce5 <unknown>
#12 0x559d80cddb7b <unknown>
#13 0x559d80cf8050 <unknown>
#14 0x559d80cc6ae9 <unknown>
#15 0x559d80d16df5 <unknown>
#16 0x559d80d16fdb <unknown>
#17 0x559d80d26c05 <unknown>
#18 0x7fb8dd060134 <unknown>
[2025-05-07T10:00:41.268+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:00:41.282+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.713 seconds
[2025-05-07T10:01:13.718+0000] {processor.py:186} INFO - Started process (PID=7202) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:01:13.719+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:01:13.720+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:01:13.720+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:01:14.668+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:01:15.030+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:01:15.029+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:01:16.631+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:01:16.631+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55786fceaa8e <unknown>
#1 0x55786f7a7b0b <unknown>
#2 0x55786f7de4ea <unknown>
#3 0x55786f7d9aef <unknown>
#4 0x55786f82db18 <unknown>
#5 0x55786f81b9b3 <unknown>
#6 0x55786f7e5c59 <unknown>
#7 0x55786f7e6a08 <unknown>
#8 0x55786fcb740a <unknown>
#9 0x55786fcba85e <unknown>
#10 0x55786fcba308 <unknown>
#11 0x55786fcbace5 <unknown>
#12 0x55786fca0b7b <unknown>
#13 0x55786fcbb050 <unknown>
#14 0x55786fc89ae9 <unknown>
#15 0x55786fcd9df5 <unknown>
#16 0x55786fcd9fdb <unknown>
#17 0x55786fce9c05 <unknown>
#18 0x7f73fb837134 <unknown>
[2025-05-07T10:01:16.632+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:01:16.645+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.270 seconds
[2025-05-07T10:01:48.922+0000] {processor.py:186} INFO - Started process (PID=7351) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:01:48.923+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:01:48.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:01:48.924+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:01:49.197+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:01:49.521+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:01:49.521+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:01:51.084+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:01:51.082+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c853d7ca8e <unknown>
#1 0x55c853839b0b <unknown>
#2 0x55c8538704ea <unknown>
#3 0x55c85386baef <unknown>
#4 0x55c8538bfb18 <unknown>
#5 0x55c8538ad9b3 <unknown>
#6 0x55c853877c59 <unknown>
#7 0x55c853878a08 <unknown>
#8 0x55c853d4940a <unknown>
#9 0x55c853d4c85e <unknown>
#10 0x55c853d4c308 <unknown>
#11 0x55c853d4cce5 <unknown>
#12 0x55c853d32b7b <unknown>
#13 0x55c853d4d050 <unknown>
#14 0x55c853d1bae9 <unknown>
#15 0x55c853d6bdf5 <unknown>
#16 0x55c853d6bfdb <unknown>
#17 0x55c853d7bc05 <unknown>
#18 0x7f22e2c7c134 <unknown>
[2025-05-07T10:01:51.084+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:01:51.099+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.529 seconds
[2025-05-07T10:02:21.800+0000] {processor.py:186} INFO - Started process (PID=7505) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:02:21.801+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:02:21.802+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:02:21.802+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:02:28.669+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:02:28.882+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:02:28.882+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:02:30.285+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:02:30.284+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f333221a8e <unknown>
#1 0x55f332cdeb0b <unknown>
#2 0x55f332d154ea <unknown>
#3 0x55f332d10aef <unknown>
#4 0x55f332d64b18 <unknown>
#5 0x55f332d529b3 <unknown>
#6 0x55f332d1cc59 <unknown>
#7 0x55f332d1da08 <unknown>
#8 0x55f3331ee40a <unknown>
#9 0x55f3331f185e <unknown>
#10 0x55f3331f1308 <unknown>
#11 0x55f3331f1ce5 <unknown>
#12 0x55f3331d7b7b <unknown>
#13 0x55f3331f2050 <unknown>
#14 0x55f3331c0ae9 <unknown>
#15 0x55f333210df5 <unknown>
#16 0x55f333210fdb <unknown>
#17 0x55f333220c05 <unknown>
#18 0x7f027652a134 <unknown>
[2025-05-07T10:02:30.286+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:02:30.301+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.514 seconds
[2025-05-07T10:03:00.490+0000] {processor.py:186} INFO - Started process (PID=7659) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:03:00.491+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:03:00.492+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:03:00.492+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:03:07.018+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:03:07.333+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:03:07.333+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:03:08.344+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:03:08.344+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5619e2433a8e <unknown>
#1 0x5619e1ef0b0b <unknown>
#2 0x5619e1f274ea <unknown>
#3 0x5619e1f22aef <unknown>
#4 0x5619e1f76b18 <unknown>
#5 0x5619e1f649b3 <unknown>
#6 0x5619e1f2ec59 <unknown>
#7 0x5619e1f2fa08 <unknown>
#8 0x5619e240040a <unknown>
#9 0x5619e240385e <unknown>
#10 0x5619e2403308 <unknown>
#11 0x5619e2403ce5 <unknown>
#12 0x5619e23e9b7b <unknown>
#13 0x5619e2404050 <unknown>
#14 0x5619e23d2ae9 <unknown>
#15 0x5619e2422df5 <unknown>
#16 0x5619e2422fdb <unknown>
#17 0x5619e2432c05 <unknown>
#18 0x7f61010ee134 <unknown>
[2025-05-07T10:03:08.345+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:03:08.360+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.872 seconds
[2025-05-07T10:03:38.523+0000] {processor.py:186} INFO - Started process (PID=7803) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:03:38.540+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:03:38.541+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:03:38.540+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:03:44.758+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:03:44.967+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:03:44.966+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:03:46.460+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:03:46.459+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55fc7b6c3a8e <unknown>
#1 0x55fc7b180b0b <unknown>
#2 0x55fc7b1b74ea <unknown>
#3 0x55fc7b1b2aef <unknown>
#4 0x55fc7b206b18 <unknown>
#5 0x55fc7b1f49b3 <unknown>
#6 0x55fc7b1bec59 <unknown>
#7 0x55fc7b1bfa08 <unknown>
#8 0x55fc7b69040a <unknown>
#9 0x55fc7b69385e <unknown>
#10 0x55fc7b693308 <unknown>
#11 0x55fc7b693ce5 <unknown>
#12 0x55fc7b679b7b <unknown>
#13 0x55fc7b694050 <unknown>
#14 0x55fc7b662ae9 <unknown>
#15 0x55fc7b6b2df5 <unknown>
#16 0x55fc7b6b2fdb <unknown>
#17 0x55fc7b6c2c05 <unknown>
#18 0x7fded38b7134 <unknown>
[2025-05-07T10:03:46.461+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:03:46.488+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.967 seconds
[2025-05-07T10:04:19.178+0000] {processor.py:186} INFO - Started process (PID=7961) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:04:19.186+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:04:19.187+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:04:19.186+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:04:19.503+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:04:19.898+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:04:19.898+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:04:21.575+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:04:21.574+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5571013d1a8e <unknown>
#1 0x557100e8eb0b <unknown>
#2 0x557100ec54ea <unknown>
#3 0x557100ec0aef <unknown>
#4 0x557100f14b18 <unknown>
#5 0x557100f029b3 <unknown>
#6 0x557100eccc59 <unknown>
#7 0x557100ecda08 <unknown>
#8 0x55710139e40a <unknown>
#9 0x5571013a185e <unknown>
#10 0x5571013a1308 <unknown>
#11 0x5571013a1ce5 <unknown>
#12 0x557101387b7b <unknown>
#13 0x5571013a2050 <unknown>
#14 0x557101370ae9 <unknown>
#15 0x5571013c0df5 <unknown>
#16 0x5571013c0fdb <unknown>
#17 0x5571013d0c05 <unknown>
#18 0x7fb58ffc4134 <unknown>
[2025-05-07T10:04:21.576+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:04:21.592+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.767 seconds
[2025-05-07T10:04:51.611+0000] {processor.py:186} INFO - Started process (PID=8114) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:04:51.612+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:04:51.613+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:04:51.612+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:04:58.664+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:04:58.867+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:04:58.866+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:05:00.405+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:05:00.404+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56367fdc7a8e <unknown>
#1 0x56367f884b0b <unknown>
#2 0x56367f8bb4ea <unknown>
#3 0x56367f8b6aef <unknown>
#4 0x56367f90ab18 <unknown>
#5 0x56367f8f89b3 <unknown>
#6 0x56367f8c2c59 <unknown>
#7 0x56367f8c3a08 <unknown>
#8 0x56367fd9440a <unknown>
#9 0x56367fd9785e <unknown>
#10 0x56367fd97308 <unknown>
#11 0x56367fd97ce5 <unknown>
#12 0x56367fd7db7b <unknown>
#13 0x56367fd98050 <unknown>
#14 0x56367fd66ae9 <unknown>
#15 0x56367fdb6df5 <unknown>
#16 0x56367fdb6fdb <unknown>
#17 0x56367fdc6c05 <unknown>
#18 0x7f15abebe134 <unknown>
[2025-05-07T10:05:00.405+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:05:00.435+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.829 seconds
[2025-05-07T10:05:30.801+0000] {processor.py:186} INFO - Started process (PID=8269) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:05:30.803+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:05:30.803+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:05:30.803+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:05:37.686+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:05:37.908+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:05:37.908+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:05:38.882+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:05:38.880+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561e6eacba8e <unknown>
#1 0x561e6e588b0b <unknown>
#2 0x561e6e5bf4ea <unknown>
#3 0x561e6e5baaef <unknown>
#4 0x561e6e60eb18 <unknown>
#5 0x561e6e5fc9b3 <unknown>
#6 0x561e6e5c6c59 <unknown>
#7 0x561e6e5c7a08 <unknown>
#8 0x561e6ea9840a <unknown>
#9 0x561e6ea9b85e <unknown>
#10 0x561e6ea9b308 <unknown>
#11 0x561e6ea9bce5 <unknown>
#12 0x561e6ea81b7b <unknown>
#13 0x561e6ea9c050 <unknown>
#14 0x561e6ea6aae9 <unknown>
#15 0x561e6eabadf5 <unknown>
#16 0x561e6eabafdb <unknown>
#17 0x561e6eacac05 <unknown>
#18 0x7fac650f3134 <unknown>
[2025-05-07T10:05:38.882+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:05:38.896+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.099 seconds
[2025-05-07T10:06:09.826+0000] {processor.py:186} INFO - Started process (PID=8431) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:06:09.827+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:06:09.829+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:06:09.829+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:06:16.169+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:06:16.401+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:06:16.401+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:06:17.857+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:06:17.856+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x564acd534a8e <unknown>
#1 0x564accff1b0b <unknown>
#2 0x564acd0284ea <unknown>
#3 0x564acd023aef <unknown>
#4 0x564acd077b18 <unknown>
#5 0x564acd0659b3 <unknown>
#6 0x564acd02fc59 <unknown>
#7 0x564acd030a08 <unknown>
#8 0x564acd50140a <unknown>
#9 0x564acd50485e <unknown>
#10 0x564acd504308 <unknown>
#11 0x564acd504ce5 <unknown>
#12 0x564acd4eab7b <unknown>
#13 0x564acd505050 <unknown>
#14 0x564acd4d3ae9 <unknown>
#15 0x564acd523df5 <unknown>
#16 0x564acd523fdb <unknown>
#17 0x564acd533c05 <unknown>
#18 0x7f5f58e14134 <unknown>
[2025-05-07T10:06:17.858+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:06:17.880+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.564 seconds
[2025-05-07T10:06:49.358+0000] {processor.py:186} INFO - Started process (PID=8589) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:06:49.358+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:06:49.359+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:06:49.359+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:06:49.828+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:06:50.033+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:06:50.032+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:06:51.579+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:06:51.578+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55c922ef2a8e <unknown>
#1 0x55c9229afb0b <unknown>
#2 0x55c9229e64ea <unknown>
#3 0x55c9229e1aef <unknown>
#4 0x55c922a35b18 <unknown>
#5 0x55c922a239b3 <unknown>
#6 0x55c9229edc59 <unknown>
#7 0x55c9229eea08 <unknown>
#8 0x55c922ebf40a <unknown>
#9 0x55c922ec285e <unknown>
#10 0x55c922ec2308 <unknown>
#11 0x55c922ec2ce5 <unknown>
#12 0x55c922ea8b7b <unknown>
#13 0x55c922ec3050 <unknown>
#14 0x55c922e91ae9 <unknown>
#15 0x55c922ee1df5 <unknown>
#16 0x55c922ee1fdb <unknown>
#17 0x55c922ef1c05 <unknown>
#18 0x7f589c57f134 <unknown>
[2025-05-07T10:06:51.580+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:06:51.597+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.587 seconds
[2025-05-07T10:07:22.309+0000] {processor.py:186} INFO - Started process (PID=8744) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:07:22.310+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:07:22.311+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:07:22.311+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:07:34.435+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:07:28.812+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:07:28.811+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:07:30.376+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:07:30.370+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56027b269a8e <unknown>
#1 0x56027ad26b0b <unknown>
#2 0x56027ad5d4ea <unknown>
#3 0x56027ad58aef <unknown>
#4 0x56027adacb18 <unknown>
#5 0x56027ad9a9b3 <unknown>
#6 0x56027ad64c59 <unknown>
#7 0x56027ad65a08 <unknown>
#8 0x56027b23640a <unknown>
#9 0x56027b23985e <unknown>
#10 0x56027b239308 <unknown>
#11 0x56027b239ce5 <unknown>
#12 0x56027b21fb7b <unknown>
#13 0x56027b23a050 <unknown>
#14 0x56027b208ae9 <unknown>
#15 0x56027b258df5 <unknown>
#16 0x56027b258fdb <unknown>
#17 0x56027b268c05 <unknown>
#18 0x7f944c52c134 <unknown>
[2025-05-07T10:07:30.377+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:07:30.395+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.089 seconds
[2025-05-07T10:08:01.177+0000] {processor.py:186} INFO - Started process (PID=8906) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:08:01.180+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:08:01.181+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:08:01.181+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:08:08.159+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:08:08.358+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:08:08.358+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:08:09.372+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:08:09.371+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55d813ceda8e <unknown>
#1 0x55d8137aab0b <unknown>
#2 0x55d8137e14ea <unknown>
#3 0x55d8137dcaef <unknown>
#4 0x55d813830b18 <unknown>
#5 0x55d81381e9b3 <unknown>
#6 0x55d8137e8c59 <unknown>
#7 0x55d8137e9a08 <unknown>
#8 0x55d813cba40a <unknown>
#9 0x55d813cbd85e <unknown>
#10 0x55d813cbd308 <unknown>
#11 0x55d813cbdce5 <unknown>
#12 0x55d813ca3b7b <unknown>
#13 0x55d813cbe050 <unknown>
#14 0x55d813c8cae9 <unknown>
#15 0x55d813cdcdf5 <unknown>
#16 0x55d813cdcfdb <unknown>
#17 0x55d813cecc05 <unknown>
#18 0x7f771a8f4134 <unknown>
[2025-05-07T10:08:09.373+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:08:09.387+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.216 seconds
[2025-05-07T10:08:39.479+0000] {processor.py:186} INFO - Started process (PID=9057) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:08:39.480+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:08:39.481+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:08:39.481+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:08:40.607+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:08:40.810+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:08:40.810+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:08:42.261+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:08:42.260+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55cd29091a8e <unknown>
#1 0x55cd28b4eb0b <unknown>
#2 0x55cd28b854ea <unknown>
#3 0x55cd28b80aef <unknown>
#4 0x55cd28bd4b18 <unknown>
#5 0x55cd28bc29b3 <unknown>
#6 0x55cd28b8cc59 <unknown>
#7 0x55cd28b8da08 <unknown>
#8 0x55cd2905e40a <unknown>
#9 0x55cd2906185e <unknown>
#10 0x55cd29061308 <unknown>
#11 0x55cd29061ce5 <unknown>
#12 0x55cd29047b7b <unknown>
#13 0x55cd29062050 <unknown>
#14 0x55cd29030ae9 <unknown>
#15 0x55cd29080df5 <unknown>
#16 0x55cd29080fdb <unknown>
#17 0x55cd29090c05 <unknown>
#18 0x7f4867484134 <unknown>
[2025-05-07T10:08:42.261+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:08:42.276+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.150 seconds
[2025-05-07T10:09:13.043+0000] {processor.py:186} INFO - Started process (PID=9202) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:09:13.060+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:09:13.061+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:09:13.061+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:09:19.376+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:09:19.623+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:09:19.623+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:09:21.237+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:09:21.236+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x562b09e25a8e <unknown>
#1 0x562b098e2b0b <unknown>
#2 0x562b099194ea <unknown>
#3 0x562b09914aef <unknown>
#4 0x562b09968b18 <unknown>
#5 0x562b099569b3 <unknown>
#6 0x562b09920c59 <unknown>
#7 0x562b09921a08 <unknown>
#8 0x562b09df240a <unknown>
#9 0x562b09df585e <unknown>
#10 0x562b09df5308 <unknown>
#11 0x562b09df5ce5 <unknown>
#12 0x562b09ddbb7b <unknown>
#13 0x562b09df6050 <unknown>
#14 0x562b09dc4ae9 <unknown>
#15 0x562b09e14df5 <unknown>
#16 0x562b09e14fdb <unknown>
#17 0x562b09e24c05 <unknown>
#18 0x7fc5e587f134 <unknown>
[2025-05-07T10:09:21.238+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:09:21.252+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.215 seconds
[2025-05-07T10:09:54.665+0000] {processor.py:186} INFO - Started process (PID=9358) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:09:54.675+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:09:54.676+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:09:54.676+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:09:55.706+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:09:55.894+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:09:55.894+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:09:57.468+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:09:57.467+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x556bd00c3a8e <unknown>
#1 0x556bcfb80b0b <unknown>
#2 0x556bcfbb74ea <unknown>
#3 0x556bcfbb2aef <unknown>
#4 0x556bcfc06b18 <unknown>
#5 0x556bcfbf49b3 <unknown>
#6 0x556bcfbbec59 <unknown>
#7 0x556bcfbbfa08 <unknown>
#8 0x556bd009040a <unknown>
#9 0x556bd009385e <unknown>
#10 0x556bd0093308 <unknown>
#11 0x556bd0093ce5 <unknown>
#12 0x556bd0079b7b <unknown>
#13 0x556bd0094050 <unknown>
#14 0x556bd0062ae9 <unknown>
#15 0x556bd00b2df5 <unknown>
#16 0x556bd00b2fdb <unknown>
#17 0x556bd00c2c05 <unknown>
#18 0x7ff448ad6134 <unknown>
[2025-05-07T10:09:57.469+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:09:57.482+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.173 seconds
[2025-05-07T10:10:28.504+0000] {processor.py:186} INFO - Started process (PID=9518) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:10:28.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:10:28.508+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:10:28.508+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:10:34.917+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:10:35.148+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:10:35.148+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:10:36.689+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:10:36.688+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x559562737a8e <unknown>
#1 0x5595621f4b0b <unknown>
#2 0x55956222b4ea <unknown>
#3 0x559562226aef <unknown>
#4 0x55956227ab18 <unknown>
#5 0x5595622689b3 <unknown>
#6 0x559562232c59 <unknown>
#7 0x559562233a08 <unknown>
#8 0x55956270440a <unknown>
#9 0x55956270785e <unknown>
#10 0x559562707308 <unknown>
#11 0x559562707ce5 <unknown>
#12 0x5595626edb7b <unknown>
#13 0x559562708050 <unknown>
#14 0x5595626d6ae9 <unknown>
#15 0x559562726df5 <unknown>
#16 0x559562726fdb <unknown>
#17 0x559562736c05 <unknown>
#18 0x7fdfcd730134 <unknown>
[2025-05-07T10:10:36.689+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:10:36.703+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.210 seconds
[2025-05-07T10:11:07.765+0000] {processor.py:186} INFO - Started process (PID=9677) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:11:07.769+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:11:07.770+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:11:07.769+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:11:14.190+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:11:14.372+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:11:14.372+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:11:16.025+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:11:16.025+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5645ea472a8e <unknown>
#1 0x5645e9f2fb0b <unknown>
#2 0x5645e9f664ea <unknown>
#3 0x5645e9f61aef <unknown>
#4 0x5645e9fb5b18 <unknown>
#5 0x5645e9fa39b3 <unknown>
#6 0x5645e9f6dc59 <unknown>
#7 0x5645e9f6ea08 <unknown>
#8 0x5645ea43f40a <unknown>
#9 0x5645ea44285e <unknown>
#10 0x5645ea442308 <unknown>
#11 0x5645ea442ce5 <unknown>
#12 0x5645ea428b7b <unknown>
#13 0x5645ea443050 <unknown>
#14 0x5645ea411ae9 <unknown>
#15 0x5645ea461df5 <unknown>
#16 0x5645ea461fdb <unknown>
#17 0x5645ea471c05 <unknown>
#18 0x7f7ad836f134 <unknown>
[2025-05-07T10:11:16.026+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:11:16.040+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.294 seconds
[2025-05-07T10:11:46.621+0000] {processor.py:186} INFO - Started process (PID=9825) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:11:46.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:11:46.624+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:11:46.623+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:11:52.885+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:11:53.108+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:11:53.108+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:11:54.160+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:11:54.159+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561cb02a6a8e <unknown>
#1 0x561cafd63b0b <unknown>
#2 0x561cafd9a4ea <unknown>
#3 0x561cafd95aef <unknown>
#4 0x561cafde9b18 <unknown>
#5 0x561cafdd79b3 <unknown>
#6 0x561cafda1c59 <unknown>
#7 0x561cafda2a08 <unknown>
#8 0x561cb027340a <unknown>
#9 0x561cb027685e <unknown>
#10 0x561cb0276308 <unknown>
#11 0x561cb0276ce5 <unknown>
#12 0x561cb025cb7b <unknown>
#13 0x561cb0277050 <unknown>
#14 0x561cb0245ae9 <unknown>
#15 0x561cb0295df5 <unknown>
#16 0x561cb0295fdb <unknown>
#17 0x561cb02a5c05 <unknown>
#18 0x7f3a46202134 <unknown>
[2025-05-07T10:11:54.161+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:11:54.177+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.560 seconds
[2025-05-07T10:12:30.013+0000] {processor.py:186} INFO - Started process (PID=9984) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:12:30.014+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:12:30.015+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:12:30.015+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:12:31.024+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:12:31.223+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:12:31.223+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:12:32.730+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:12:32.726+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x555bd610da8e <unknown>
#1 0x555bd5bcab0b <unknown>
#2 0x555bd5c014ea <unknown>
#3 0x555bd5bfcaef <unknown>
#4 0x555bd5c50b18 <unknown>
#5 0x555bd5c3e9b3 <unknown>
#6 0x555bd5c08c59 <unknown>
#7 0x555bd5c09a08 <unknown>
#8 0x555bd60da40a <unknown>
#9 0x555bd60dd85e <unknown>
#10 0x555bd60dd308 <unknown>
#11 0x555bd60ddce5 <unknown>
#12 0x555bd60c3b7b <unknown>
#13 0x555bd60de050 <unknown>
#14 0x555bd60acae9 <unknown>
#15 0x555bd60fcdf5 <unknown>
#16 0x555bd60fcfdb <unknown>
#17 0x555bd610cc05 <unknown>
#18 0x7fa7088bc134 <unknown>
[2025-05-07T10:12:32.731+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:12:32.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.082 seconds
[2025-05-07T10:13:03.388+0000] {processor.py:186} INFO - Started process (PID=10138) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:13:03.390+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:13:03.390+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:13:03.390+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:13:15.031+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:13:09.366+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:13:09.365+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:13:10.870+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:13:10.869+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a8bbc6ea8e <unknown>
#1 0x55a8bb72bb0b <unknown>
#2 0x55a8bb7624ea <unknown>
#3 0x55a8bb75daef <unknown>
#4 0x55a8bb7b1b18 <unknown>
#5 0x55a8bb79f9b3 <unknown>
#6 0x55a8bb769c59 <unknown>
#7 0x55a8bb76aa08 <unknown>
#8 0x55a8bbc3b40a <unknown>
#9 0x55a8bbc3e85e <unknown>
#10 0x55a8bbc3e308 <unknown>
#11 0x55a8bbc3ece5 <unknown>
#12 0x55a8bbc24b7b <unknown>
#13 0x55a8bbc3f050 <unknown>
#14 0x55a8bbc0dae9 <unknown>
#15 0x55a8bbc5ddf5 <unknown>
#16 0x55a8bbc5dfdb <unknown>
#17 0x55a8bbc6dc05 <unknown>
#18 0x7f1bee588134 <unknown>
[2025-05-07T10:13:10.871+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:13:10.884+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.502 seconds
[2025-05-07T10:13:44.974+0000] {processor.py:186} INFO - Started process (PID=10288) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:13:44.979+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:13:44.980+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:13:44.980+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:13:45.467+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:13:45.664+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:13:45.664+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:13:47.208+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:13:47.207+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x557ce5253a8e <unknown>
#1 0x557ce4d10b0b <unknown>
#2 0x557ce4d474ea <unknown>
#3 0x557ce4d42aef <unknown>
#4 0x557ce4d96b18 <unknown>
#5 0x557ce4d849b3 <unknown>
#6 0x557ce4d4ec59 <unknown>
#7 0x557ce4d4fa08 <unknown>
#8 0x557ce522040a <unknown>
#9 0x557ce522385e <unknown>
#10 0x557ce5223308 <unknown>
#11 0x557ce5223ce5 <unknown>
#12 0x557ce5209b7b <unknown>
#13 0x557ce5224050 <unknown>
#14 0x557ce51f2ae9 <unknown>
#15 0x557ce5242df5 <unknown>
#16 0x557ce5242fdb <unknown>
#17 0x557ce5252c05 <unknown>
#18 0x7f01d7b14134 <unknown>
[2025-05-07T10:13:47.209+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:13:47.229+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.613 seconds
[2025-05-07T10:14:17.560+0000] {processor.py:186} INFO - Started process (PID=10448) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:14:17.563+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:14:17.564+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:14:17.564+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:14:23.860+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:14:24.091+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:14:24.091+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:14:25.104+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:14:25.101+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55aec7ed8a8e <unknown>
#1 0x55aec7995b0b <unknown>
#2 0x55aec79cc4ea <unknown>
#3 0x55aec79c7aef <unknown>
#4 0x55aec7a1bb18 <unknown>
#5 0x55aec7a099b3 <unknown>
#6 0x55aec79d3c59 <unknown>
#7 0x55aec79d4a08 <unknown>
#8 0x55aec7ea540a <unknown>
#9 0x55aec7ea885e <unknown>
#10 0x55aec7ea8308 <unknown>
#11 0x55aec7ea8ce5 <unknown>
#12 0x55aec7e8eb7b <unknown>
#13 0x55aec7ea9050 <unknown>
#14 0x55aec7e77ae9 <unknown>
#15 0x55aec7ec7df5 <unknown>
#16 0x55aec7ec7fdb <unknown>
#17 0x55aec7ed7c05 <unknown>
#18 0x7f1449b4d134 <unknown>
[2025-05-07T10:14:25.105+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:14:25.118+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.568 seconds
[2025-05-07T10:14:55.732+0000] {processor.py:186} INFO - Started process (PID=10610) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:14:55.733+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:14:55.734+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:14:55.734+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:15:01.987+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:15:02.211+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:15:02.211+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:15:03.672+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:15:03.672+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55a176766a8e <unknown>
#1 0x55a176223b0b <unknown>
#2 0x55a17625a4ea <unknown>
#3 0x55a176255aef <unknown>
#4 0x55a1762a9b18 <unknown>
#5 0x55a1762979b3 <unknown>
#6 0x55a176261c59 <unknown>
#7 0x55a176262a08 <unknown>
#8 0x55a17673340a <unknown>
#9 0x55a17673685e <unknown>
#10 0x55a176736308 <unknown>
#11 0x55a176736ce5 <unknown>
#12 0x55a17671cb7b <unknown>
#13 0x55a176737050 <unknown>
#14 0x55a176705ae9 <unknown>
#15 0x55a176755df5 <unknown>
#16 0x55a176755fdb <unknown>
#17 0x55a176765c05 <unknown>
#18 0x7f9a9c449134 <unknown>
[2025-05-07T10:15:03.673+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:15:03.700+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.472 seconds
[2025-05-07T10:15:35.101+0000] {processor.py:186} INFO - Started process (PID=10760) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:15:35.115+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:15:35.116+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:15:35.116+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:15:36.073+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:15:36.283+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:15:36.283+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:15:37.745+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:15:37.745+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5626dd895a8e <unknown>
#1 0x5626dd352b0b <unknown>
#2 0x5626dd3894ea <unknown>
#3 0x5626dd384aef <unknown>
#4 0x5626dd3d8b18 <unknown>
#5 0x5626dd3c69b3 <unknown>
#6 0x5626dd390c59 <unknown>
#7 0x5626dd391a08 <unknown>
#8 0x5626dd86240a <unknown>
#9 0x5626dd86585e <unknown>
#10 0x5626dd865308 <unknown>
#11 0x5626dd865ce5 <unknown>
#12 0x5626dd84bb7b <unknown>
#13 0x5626dd866050 <unknown>
#14 0x5626dd834ae9 <unknown>
#15 0x5626dd884df5 <unknown>
#16 0x5626dd884fdb <unknown>
#17 0x5626dd894c05 <unknown>
#18 0x7fb3f3f8a134 <unknown>
[2025-05-07T10:15:37.746+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:15:37.759+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.016 seconds
[2025-05-07T10:16:10.375+0000] {processor.py:186} INFO - Started process (PID=10910) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:16:10.376+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:16:10.377+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:16:10.377+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:16:10.937+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:16:11.203+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:16:11.203+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:16:12.827+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:16:12.826+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55cfb9238a8e <unknown>
#1 0x55cfb8cf5b0b <unknown>
#2 0x55cfb8d2c4ea <unknown>
#3 0x55cfb8d27aef <unknown>
#4 0x55cfb8d7bb18 <unknown>
#5 0x55cfb8d699b3 <unknown>
#6 0x55cfb8d33c59 <unknown>
#7 0x55cfb8d34a08 <unknown>
#8 0x55cfb920540a <unknown>
#9 0x55cfb920885e <unknown>
#10 0x55cfb9208308 <unknown>
#11 0x55cfb9208ce5 <unknown>
#12 0x55cfb91eeb7b <unknown>
#13 0x55cfb9209050 <unknown>
#14 0x55cfb91d7ae9 <unknown>
#15 0x55cfb9227df5 <unknown>
#16 0x55cfb9227fdb <unknown>
#17 0x55cfb9237c05 <unknown>
#18 0x7fb459902134 <unknown>
[2025-05-07T10:16:12.827+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:16:12.842+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.820 seconds
[2025-05-07T10:16:45.275+0000] {processor.py:186} INFO - Started process (PID=11063) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:16:45.276+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:16:45.277+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:16:45.277+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:16:46.214+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:16:46.458+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:16:46.458+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:16:47.961+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:16:47.960+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55fff7a21a8e <unknown>
#1 0x55fff74deb0b <unknown>
#2 0x55fff75154ea <unknown>
#3 0x55fff7510aef <unknown>
#4 0x55fff7564b18 <unknown>
#5 0x55fff75529b3 <unknown>
#6 0x55fff751cc59 <unknown>
#7 0x55fff751da08 <unknown>
#8 0x55fff79ee40a <unknown>
#9 0x55fff79f185e <unknown>
#10 0x55fff79f1308 <unknown>
#11 0x55fff79f1ce5 <unknown>
#12 0x55fff79d7b7b <unknown>
#13 0x55fff79f2050 <unknown>
#14 0x55fff79c0ae9 <unknown>
#15 0x55fff7a10df5 <unknown>
#16 0x55fff7a10fdb <unknown>
#17 0x55fff7a20c05 <unknown>
#18 0x7f2a58195134 <unknown>
[2025-05-07T10:16:47.962+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:16:47.977+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.057 seconds
[2025-05-07T10:17:20.442+0000] {processor.py:186} INFO - Started process (PID=11219) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:17:20.444+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:17:20.445+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:17:20.445+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:17:21.377+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:17:21.603+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:17:21.602+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:17:23.164+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:17:23.163+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55f054d82a8e <unknown>
#1 0x55f05483fb0b <unknown>
#2 0x55f0548764ea <unknown>
#3 0x55f054871aef <unknown>
#4 0x55f0548c5b18 <unknown>
#5 0x55f0548b39b3 <unknown>
#6 0x55f05487dc59 <unknown>
#7 0x55f05487ea08 <unknown>
#8 0x55f054d4f40a <unknown>
#9 0x55f054d5285e <unknown>
#10 0x55f054d52308 <unknown>
#11 0x55f054d52ce5 <unknown>
#12 0x55f054d38b7b <unknown>
#13 0x55f054d53050 <unknown>
#14 0x55f054d21ae9 <unknown>
#15 0x55f054d71df5 <unknown>
#16 0x55f054d71fdb <unknown>
#17 0x55f054d81c05 <unknown>
#18 0x7f66944e3134 <unknown>
[2025-05-07T10:17:23.164+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:17:23.177+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.100 seconds
[2025-05-07T10:17:55.545+0000] {processor.py:186} INFO - Started process (PID=11373) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:17:55.546+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:17:55.547+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:17:55.546+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:17:56.446+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:17:56.669+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:17:56.668+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:17:58.229+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:17:58.227+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x558c3f49ea8e <unknown>
#1 0x558c3ef5bb0b <unknown>
#2 0x558c3ef924ea <unknown>
#3 0x558c3ef8daef <unknown>
#4 0x558c3efe1b18 <unknown>
#5 0x558c3efcf9b3 <unknown>
#6 0x558c3ef99c59 <unknown>
#7 0x558c3ef9aa08 <unknown>
#8 0x558c3f46b40a <unknown>
#9 0x558c3f46e85e <unknown>
#10 0x558c3f46e308 <unknown>
#11 0x558c3f46ece5 <unknown>
#12 0x558c3f454b7b <unknown>
#13 0x558c3f46f050 <unknown>
#14 0x558c3f43dae9 <unknown>
#15 0x558c3f48ddf5 <unknown>
#16 0x558c3f48dfdb <unknown>
#17 0x558c3f49dc05 <unknown>
#18 0x7fa35571b134 <unknown>
[2025-05-07T10:17:58.230+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:17:58.245+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.056 seconds
[2025-05-07T10:18:29.080+0000] {processor.py:186} INFO - Started process (PID=11533) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:18:29.082+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:18:29.083+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:18:29.083+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:18:34.907+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:18:35.102+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:18:35.102+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:18:36.589+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:18:36.589+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x561b3e3cca8e <unknown>
#1 0x561b3de89b0b <unknown>
#2 0x561b3dec04ea <unknown>
#3 0x561b3debbaef <unknown>
#4 0x561b3df0fb18 <unknown>
#5 0x561b3defd9b3 <unknown>
#6 0x561b3dec7c59 <unknown>
#7 0x561b3dec8a08 <unknown>
#8 0x561b3e39940a <unknown>
#9 0x561b3e39c85e <unknown>
#10 0x561b3e39c308 <unknown>
#11 0x561b3e39cce5 <unknown>
#12 0x561b3e382b7b <unknown>
#13 0x561b3e39d050 <unknown>
#14 0x561b3e36bae9 <unknown>
#15 0x561b3e3bbdf5 <unknown>
#16 0x561b3e3bbfdb <unknown>
#17 0x561b3e3cbc05 <unknown>
#18 0x7f897a44f134 <unknown>
[2025-05-07T10:18:36.590+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:18:36.605+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.524 seconds
[2025-05-07T10:19:06.976+0000] {processor.py:186} INFO - Started process (PID=11690) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:19:06.978+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:19:06.978+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:19:06.978+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:19:13.790+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:19:14.009+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:19:14.009+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:19:15.008+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:19:15.007+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55932e414a8e <unknown>
#1 0x55932ded1b0b <unknown>
#2 0x55932df084ea <unknown>
#3 0x55932df03aef <unknown>
#4 0x55932df57b18 <unknown>
#5 0x55932df459b3 <unknown>
#6 0x55932df0fc59 <unknown>
#7 0x55932df10a08 <unknown>
#8 0x55932e3e140a <unknown>
#9 0x55932e3e485e <unknown>
#10 0x55932e3e4308 <unknown>
#11 0x55932e3e4ce5 <unknown>
#12 0x55932e3cab7b <unknown>
#13 0x55932e3e5050 <unknown>
#14 0x55932e3b3ae9 <unknown>
#15 0x55932e403df5 <unknown>
#16 0x55932e403fdb <unknown>
#17 0x55932e413c05 <unknown>
#18 0x7f190663b134 <unknown>
[2025-05-07T10:19:15.009+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:19:15.023+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.054 seconds
[2025-05-07T10:19:45.107+0000] {processor.py:186} INFO - Started process (PID=11843) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:19:45.108+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:19:45.109+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:19:45.109+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:19:50.940+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:19:51.172+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:19:51.172+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:19:52.789+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:19:52.788+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x5628d86d0a8e <unknown>
#1 0x5628d818db0b <unknown>
#2 0x5628d81c44ea <unknown>
#3 0x5628d81bfaef <unknown>
#4 0x5628d8213b18 <unknown>
#5 0x5628d82019b3 <unknown>
#6 0x5628d81cbc59 <unknown>
#7 0x5628d81cca08 <unknown>
#8 0x5628d869d40a <unknown>
#9 0x5628d86a085e <unknown>
#10 0x5628d86a0308 <unknown>
#11 0x5628d86a0ce5 <unknown>
#12 0x5628d8686b7b <unknown>
#13 0x5628d86a1050 <unknown>
#14 0x5628d866fae9 <unknown>
#15 0x5628d86bfdf5 <unknown>
#16 0x5628d86bffdb <unknown>
#17 0x5628d86cfc05 <unknown>
#18 0x7facd8f4d134 <unknown>
[2025-05-07T10:19:52.789+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:19:52.806+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.702 seconds
[2025-05-07T10:20:25.752+0000] {processor.py:186} INFO - Started process (PID=11995) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:20:25.765+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:20:25.766+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:20:25.766+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:20:26.811+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:20:27.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:20:27.050+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:20:28.533+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:20:28.532+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55dd0e363a8e <unknown>
#1 0x55dd0de20b0b <unknown>
#2 0x55dd0de574ea <unknown>
#3 0x55dd0de52aef <unknown>
#4 0x55dd0dea6b18 <unknown>
#5 0x55dd0de949b3 <unknown>
#6 0x55dd0de5ec59 <unknown>
#7 0x55dd0de5fa08 <unknown>
#8 0x55dd0e33040a <unknown>
#9 0x55dd0e33385e <unknown>
#10 0x55dd0e333308 <unknown>
#11 0x55dd0e333ce5 <unknown>
#12 0x55dd0e319b7b <unknown>
#13 0x55dd0e334050 <unknown>
#14 0x55dd0e302ae9 <unknown>
#15 0x55dd0e352df5 <unknown>
#16 0x55dd0e352fdb <unknown>
#17 0x55dd0e362c05 <unknown>
#18 0x7fa488d80134 <unknown>
[2025-05-07T10:20:28.534+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:20:28.548+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.153 seconds
[2025-05-07T10:20:58.691+0000] {processor.py:186} INFO - Started process (PID=12149) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:20:58.691+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:20:58.692+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:20:58.692+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:21:05.066+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:21:05.291+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:21:05.291+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:21:06.814+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:21:06.813+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    scraper = ProfileScraper(id)
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x55833a8f6a8e <unknown>
#1 0x55833a3b3b0b <unknown>
#2 0x55833a3ea4ea <unknown>
#3 0x55833a3e5aef <unknown>
#4 0x55833a439b18 <unknown>
#5 0x55833a4279b3 <unknown>
#6 0x55833a3f1c59 <unknown>
#7 0x55833a3f2a08 <unknown>
#8 0x55833a8c340a <unknown>
#9 0x55833a8c685e <unknown>
#10 0x55833a8c6308 <unknown>
#11 0x55833a8c6ce5 <unknown>
#12 0x55833a8acb7b <unknown>
#13 0x55833a8c7050 <unknown>
#14 0x55833a895ae9 <unknown>
#15 0x55833a8e5df5 <unknown>
#16 0x55833a8e5fdb <unknown>
#17 0x55833a8f5c05 <unknown>
#18 0x7f4036ab4134 <unknown>
[2025-05-07T10:21:06.815+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:21:06.829+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.139 seconds
[2025-05-07T10:21:40.729+0000] {processor.py:186} INFO - Started process (PID=12304) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:21:40.730+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:21:40.731+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:21:40.731+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:21:41.409+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:21:41.690+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:21:41.690+0000] {selenium_manager.py:133} WARNING - The chromedriver version (136.0.7103.59) detected in PATH at /usr/bin/chromedriver might not be compatible with the detected chrome version (136.0.7103.92); currently, chromedriver 136.0.7103.92 is recommended for chrome 136.*, so it is advised to delete the driver in PATH and retry
[2025-05-07T10:21:43.259+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:21:43.256+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 48, in <module>
    scrape_task = ins_videos_scraper(id="baukrysie",
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 5, in ins_videos_scraper
    def ins_videos_scraper(id = "baukrysie",INSTAGRAM_FILE_PATH=""):
  File "/opt/airflow/dags/utils/instagram_profile.py", line 16, in __init__
    super().__init__(
  File "/opt/airflow/dags/utils/instagram_base.py", line 26, in __init__
    self._driver = webdriver.Chrome(options=options)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py", line 56, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 206, in __init__
    self.start_session(capabilities)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 290, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 345, in execute
    self.error_handler.check_response(response)
  File "/home/airflow/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir
Stacktrace:
#0 0x56340ae31a8e <unknown>
#1 0x56340a8eeb0b <unknown>
#2 0x56340a9254ea <unknown>
#3 0x56340a920aef <unknown>
#4 0x56340a974b18 <unknown>
#5 0x56340a9629b3 <unknown>
#6 0x56340a92cc59 <unknown>
#7 0x56340a92da08 <unknown>
#8 0x56340adfe40a <unknown>
#9 0x56340ae0185e <unknown>
#10 0x56340ae01308 <unknown>
#11 0x56340ae01ce5 <unknown>
#12 0x56340ade7b7b <unknown>
#13 0x56340ae02050 <unknown>
#14 0x56340add0ae9 <unknown>
#15 0x56340ae20df5 <unknown>
#16 0x56340ae20fdb <unknown>
#17 0x56340ae30c05 <unknown>
#18 0x7fb7567db134 <unknown>
[2025-05-07T10:21:43.260+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:21:43.275+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.909 seconds
[2025-05-07T10:22:13.572+0000] {processor.py:186} INFO - Started process (PID=12454) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:22:13.574+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:22:13.575+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:13.574+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:22:13.650+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:13.648+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 5, in <module>
    from tasks.insta_scraper import ins_videos_scraper
  File "/opt/airflow/dags/tasks/insta_scraper.py", line 4, in <module>
    @task
NameError: name 'task' is not defined
[2025-05-07T10:22:13.650+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:22:13.664+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 0.097 seconds
[2025-05-07T10:22:46.000+0000] {processor.py:186} INFO - Started process (PID=12468) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:22:46.001+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:22:46.002+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:46.001+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:22:47.062+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:22:47.278+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:22:47.365+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:47.365+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:22:47.373+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:47.373+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:22:47.375+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:47.374+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:22:47.375+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:47.375+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:22:47.376+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:22:47.376+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:22:47.396+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.761 seconds
[2025-05-07T10:23:20.869+0000] {processor.py:186} INFO - Started process (PID=12592) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:23:20.870+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:23:20.871+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:20.871+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:23:21.255+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:23:21.603+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:23:21.631+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:21.631+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:23:21.641+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:21.641+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:23:21.644+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:21.643+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:23:21.645+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:21.644+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:23:21.645+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:21.645+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:23:21.658+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.151 seconds
[2025-05-07T10:23:56.105+0000] {processor.py:186} INFO - Started process (PID=12713) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:23:56.106+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:23:56.108+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:56.107+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:23:57.989+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:23:58.581+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:23:58.616+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:58.616+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:23:58.627+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:58.627+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:23:58.631+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:58.631+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:23:58.632+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:58.632+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:23:58.633+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:23:58.633+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:23:58.648+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.902 seconds
[2025-05-07T10:24:31.196+0000] {processor.py:186} INFO - Started process (PID=12860) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:24:31.197+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:24:31.198+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:24:31.198+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:24:32.641+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:24:32.832+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:24:32.856+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:24:32.855+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:24:32.866+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:24:32.865+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:24:32.868+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:24:32.868+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:24:32.869+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:24:32.869+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:24:32.870+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:24:32.869+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:24:32.894+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.062 seconds
[2025-05-07T10:25:03.627+0000] {processor.py:186} INFO - Started process (PID=12987) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:25:03.637+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:25:03.638+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:03.638+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:25:16.051+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:25:16.272+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:25:10.441+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:10.440+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:25:10.450+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:10.450+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:25:10.452+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:10.452+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:25:10.453+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:10.453+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:25:10.453+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:10.453+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:25:10.464+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.837 seconds
[2025-05-07T10:25:41.066+0000] {processor.py:186} INFO - Started process (PID=13113) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:25:41.067+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:25:41.068+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:41.068+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:25:42.035+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:25:42.216+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:25:42.240+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:42.240+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:25:42.249+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:42.249+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:25:42.252+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:42.252+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:25:42.253+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:42.253+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:25:42.253+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:25:42.253+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:25:42.265+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.566 seconds
[2025-05-07T10:26:16.329+0000] {processor.py:186} INFO - Started process (PID=13258) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:26:16.332+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:26:16.334+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:16.333+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:26:16.906+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:26:17.132+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:26:17.153+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:17.153+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:26:17.164+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:17.164+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:26:17.166+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:17.166+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:26:17.167+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:17.167+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:26:17.168+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:17.168+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:26:17.179+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.207 seconds
[2025-05-07T10:26:47.902+0000] {processor.py:186} INFO - Started process (PID=13381) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:26:47.903+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:26:47.904+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:47.903+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:26:55.803+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:26:55.961+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:26:55.984+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:55.984+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:26:55.993+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:55.992+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:26:55.995+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:55.995+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:26:55.995+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:55.995+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:26:55.996+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:26:55.996+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:26:56.007+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.110 seconds
[2025-05-07T10:27:09.287+0000] {processor.py:186} INFO - Started process (PID=13500) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:27:09.288+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:27:09.288+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:09.288+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:27:15.715+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:27:15.937+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:27:16.003+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:16.003+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:27:16.011+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:16.011+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:27:16.013+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:16.013+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:27:16.014+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:16.014+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:27:16.014+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:16.014+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:27:16.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.740 seconds
[2025-05-07T10:27:51.526+0000] {processor.py:186} INFO - Started process (PID=13626) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:27:45.668+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:27:45.669+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:45.669+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:27:52.885+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:27:53.117+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:27:53.147+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:53.147+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:27:53.158+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:53.158+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:27:53.161+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:53.161+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:27:53.162+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:53.162+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:27:53.162+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:27:53.162+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:27:53.174+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.013 seconds
[2025-05-07T10:28:23.811+0000] {processor.py:186} INFO - Started process (PID=13759) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:28:23.824+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:28:23.825+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:28:23.825+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:28:30.563+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:28:30.886+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:28:30.910+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:28:30.909+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:28:30.921+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:28:30.921+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:28:30.923+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:28:30.923+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:28:30.924+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:28:30.924+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:28:30.925+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:28:30.925+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:28:30.937+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.639 seconds
[2025-05-07T10:29:01.591+0000] {processor.py:186} INFO - Started process (PID=13895) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:29:01.592+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:29:01.593+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:01.593+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:29:02.866+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:29:03.080+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:29:03.114+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:03.114+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:29:03.123+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:03.123+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:29:03.126+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:03.125+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:29:03.126+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:03.126+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:29:03.127+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:03.127+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:29:03.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.907 seconds
[2025-05-07T10:29:34.046+0000] {processor.py:186} INFO - Started process (PID=14026) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:29:34.046+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:29:34.047+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:34.047+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:29:40.619+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:29:40.843+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:29:40.866+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:40.865+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:29:40.875+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:40.875+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:29:40.877+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:40.877+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:29:40.878+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:40.878+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:29:40.879+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:29:40.879+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:29:40.892+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.350 seconds
[2025-05-07T10:30:11.522+0000] {processor.py:186} INFO - Started process (PID=14153) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:30:11.523+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:30:11.524+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:11.524+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:30:12.801+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:30:13.035+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:30:13.057+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:13.057+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:30:13.067+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:13.067+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:30:13.069+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:13.069+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:30:13.070+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:13.070+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:30:13.071+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:13.071+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:30:13.082+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.923 seconds
[2025-05-07T10:30:46.782+0000] {processor.py:186} INFO - Started process (PID=14291) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:30:46.783+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:30:46.784+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:46.784+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:30:48.122+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:30:48.652+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:30:48.679+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:48.679+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:30:48.688+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:48.688+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:30:48.691+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:48.690+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:30:48.691+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:48.691+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:30:48.692+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:30:48.692+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:30:48.702+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.282 seconds
[2025-05-07T10:31:18.775+0000] {processor.py:186} INFO - Started process (PID=14432) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:31:18.776+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:31:18.777+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:31:18.777+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:31:25.864+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:31:31.720+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:31:31.752+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:31:31.751+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:31:31.761+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:31:31.761+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:31:31.763+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:31:31.763+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:31:31.764+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:31:31.764+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:31:31.765+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:31:31.765+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:31:31.776+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.154 seconds
[2025-05-07T10:32:02.732+0000] {processor.py:186} INFO - Started process (PID=14568) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:32:02.733+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:32:02.735+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:02.735+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:32:10.504+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:32:10.748+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:32:10.777+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:10.777+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:32:10.786+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:10.786+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:32:10.789+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:10.789+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:32:10.790+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:10.789+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:32:10.790+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:10.790+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:32:10.801+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.571 seconds
[2025-05-07T10:32:41.905+0000] {processor.py:186} INFO - Started process (PID=14708) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:32:41.906+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:32:41.906+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:41.906+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:32:42.912+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:32:43.170+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:32:43.214+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:43.213+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:32:43.224+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:43.224+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:32:43.226+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:43.226+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:32:43.227+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:43.227+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:32:43.228+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:32:43.228+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:32:43.242+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.700 seconds
[2025-05-07T10:33:13.713+0000] {processor.py:186} INFO - Started process (PID=14832) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:33:13.714+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:33:13.715+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:13.715+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:33:21.103+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:33:26.887+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:33:26.926+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:26.925+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:33:26.935+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:26.935+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:33:26.938+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:26.938+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:33:26.939+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:26.939+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:33:26.940+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:26.940+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:33:26.952+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.384 seconds
[2025-05-07T10:33:57.101+0000] {processor.py:186} INFO - Started process (PID=14972) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:33:57.102+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:33:57.103+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:57.103+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:33:58.185+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:33:58.436+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:33:58.479+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:58.479+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:33:58.489+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:58.489+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:33:58.492+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:58.492+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:33:58.492+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:58.492+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:33:58.493+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:33:58.493+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:33:58.507+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.779 seconds
[2025-05-07T10:34:32.041+0000] {processor.py:186} INFO - Started process (PID=15106) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:34:32.043+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:34:32.043+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:34:32.043+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:34:33.293+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:34:33.549+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:34:33.603+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:34:33.603+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:34:33.615+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:34:33.615+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:34:33.618+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:34:33.618+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:34:33.619+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:34:33.619+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:34:33.620+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:34:33.620+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:34:33.633+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.955 seconds
[2025-05-07T10:35:07.026+0000] {processor.py:186} INFO - Started process (PID=15238) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:35:07.028+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:35:07.032+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:07.031+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:35:17.065+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:35:11.483+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:35:11.533+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:11.533+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:35:11.542+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:11.542+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:35:11.544+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:11.544+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:35:11.545+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:11.545+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:35:11.547+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:11.546+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:35:11.560+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 11.403 seconds
[2025-05-07T10:35:41.678+0000] {processor.py:186} INFO - Started process (PID=15363) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:35:41.679+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:35:41.679+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:41.679+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:35:48.700+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:35:49.007+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:35:49.042+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:49.042+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:35:49.051+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:49.051+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:35:49.054+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:49.054+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:35:49.054+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:49.054+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:35:49.055+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:35:49.055+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:35:49.067+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.406 seconds
[2025-05-07T10:36:19.744+0000] {processor.py:186} INFO - Started process (PID=15511) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:36:19.745+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:36:19.746+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:36:19.746+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:36:32.381+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:36:26.756+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:36:26.792+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:36:26.791+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:36:26.802+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:36:26.802+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:36:26.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:36:26.804+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:36:26.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:36:26.805+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:36:26.806+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:36:26.806+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:36:26.817+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.077 seconds
[2025-05-07T10:36:56.895+0000] {processor.py:186} INFO - Started process (PID=15641) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:36:56.896+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:36:56.897+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:36:56.897+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:37:04.181+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:37:04.428+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:37:04.457+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:04.457+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:37:04.468+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:04.468+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:37:04.470+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:04.470+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:37:04.471+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:04.471+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:37:04.472+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:04.472+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:37:04.483+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.098 seconds
[2025-05-07T10:37:37.401+0000] {processor.py:186} INFO - Started process (PID=15780) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:37:37.402+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:37:37.403+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:37.403+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:37:39.133+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:37:39.383+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:37:39.440+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:39.440+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:37:39.450+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:39.450+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:37:39.454+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:39.454+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:37:39.455+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:39.455+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:37:39.456+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:37:39.456+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:37:39.469+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.439 seconds
[2025-05-07T10:38:10.218+0000] {processor.py:186} INFO - Started process (PID=15913) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:38:10.219+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:38:10.220+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:10.220+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:38:17.356+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:38:17.635+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:38:17.659+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:17.658+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:38:17.668+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:17.668+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:38:17.671+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:17.671+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:38:17.672+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:17.671+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:38:17.672+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:17.672+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:38:17.683+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.458 seconds
[2025-05-07T10:38:48.555+0000] {processor.py:186} INFO - Started process (PID=16049) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:38:48.556+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:38:48.557+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:48.557+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:38:56.235+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:38:56.480+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:38:56.517+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:56.516+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:38:56.529+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:56.529+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:38:56.531+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:56.531+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:38:56.532+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:56.532+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:38:56.533+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:38:56.533+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:38:56.547+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.495 seconds
[2025-05-07T10:39:32.589+0000] {processor.py:186} INFO - Started process (PID=16185) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:39:32.591+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:39:32.591+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:39:32.591+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:39:33.624+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:39:33.830+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:39:33.882+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:39:33.882+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:39:33.891+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:39:33.891+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:39:33.894+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:39:33.894+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:39:33.895+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:39:33.895+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:39:33.896+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:39:33.896+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:39:33.908+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.690 seconds
[2025-05-07T10:40:07.594+0000] {processor.py:186} INFO - Started process (PID=16318) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:40:07.599+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:40:07.600+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:07.600+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:40:10.165+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:40:10.909+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:40:10.983+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:10.983+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:40:10.993+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:10.993+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:40:10.996+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:10.996+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:40:10.997+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:10.997+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:40:10.998+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:10.998+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:40:11.008+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.783 seconds
[2025-05-07T10:40:42.777+0000] {processor.py:186} INFO - Started process (PID=16439) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:40:42.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:40:42.779+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:42.779+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:40:44.524+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:40:44.831+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:40:44.874+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:44.874+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:40:44.886+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:44.886+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:40:44.889+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:44.888+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:40:44.890+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:44.889+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:40:44.891+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:40:44.890+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:40:44.905+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.499 seconds
[2025-05-07T10:41:17.786+0000] {processor.py:186} INFO - Started process (PID=16582) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:41:17.787+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:41:17.789+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:17.788+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:41:19.091+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:41:19.307+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:41:19.354+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:19.354+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:41:19.364+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:19.363+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:41:19.366+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:19.366+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:41:19.367+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:19.366+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:41:19.367+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:19.367+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:41:19.380+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.964 seconds
[2025-05-07T10:41:52.985+0000] {processor.py:186} INFO - Started process (PID=16708) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:41:52.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:41:52.986+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:52.986+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:41:54.007+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:41:54.231+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:41:54.287+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:54.287+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:41:54.298+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:54.297+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:41:54.300+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:54.300+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:41:54.300+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:54.300+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:41:54.301+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:41:54.301+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:41:54.315+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.704 seconds
[2025-05-07T10:42:28.024+0000] {processor.py:186} INFO - Started process (PID=16841) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:42:28.027+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:42:28.029+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:42:28.028+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:42:29.430+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:42:29.652+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:42:29.696+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:42:29.696+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:42:29.704+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:42:29.704+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:42:29.707+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:42:29.707+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:42:29.708+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:42:29.708+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:42:29.708+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:42:29.708+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:42:29.719+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.070 seconds
[2025-05-07T10:43:00.500+0000] {processor.py:186} INFO - Started process (PID=16979) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:43:00.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:43:00.503+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:00.502+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:43:07.287+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:43:07.252+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:43:07.312+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:07.312+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:43:07.321+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:07.321+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:43:07.324+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:07.324+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:43:07.324+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:07.324+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:43:07.326+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:07.325+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:43:07.338+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.851 seconds
[2025-05-07T10:43:37.917+0000] {processor.py:186} INFO - Started process (PID=17110) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:43:37.918+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:43:37.919+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:37.919+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:43:39.012+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:43:39.935+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:43:40.049+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:40.049+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:43:40.062+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:40.061+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:43:40.066+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:40.065+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:43:40.067+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:40.067+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:43:40.068+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:43:40.068+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:43:40.088+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.549 seconds
[2025-05-07T10:44:13.100+0000] {processor.py:186} INFO - Started process (PID=17241) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:44:13.101+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:44:13.103+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:13.102+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:44:14.646+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:44:14.909+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:44:14.938+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:14.938+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:44:14.949+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:14.949+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:44:14.952+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:14.952+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:44:14.953+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:14.953+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:44:14.954+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:14.954+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:44:14.964+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.243 seconds
[2025-05-07T10:44:48.031+0000] {processor.py:186} INFO - Started process (PID=17372) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:44:48.032+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:44:48.033+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:48.033+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:44:50.279+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:44:50.506+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:44:50.532+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:50.532+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:44:50.543+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:50.543+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:44:50.545+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:50.545+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:44:50.546+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:50.546+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:44:50.547+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:44:50.547+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:44:50.559+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.899 seconds
[2025-05-07T10:45:20.782+0000] {processor.py:186} INFO - Started process (PID=17509) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:45:20.784+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:45:20.785+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:20.784+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:45:27.503+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:45:27.862+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'tiktok_videos_scraper_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:45:27.916+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:27.915+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:45:27.927+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:27.926+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:45:27.930+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:27.929+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:45:27.931+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:27.931+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:45:27.932+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:27.931+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:45:27.947+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.165 seconds
[2025-05-07T10:45:58.192+0000] {processor.py:186} INFO - Started process (PID=17641) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:45:58.193+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:45:58.194+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:58.193+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:45:59.018+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:45:59.353+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:45:59.387+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:59.387+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:45:59.396+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:59.396+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:45:59.400+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:59.400+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:45:59.401+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:59.400+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:45:59.402+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:45:59.401+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:45:59.416+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.599 seconds
[2025-05-07T10:46:33.172+0000] {processor.py:186} INFO - Started process (PID=17778) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:46:33.173+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:46:33.174+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:46:33.174+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:46:54.970+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:46:54.805+0000] {timeout.py:68} ERROR - Process timed out, PID: 17778
[2025-05-07T10:47:55.117+0000] {processor.py:186} INFO - Started process (PID=17905) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:47:55.119+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:47:55.120+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:47:55.120+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:48:02.465+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:48:02.687+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:48:02.822+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:02.822+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:48:02.830+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:02.830+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:48:02.832+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:02.832+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:48:02.833+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:02.833+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:48:02.834+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:02.834+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:48:02.852+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.244 seconds
[2025-05-07T10:48:33.342+0000] {processor.py:186} INFO - Started process (PID=18053) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:48:33.343+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:48:33.344+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:33.344+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:48:35.724+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:48:36.416+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:48:36.488+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:36.484+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:48:36.500+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:36.500+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:48:36.503+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:36.503+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:48:36.505+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:36.505+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:48:36.506+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:48:36.506+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:48:36.523+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 9.559 seconds
[2025-05-07T10:49:07.111+0000] {processor.py:186} INFO - Started process (PID=18202) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:49:07.112+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:49:07.113+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:49:07.113+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:49:19.199+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:49:20.411+0000] {processor.py:925} INFO - DAG(s) 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_login_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:49:20.498+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:49:20.493+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:49:20.512+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:49:20.512+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:49:20.515+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:49:20.515+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:49:20.517+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:49:20.516+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:49:20.517+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:49:20.517+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:49:20.532+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 14.928 seconds
[2025-05-07T10:49:51.088+0000] {processor.py:186} INFO - Started process (PID=18343) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:49:51.090+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:49:51.091+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:49:51.090+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:50:19.528+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:50:24.032+0000] {timeout.py:68} ERROR - Process timed out, PID: 18343
[2025-05-07T10:51:23.710+0000] {processor.py:186} INFO - Started process (PID=18456) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:51:23.712+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:51:23.713+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:51:23.713+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:51:46.026+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:51:45.412+0000] {timeout.py:68} ERROR - Process timed out, PID: 18456
[2025-05-07T10:54:53.892+0000] {processor.py:186} INFO - Started process (PID=18577) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:54:53.893+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:54:53.894+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:54:53.894+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:54:56.963+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:54:57.301+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:54:57.474+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:54:57.474+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:54:57.485+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:54:57.485+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:54:57.487+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:54:57.487+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:54:57.489+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:54:57.489+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:54:57.489+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:54:57.489+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:54:57.510+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 10.002 seconds
[2025-05-07T10:55:28.077+0000] {processor.py:186} INFO - Started process (PID=18722) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:55:28.078+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:55:28.080+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:55:28.080+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:55:35.268+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:55:35.742+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:55:35.784+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:55:35.784+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:55:35.794+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:55:35.794+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:55:35.796+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:55:35.796+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:55:35.797+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:55:35.797+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:55:35.798+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:55:35.798+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:55:35.808+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.731 seconds
[2025-05-07T10:56:06.692+0000] {processor.py:186} INFO - Started process (PID=18854) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:56:06.693+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:56:06.694+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:56:06.693+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:56:13.883+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:56:14.226+0000] {processor.py:925} INFO - DAG(s) 'instagram_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:56:14.271+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:56:14.271+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:56:14.281+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:56:14.281+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:56:14.284+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:56:14.284+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:56:14.285+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:56:14.284+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:56:14.285+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:56:14.285+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:56:14.296+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.613 seconds
[2025-05-07T10:56:49.097+0000] {processor.py:186} INFO - Started process (PID=18996) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:56:49.099+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:56:49.099+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:56:49.099+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:56:51.786+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:57:04.138+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'instagram_videos_scraper_dag', 'x_videos_scraper_dag', 'x_login_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:57:04.189+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:57:04.189+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:57:04.201+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:57:04.201+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:57:04.204+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:57:04.204+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:57:04.205+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:57:04.205+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:57:04.206+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:57:04.206+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:57:04.219+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 16.629 seconds
[2025-05-07T10:57:34.705+0000] {processor.py:186} INFO - Started process (PID=19145) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:57:34.706+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:57:34.707+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:57:34.707+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:58:02.217+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:58:02.195+0000] {timeout.py:68} ERROR - Process timed out, PID: 19145
[2025-05-07T10:58:33.740+0000] {processor.py:186} INFO - Started process (PID=19291) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:58:33.742+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:58:33.744+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:58:33.743+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:58:40.807+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:58:41.053+0000] {processor.py:925} INFO - DAG(s) 'tiktok_videos_scraper_dag', 'x_login_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:58:41.189+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:58:41.189+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:58:41.197+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:58:41.196+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:58:41.199+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:58:41.198+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:58:41.199+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:58:41.199+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:58:41.200+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:58:41.200+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:58:41.215+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 8.487 seconds
[2025-05-07T10:59:12.050+0000] {processor.py:186} INFO - Started process (PID=19426) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:59:12.051+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:59:12.052+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:59:12.052+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2025-05-07T10:59:18.848+0000] {logging_mixin.py:190} INFO - [INFO] Final URL: https://x.com/home
[2025-05-07T10:59:18.768+0000] {processor.py:925} INFO - DAG(s) 'x_login_dag', 'x_videos_scraper_dag', 'instagram_videos_scraper_dag', 'tiktok_videos_scraper_dag' retrieved from /opt/airflow/dags/pipeline.py
[2025-05-07T10:59:18.792+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:59:18.792+0000] {dag.py:3211} INFO - Sync 4 DAGs
[2025-05-07T10:59:18.802+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:59:18.802+0000] {dag.py:4138} INFO - Setting next_dagrun for instagram_videos_scraper_dag to None, run_after=None
[2025-05-07T10:59:18.805+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:59:18.805+0000] {dag.py:4138} INFO - Setting next_dagrun for tiktok_videos_scraper_dag to None, run_after=None
[2025-05-07T10:59:18.806+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:59:18.805+0000] {dag.py:4138} INFO - Setting next_dagrun for x_login_dag to None, run_after=None
[2025-05-07T10:59:18.806+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:59:18.806+0000] {dag.py:4138} INFO - Setting next_dagrun for x_videos_scraper_dag to None, run_after=None
[2025-05-07T10:59:18.818+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/pipeline.py took 7.767 seconds
[2025-05-07T10:59:49.262+0000] {processor.py:186} INFO - Started process (PID=19550) to work on /opt/airflow/dags/pipeline.py
[2025-05-07T10:59:49.264+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2025-05-07T10:59:49.265+0000] {logging_mixin.py:190} INFO - [2025-05-07T10:59:49.265+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
